{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# importing packages\n",
    "########################################\n",
    "import matplotlib.pyplot as plt                        # data visualization\n",
    "import pandas as pd                                    # data science essentials\n",
    "from sklearn.model_selection import train_test_split   # train-test split\n",
    "from sklearn.metrics import roc_auc_score              # auc score\n",
    "from sklearn.model_selection import RandomizedSearchCV # hyperparameter tuning\n",
    "from sklearn.metrics import make_scorer                # customizable scorer\n",
    "from sklearn.metrics import confusion_matrix           # confusion matrix\n",
    "\n",
    "#logistic\n",
    "from sklearn.linear_model import LogisticRegression  # logistic regression\n",
    "import statsmodels.formula.api as smf                # logistic regression\n",
    "from sklearn.metrics import confusion_matrix         # confusion matrix\n",
    "from sklearn.metrics import roc_auc_score            # auc score\n",
    "from sklearn.neighbors import KNeighborsClassifier   # KNN for classification\n",
    "from sklearn.neighbors import KNeighborsRegressor    # KNN for regression\n",
    "from sklearn.preprocessing import StandardScaler     # standard scaler\n",
    "\n",
    "# CART model packages\n",
    "from sklearn.tree import DecisionTreeClassifier      # classification trees\n",
    "from sklearn.tree import export_graphviz             # exports graphics\n",
    "from six import StringIO                             # saves objects in memory\n",
    "from IPython.display import Image                    # displays on frontend\n",
    "import pydotplus\n",
    "\n",
    "# Hypertuning packages\n",
    "from sklearn.model_selection import RandomizedSearchCV     # hyperparameter tuning\n",
    "from sklearn.metrics import make_scorer              # customizable scorer\n",
    "\n",
    "#Ensemble Modeling packages\n",
    "from sklearn.ensemble import RandomForestClassifier     # random forest\n",
    "from sklearn.ensemble import GradientBoostingClassifier # gbm\n",
    "\n",
    "\n",
    "########################################\n",
    "# loading data and setting display options\n",
    "########################################\n",
    "# loading data\n",
    "chef = pd.read_excel('./datasets/Apprentice_Chef_Dataset.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1946 entries, 0 to 1945\n",
      "Data columns (total 28 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   REVENUE                      1946 non-null   float64\n",
      " 1   CROSS_SELL_SUCCESS           1946 non-null   int64  \n",
      " 2   NAME                         1946 non-null   object \n",
      " 3   EMAIL                        1946 non-null   object \n",
      " 4   FIRST_NAME                   1946 non-null   object \n",
      " 5   FAMILY_NAME                  1899 non-null   object \n",
      " 6   TOTAL_MEALS_ORDERED          1946 non-null   int64  \n",
      " 7   UNIQUE_MEALS_PURCH           1946 non-null   int64  \n",
      " 8   CONTACTS_W_CUSTOMER_SERVICE  1946 non-null   int64  \n",
      " 9   PRODUCT_CATEGORIES_VIEWED    1946 non-null   int64  \n",
      " 10  AVG_TIME_PER_SITE_VISIT      1946 non-null   float64\n",
      " 11  MOBILE_NUMBER                1946 non-null   int64  \n",
      " 12  CANCELLATIONS_BEFORE_NOON    1946 non-null   int64  \n",
      " 13  CANCELLATIONS_AFTER_NOON     1946 non-null   int64  \n",
      " 14  TASTES_AND_PREFERENCES       1946 non-null   int64  \n",
      " 15  PC_LOGINS                    1946 non-null   int64  \n",
      " 16  MOBILE_LOGINS                1946 non-null   int64  \n",
      " 17  WEEKLY_PLAN                  1946 non-null   int64  \n",
      " 18  EARLY_DELIVERIES             1946 non-null   int64  \n",
      " 19  LATE_DELIVERIES              1946 non-null   int64  \n",
      " 20  PACKAGE_LOCKER               1946 non-null   int64  \n",
      " 21  REFRIGERATED_LOCKER          1946 non-null   int64  \n",
      " 22  AVG_PREP_VID_TIME            1946 non-null   float64\n",
      " 23  LARGEST_ORDER_SIZE           1946 non-null   int64  \n",
      " 24  MASTER_CLASSES_ATTENDED      1946 non-null   int64  \n",
      " 25  MEDIAN_MEAL_RATING           1946 non-null   int64  \n",
      " 26  AVG_CLICKS_PER_VISIT         1946 non-null   int64  \n",
      " 27  TOTAL_PHOTOS_VIEWED          1946 non-null   int64  \n",
      "dtypes: float64(3), int64(21), object(4)\n",
      "memory usage: 425.8+ KB\n"
     ]
    }
   ],
   "source": [
    "chef.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>REVENUE</td>\n",
       "      <td>Float</td>\n",
       "      <td>Total revenue generated from each customer. No...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CROSS_SELL_SUCCESS</td>\n",
       "      <td>Integer</td>\n",
       "      <td>Success of the cross-sell promotion (1 = yes, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NAME</td>\n",
       "      <td>string</td>\n",
       "      <td>Full name of customer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EMAIL</td>\n",
       "      <td>string</td>\n",
       "      <td>Email of customer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FIRST_NAME</td>\n",
       "      <td>string</td>\n",
       "      <td>First name of customer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FAMILY_NAME</td>\n",
       "      <td>string</td>\n",
       "      <td>Last name of customer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TOTAL_MEALS_ORDERED</td>\n",
       "      <td>Integer</td>\n",
       "      <td>Total number of meals ordered by each customer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>UNIQUE_MEALS_PURCH</td>\n",
       "      <td>Integer</td>\n",
       "      <td>Number of unique meal sets ordered by each cus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CONTACTS_W_CUSTOMER_SERVICE</td>\n",
       "      <td>Integer</td>\n",
       "      <td>Number of times each customer made contact wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PRODUCT_CATEGORIES_VIEWED</td>\n",
       "      <td>Integer</td>\n",
       "      <td>Total number of meal categories viewed (vegan,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AVG_TIME_PER_SITE_VISIT</td>\n",
       "      <td>Float</td>\n",
       "      <td>Average time each customer spent per website o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MOBILE_NUMBER</td>\n",
       "      <td>Integer</td>\n",
       "      <td>Indicates whether the registered phone number ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CANCELLATIONS_BEFORE_NOON</td>\n",
       "      <td>Integer</td>\n",
       "      <td>Number of meals canceled BEFORE 12:00 PM (noon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>CANCELLATIONS_AFTER_NOON</td>\n",
       "      <td>Integer</td>\n",
       "      <td>Number of meals canceled AFTER 12:00 PM (noon)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>TASTES_AND_PREFERENCES</td>\n",
       "      <td>Integer</td>\n",
       "      <td>Customer specified their tastes and preference...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MOBILE_LOGINS</td>\n",
       "      <td>Integer</td>\n",
       "      <td>Total number of logins to the mobile platform</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>PC_LOGINS</td>\n",
       "      <td>Integer</td>\n",
       "      <td>Total number of logins to the website</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>WEEKLY_PLAN</td>\n",
       "      <td>Integer</td>\n",
       "      <td>Number of weeks customer subscribed to the wee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>EARLY_DELIVERIES</td>\n",
       "      <td>Integer</td>\n",
       "      <td>Total meal deliveries that arrived early</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LATE_DELIVERIES</td>\n",
       "      <td>Integer</td>\n",
       "      <td>Total meal deliveries that arrived late</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>PACKAGE_LOCKER</td>\n",
       "      <td>Integer</td>\n",
       "      <td>Customer's residence/building has a package lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>REFRIGERATED_LOCKER</td>\n",
       "      <td>Integer</td>\n",
       "      <td>Package locker has refrigerated compartments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>AVG_PREP_VID_TIME</td>\n",
       "      <td>Float</td>\n",
       "      <td>Average time in seconds meal prep instruction ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LARGEST_ORDER_SIZE</td>\n",
       "      <td>Integer</td>\n",
       "      <td>MISLABELED: Despite the fact that this feature...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>MASTER_CLASSES_ATTENDED</td>\n",
       "      <td>Integer</td>\n",
       "      <td>Number of times each customer attended a maste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>MEDIAN_MEAL_RATING</td>\n",
       "      <td>Integer</td>\n",
       "      <td>Median rating of meal sets by each customer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>AVG_CLICKS_PER_VISIT</td>\n",
       "      <td>Float</td>\n",
       "      <td>Average number of clicks per site or mobile ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>TOTAL_PHOTOS_VIEWED</td>\n",
       "      <td>Integer</td>\n",
       "      <td>Total number of clicks on photos across all we...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Feature Data Type  \\\n",
       "0                       REVENUE     Float   \n",
       "1            CROSS_SELL_SUCCESS   Integer   \n",
       "2                          NAME    string   \n",
       "3                         EMAIL    string   \n",
       "4                    FIRST_NAME    string   \n",
       "5                   FAMILY_NAME    string   \n",
       "6           TOTAL_MEALS_ORDERED   Integer   \n",
       "7            UNIQUE_MEALS_PURCH   Integer   \n",
       "8   CONTACTS_W_CUSTOMER_SERVICE   Integer   \n",
       "9     PRODUCT_CATEGORIES_VIEWED   Integer   \n",
       "10      AVG_TIME_PER_SITE_VISIT     Float   \n",
       "11                MOBILE_NUMBER   Integer   \n",
       "12    CANCELLATIONS_BEFORE_NOON   Integer   \n",
       "13     CANCELLATIONS_AFTER_NOON   Integer   \n",
       "14       TASTES_AND_PREFERENCES   Integer   \n",
       "15                MOBILE_LOGINS   Integer   \n",
       "16                    PC_LOGINS   Integer   \n",
       "17                  WEEKLY_PLAN   Integer   \n",
       "18             EARLY_DELIVERIES   Integer   \n",
       "19              LATE_DELIVERIES   Integer   \n",
       "20               PACKAGE_LOCKER   Integer   \n",
       "21          REFRIGERATED_LOCKER   Integer   \n",
       "22            AVG_PREP_VID_TIME     Float   \n",
       "23           LARGEST_ORDER_SIZE   Integer   \n",
       "24      MASTER_CLASSES_ATTENDED   Integer   \n",
       "25           MEDIAN_MEAL_RATING   Integer   \n",
       "26         AVG_CLICKS_PER_VISIT     Float   \n",
       "27          TOTAL_PHOTOS_VIEWED   Integer   \n",
       "\n",
       "                                          Description  \n",
       "0   Total revenue generated from each customer. No...  \n",
       "1   Success of the cross-sell promotion (1 = yes, ...  \n",
       "2                               Full name of customer  \n",
       "3                                   Email of customer  \n",
       "4                              First name of customer  \n",
       "5                               Last name of customer  \n",
       "6   Total number of meals ordered by each customer...  \n",
       "7   Number of unique meal sets ordered by each cus...  \n",
       "8   Number of times each customer made contact wit...  \n",
       "9   Total number of meal categories viewed (vegan,...  \n",
       "10  Average time each customer spent per website o...  \n",
       "11  Indicates whether the registered phone number ...  \n",
       "12  Number of meals canceled BEFORE 12:00 PM (noon...  \n",
       "13  Number of meals canceled AFTER 12:00 PM (noon)...  \n",
       "14  Customer specified their tastes and preference...  \n",
       "15      Total number of logins to the mobile platform  \n",
       "16              Total number of logins to the website  \n",
       "17  Number of weeks customer subscribed to the wee...  \n",
       "18           Total meal deliveries that arrived early  \n",
       "19            Total meal deliveries that arrived late  \n",
       "20  Customer's residence/building has a package lo...  \n",
       "21       Package locker has refrigerated compartments  \n",
       "22  Average time in seconds meal prep instruction ...  \n",
       "23  MISLABELED: Despite the fact that this feature...  \n",
       "24  Number of times each customer attended a maste...  \n",
       "25        Median rating of meal sets by each customer  \n",
       "26  Average number of clicks per site or mobile ap...  \n",
       "27  Total number of clicks on photos across all we...  "
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pulling up data dictionary\n",
    "chef_description = pd.read_excel('./datasets/Apprentice_Chef_Data_Dictionary.xlsx')\n",
    "\n",
    "\n",
    "# displaying the data dictionary\n",
    "chef_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "REVENUE                         0\n",
       "CROSS_SELL_SUCCESS              0\n",
       "NAME                            0\n",
       "EMAIL                           0\n",
       "FIRST_NAME                      0\n",
       "FAMILY_NAME                    47\n",
       "TOTAL_MEALS_ORDERED             0\n",
       "UNIQUE_MEALS_PURCH              0\n",
       "CONTACTS_W_CUSTOMER_SERVICE     0\n",
       "PRODUCT_CATEGORIES_VIEWED       0\n",
       "AVG_TIME_PER_SITE_VISIT         0\n",
       "MOBILE_NUMBER                   0\n",
       "CANCELLATIONS_BEFORE_NOON       0\n",
       "CANCELLATIONS_AFTER_NOON        0\n",
       "TASTES_AND_PREFERENCES          0\n",
       "PC_LOGINS                       0\n",
       "MOBILE_LOGINS                   0\n",
       "WEEKLY_PLAN                     0\n",
       "EARLY_DELIVERIES                0\n",
       "LATE_DELIVERIES                 0\n",
       "PACKAGE_LOCKER                  0\n",
       "REFRIGERATED_LOCKER             0\n",
       "AVG_PREP_VID_TIME               0\n",
       "LARGEST_ORDER_SIZE              0\n",
       "MASTER_CLASSES_ATTENDED         0\n",
       "MEDIAN_MEAL_RATING              0\n",
       "AVG_CLICKS_PER_VISIT            0\n",
       "TOTAL_PHOTOS_VIEWED             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for missing values\n",
    "chef.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>saathos</td>\n",
       "      <td>unitedhealth.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alysanne.osgrey</td>\n",
       "      <td>ge.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>edwyd.fossoway</td>\n",
       "      <td>jnj.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eleyna.westerling</td>\n",
       "      <td>ge.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>elyn.norridge</td>\n",
       "      <td>jnj.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1941</th>\n",
       "      <td>obara.sand</td>\n",
       "      <td>yahoo.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1942</th>\n",
       "      <td>quentyn.blackwood</td>\n",
       "      <td>yahoo.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1943</th>\n",
       "      <td>rhonda.rowan</td>\n",
       "      <td>gmail.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1944</th>\n",
       "      <td>turnip</td>\n",
       "      <td>yahoo.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1945</th>\n",
       "      <td>tommard.heddle</td>\n",
       "      <td>merck.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1946 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0                 1\n",
       "0               saathos  unitedhealth.com\n",
       "1       alysanne.osgrey            ge.org\n",
       "2        edwyd.fossoway           jnj.com\n",
       "3     eleyna.westerling            ge.org\n",
       "4         elyn.norridge           jnj.com\n",
       "...                 ...               ...\n",
       "1941         obara.sand         yahoo.com\n",
       "1942  quentyn.blackwood         yahoo.com\n",
       "1943       rhonda.rowan         gmail.com\n",
       "1944             turnip         yahoo.com\n",
       "1945     tommard.heddle         merck.com\n",
       "\n",
       "[1946 rows x 2 columns]"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting personal emails\n",
    "\n",
    "# placeholder list\n",
    "placeholder_lst = []\n",
    "\n",
    "# looping over each email address\n",
    "for index, col in chef.iterrows():\n",
    "    \n",
    "    # splitting email domain at '@'\n",
    "    split_email = chef.loc[index, 'EMAIL'].split(sep = '@')\n",
    "    \n",
    "    # appending placeholder_lst with the results\n",
    "    placeholder_lst.append(split_email)\n",
    "    \n",
    "\n",
    "# converting placeholder_lst into a DataFrame \n",
    "email_df = pd.DataFrame(placeholder_lst)\n",
    "\n",
    "\n",
    "# displaying the results\n",
    "email_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gmail.com           303\n",
       "protonmail.com      284\n",
       "yahoo.com           274\n",
       "msn.com              72\n",
       "aol.com              69\n",
       "passport.com         64\n",
       "hotmail.com          63\n",
       "live.com             62\n",
       "me.com               59\n",
       "amex.com             30\n",
       "merck.com            28\n",
       "mcdonalds.com        28\n",
       "jnj.com              28\n",
       "cocacola.com         28\n",
       "nike.com             27\n",
       "apple.com            27\n",
       "ge.org               26\n",
       "ibm.com              26\n",
       "dupont.com           26\n",
       "microsoft.com        25\n",
       "chevron.com          25\n",
       "unitedhealth.com     24\n",
       "exxon.com            24\n",
       "travelers.com        24\n",
       "boeing.com           23\n",
       "pg.com               22\n",
       "verizon.com          22\n",
       "mmm.com              22\n",
       "caterpillar.com      22\n",
       "walmart.com          21\n",
       "disney.com           21\n",
       "visa.com             20\n",
       "pfizer.com           20\n",
       "jpmorgan.com         19\n",
       "cisco.com            18\n",
       "unitedtech.com       18\n",
       "goldmansacs.com      18\n",
       "intel.com            17\n",
       "homedepot.com        17\n",
       "Name: personal_email_domain, dtype: int64"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#concatenating with original DataFrame\n",
    "\n",
    "# renaming column to concatenate\n",
    "email_df.columns = ['0' , 'personal_email_domain']\n",
    "\n",
    "\n",
    "# concatenating personal_email_domain with friends DataFrame\n",
    "chef = pd.concat([chef, email_df['personal_email_domain']],\n",
    "                     axis = 1)\n",
    "\n",
    "\n",
    "# printing value counts of personal_email_domain\n",
    "chef.loc[: ,'personal_email_domain'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "personal        861\n",
       "professional    696\n",
       "junk            389\n",
       "Name: domain_group, dtype: int64"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "personal_email_domains = ['@gmail.com','@yahoo.com','@protonmail.com']\n",
    "junk_email_domains  = ['@me.com','@aol.com','@hotmail.com','@live.com','@msn.com','@passport.com']\n",
    "\n",
    "# placeholder list\n",
    "placeholder_lst = []\n",
    "\n",
    "\n",
    "# looping to group observations by domain type\n",
    "for domain in chef['personal_email_domain']:\n",
    "    \n",
    "    if '@' + domain in personal_email_domains:\n",
    "        placeholder_lst.append('personal')\n",
    "        \n",
    "\n",
    "    elif '@' + domain in junk_email_domains:\n",
    "        placeholder_lst.append('junk')\n",
    "    \n",
    "    else:\n",
    "        placeholder_lst.append('professional')\n",
    "\n",
    "\n",
    "# concatenating with original DataFrame\n",
    "chef['domain_group'] = pd.Series(placeholder_lst)\n",
    "\n",
    "\n",
    "# checking results\n",
    "chef['domain_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_split_feature(col, df, sep=' ', new_col_name='number_of_names'):\n",
    "    \"\"\"\n",
    "Splits values in a string Series (as part of a DataFrame) and sums the number\n",
    "of resulting items. Automatically appends summed column to original DataFrame.\n",
    "\n",
    "PARAMETERS\n",
    "----------\n",
    "col          : column to split\n",
    "df           : DataFrame where column is located\n",
    "sep          : string sequence to split by, default ' '\n",
    "new_col_name : name of new column after summing split, default\n",
    "               'number_of_names'\n",
    "\"\"\"\n",
    "    \n",
    "    df[new_col_name] = 0\n",
    "    \n",
    "    \n",
    "    for index, val in df.iterrows():\n",
    "        df.loc[index, new_col_name] = len(df.loc[index, col].split(sep = ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     591\n",
       "2    1201\n",
       "3      98\n",
       "4       9\n",
       "5      35\n",
       "6      12\n",
       "Name: number_of_names, dtype: int64"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calling text_split_feature\n",
    "text_split_feature(col = 'NAME',\n",
    "                   df  = chef)\n",
    "\n",
    "\n",
    "# checking results\n",
    "chef['number_of_names'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([                    'REVENUE',          'CROSS_SELL_SUCCESS',\n",
       "                              'NAME',                       'EMAIL',\n",
       "                        'FIRST_NAME',                 'FAMILY_NAME',\n",
       "               'TOTAL_MEALS_ORDERED',          'UNIQUE_MEALS_PURCH',\n",
       "       'CONTACTS_W_CUSTOMER_SERVICE',   'PRODUCT_CATEGORIES_VIEWED',\n",
       "           'AVG_TIME_PER_SITE_VISIT',               'MOBILE_NUMBER',\n",
       "         'CANCELLATIONS_BEFORE_NOON',    'CANCELLATIONS_AFTER_NOON',\n",
       "            'TASTES_AND_PREFERENCES',                   'PC_LOGINS',\n",
       "                     'MOBILE_LOGINS',                 'WEEKLY_PLAN',\n",
       "                  'EARLY_DELIVERIES',             'LATE_DELIVERIES',\n",
       "                    'PACKAGE_LOCKER',         'REFRIGERATED_LOCKER',\n",
       "                 'AVG_PREP_VID_TIME',          'LARGEST_ORDER_SIZE',\n",
       "           'MASTER_CLASSES_ATTENDED',          'MEDIAN_MEAL_RATING',\n",
       "              'AVG_CLICKS_PER_VISIT',         'TOTAL_PHOTOS_VIEWED',\n",
       "             'personal_email_domain',                'domain_group',\n",
       "                   'number_of_names',                             0,\n",
       "                                   1,                             2,\n",
       "                                   3,                        'junk',\n",
       "                          'personal',                'professional'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hot encoding variables\n",
    "skill       = pd.get_dummies(chef['MASTER_CLASSES_ATTENDED'])\n",
    "domain_group = pd.get_dummies(chef['domain_group'])\n",
    "\n",
    "\n",
    "# joining codings together\n",
    "chef = chef.join([skill, domain_group])\n",
    "\n",
    "\n",
    "# checking results\n",
    "chef.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([                    'REVENUE',          'CROSS_SELL_SUCCESS',\n",
       "               'TOTAL_MEALS_ORDERED',          'UNIQUE_MEALS_PURCH',\n",
       "       'CONTACTS_W_CUSTOMER_SERVICE',   'PRODUCT_CATEGORIES_VIEWED',\n",
       "           'AVG_TIME_PER_SITE_VISIT',               'MOBILE_NUMBER',\n",
       "         'CANCELLATIONS_BEFORE_NOON',    'CANCELLATIONS_AFTER_NOON',\n",
       "            'TASTES_AND_PREFERENCES',                   'PC_LOGINS',\n",
       "                     'MOBILE_LOGINS',                 'WEEKLY_PLAN',\n",
       "                  'EARLY_DELIVERIES',             'LATE_DELIVERIES',\n",
       "                    'PACKAGE_LOCKER',         'REFRIGERATED_LOCKER',\n",
       "                 'AVG_PREP_VID_TIME',          'LARGEST_ORDER_SIZE',\n",
       "                'MEDIAN_MEAL_RATING',        'AVG_CLICKS_PER_VISIT',\n",
       "               'TOTAL_PHOTOS_VIEWED',             'number_of_names',\n",
       "                                   0,                             1,\n",
       "                                   2,                             3,\n",
       "                              'junk',                    'personal',\n",
       "                      'professional'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropping unfavorite features and categorical variables after they've been encoded\n",
    "chef = chef.drop(labels = ['FIRST_NAME','FAMILY_NAME','NAME','EMAIL','MASTER_CLASSES_ATTENDED','domain_group','personal_email_domain'],\n",
    "                        axis = 1)\n",
    "\n",
    "# checking results\n",
    "chef.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['REVENUE', 'CROSS_SELL_SUCCESS', 'TOTAL_MEALS_ORDERED',\n",
       "       'UNIQUE_MEALS_PURCH', 'CONTACTS_W_CUSTOMER_SERVICE',\n",
       "       'PRODUCT_CATEGORIES_VIEWED', 'AVG_TIME_PER_SITE_VISIT', 'MOBILE_NUMBER',\n",
       "       'CANCELLATIONS_BEFORE_NOON', 'CANCELLATIONS_AFTER_NOON',\n",
       "       'TASTES_AND_PREFERENCES', 'PC_LOGINS', 'MOBILE_LOGINS', 'WEEKLY_PLAN',\n",
       "       'EARLY_DELIVERIES', 'LATE_DELIVERIES', 'PACKAGE_LOCKER',\n",
       "       'REFRIGERATED_LOCKER', 'AVG_PREP_VID_TIME', 'LARGEST_ORDER_SIZE',\n",
       "       'MEDIAN_MEAL_RATING', 'AVG_CLICKS_PER_VISIT', 'TOTAL_PHOTOS_VIEWED',\n",
       "       'number_of_names', 'skill_0', 'skill_1', 'skill_2', 'skill_3', 'junk',\n",
       "       'personal', 'professional'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# relabeling columns\n",
    "chef.columns = [                    'REVENUE',          'CROSS_SELL_SUCCESS',\n",
    "               'TOTAL_MEALS_ORDERED',          'UNIQUE_MEALS_PURCH',\n",
    "       'CONTACTS_W_CUSTOMER_SERVICE',   'PRODUCT_CATEGORIES_VIEWED',\n",
    "           'AVG_TIME_PER_SITE_VISIT',               'MOBILE_NUMBER',\n",
    "         'CANCELLATIONS_BEFORE_NOON',    'CANCELLATIONS_AFTER_NOON',\n",
    "            'TASTES_AND_PREFERENCES',                   'PC_LOGINS',\n",
    "                     'MOBILE_LOGINS',                 'WEEKLY_PLAN',\n",
    "                  'EARLY_DELIVERIES',             'LATE_DELIVERIES',\n",
    "                    'PACKAGE_LOCKER',         'REFRIGERATED_LOCKER',\n",
    "                 'AVG_PREP_VID_TIME',          'LARGEST_ORDER_SIZE',          'MEDIAN_MEAL_RATING',\n",
    "              'AVG_CLICKS_PER_VISIT',  'TOTAL_PHOTOS_VIEWED',        'number_of_names',\n",
    "                'skill_0','skill_1','skill_2','skill_3',\n",
    "                              'junk',                    'personal',\n",
    "                      'professional']\n",
    "\n",
    "\n",
    "# checking results\n",
    "chef.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# optimal_neighbors\n",
    "########################################\n",
    "def optimal_neighbors(X_data,\n",
    "                      y_data,\n",
    "                      standardize = True,\n",
    "                      pct_test=0.25,\n",
    "                      seed=219,\n",
    "                      response_type='reg',\n",
    "                      max_neighbors=20,\n",
    "                      show_viz=True):\n",
    "    \"\"\"\n",
    "Exhaustively compute training and testing results for KNN across\n",
    "[1, max_neighbors]. Outputs the maximum test score and (by default) a\n",
    "visualization of the results.\n",
    "PARAMETERS\n",
    "----------\n",
    "X_data        : explanatory variable data\n",
    "y_data        : response variable\n",
    "standardize   : whether or not to standardize the X data, default True\n",
    "pct_test      : test size for training and validation from (0,1), default 0.25\n",
    "seed          : random seed to be used in algorithm, default 219\n",
    "response_type : type of neighbors algorithm to use, default 'reg'\n",
    "    Use 'reg' for regression (KNeighborsRegressor)\n",
    "    Use 'class' for classification (KNeighborsClassifier)\n",
    "max_neighbors : maximum number of neighbors in exhaustive search, default 20\n",
    "show_viz      : display or surpress k-neigbors visualization, default True\n",
    "\"\"\"    \n",
    "    \n",
    "    \n",
    "    if standardize == True:\n",
    "        # optionally standardizing X_data\n",
    "        scaler             = StandardScaler()\n",
    "        scaler.fit(X_data)\n",
    "        X_scaled           = scaler.transform(X_data)\n",
    "        X_scaled_df        = pd.DataFrame(X_scaled)\n",
    "        X_data             = X_scaled_df\n",
    "\n",
    "\n",
    "\n",
    "    # train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_data,\n",
    "                                                        y_data,\n",
    "                                                        test_size = pct_test,\n",
    "                                                        random_state = seed)\n",
    "\n",
    "\n",
    "    # creating lists for training set accuracy and test set accuracy\n",
    "    training_accuracy = []\n",
    "    test_accuracy = []\n",
    "    \n",
    "    \n",
    "    # setting neighbor range\n",
    "    neighbors_settings = range(1, max_neighbors + 1)\n",
    "\n",
    "\n",
    "    for n_neighbors in neighbors_settings:\n",
    "        # building the model based on response variable type\n",
    "        if response_type == 'reg':\n",
    "            clf = KNeighborsRegressor(n_neighbors = n_neighbors)\n",
    "            clf.fit(X_train, y_train)\n",
    "            \n",
    "        elif response_type == 'class':\n",
    "            clf = KNeighborsClassifier(n_neighbors = n_neighbors)\n",
    "            clf.fit(X_train, y_train)            \n",
    "            \n",
    "        else:\n",
    "            print(\"Error: response_type must be 'reg' or 'class'\")\n",
    "        \n",
    "        \n",
    "        # recording the training set accuracy\n",
    "        training_accuracy.append(clf.score(X_train, y_train))\n",
    "    \n",
    "        # recording the generalization accuracy\n",
    "        test_accuracy.append(clf.score(X_test, y_test))\n",
    "\n",
    "\n",
    "    # optionally displaying visualization\n",
    "    if show_viz == True:\n",
    "        # plotting the visualization\n",
    "        fig, ax = plt.subplots(figsize=(12,8))\n",
    "        plt.plot(neighbors_settings, training_accuracy, label = \"training accuracy\")\n",
    "        plt.plot(neighbors_settings, test_accuracy, label = \"test accuracy\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.xlabel(\"n_neighbors\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    # returning optimal number of neighbors\n",
    "    print(f\"The optimal number of neighbors is: {test_accuracy.index(max(test_accuracy))+1}\")\n",
    "    return test_accuracy.index(max(test_accuracy))+1\n",
    "\n",
    "\n",
    "########################################\n",
    "# visual_cm\n",
    "########################################\n",
    "def visual_cm(true_y, pred_y, labels = None):\n",
    "    \"\"\"\n",
    "Creates a visualization of a confusion matrix.\n",
    "\n",
    "PARAMETERS\n",
    "----------\n",
    "true_y : true values for the response variable\n",
    "pred_y : predicted values for the response variable\n",
    "labels : , default None\n",
    "    \"\"\"\n",
    "    # visualizing the confusion matrix\n",
    "\n",
    "    # setting labels\n",
    "    lbls = labels\n",
    "    \n",
    "\n",
    "    # declaring a confusion matrix object\n",
    "    cm = confusion_matrix(y_true = true_y,\n",
    "                          y_pred = pred_y)\n",
    "\n",
    "\n",
    "    # heatmap\n",
    "    sns.heatmap(cm,\n",
    "                annot       = True,\n",
    "                xticklabels = lbls,\n",
    "                yticklabels = lbls,\n",
    "                cmap        = 'Blues',\n",
    "                fmt         = 'g')\n",
    "\n",
    "\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix of the Classifier')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CROSS_SELL_SUCCESS             1.00\n",
       "professional                   0.19\n",
       "number_of_names                0.16\n",
       "CANCELLATIONS_BEFORE_NOON      0.16\n",
       "MOBILE_NUMBER                  0.10\n",
       "TASTES_AND_PREFERENCES         0.08\n",
       "REFRIGERATED_LOCKER            0.07\n",
       "skill_1                        0.05\n",
       "CONTACTS_W_CUSTOMER_SERVICE    0.04\n",
       "PC_LOGINS                      0.04\n",
       "PACKAGE_LOCKER                 0.04\n",
       "personal                       0.04\n",
       "MEDIAN_MEAL_RATING             0.03\n",
       "AVG_PREP_VID_TIME              0.03\n",
       "LARGEST_ORDER_SIZE             0.02\n",
       "EARLY_DELIVERIES               0.02\n",
       "AVG_TIME_PER_SITE_VISIT        0.01\n",
       "TOTAL_MEALS_ORDERED            0.01\n",
       "LATE_DELIVERIES                0.01\n",
       "TOTAL_PHOTOS_VIEWED            0.01\n",
       "skill_2                        0.01\n",
       "PRODUCT_CATEGORIES_VIEWED      0.00\n",
       "UNIQUE_MEALS_PURCH             0.00\n",
       "REVENUE                        0.00\n",
       "WEEKLY_PLAN                   -0.01\n",
       "AVG_CLICKS_PER_VISIT          -0.04\n",
       "skill_3                       -0.04\n",
       "MOBILE_LOGINS                 -0.05\n",
       "CANCELLATIONS_AFTER_NOON      -0.05\n",
       "skill_0                       -0.05\n",
       "junk                          -0.28\n",
       "Name: CROSS_SELL_SUCCESS, dtype: float64"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corr = chef.corr().round(2)\n",
    "\n",
    "df_corr['CROSS_SELL_SUCCESS'].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring explanatory variables\n",
    "chef_data = chef.drop('CROSS_SELL_SUCCESS', axis = 1)\n",
    "\n",
    "\n",
    "# declaring response variable\n",
    "chef_target = chef.loc[ : , 'CROSS_SELL_SUCCESS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REVENUE</th>\n",
       "      <th>CROSS_SELL_SUCCESS</th>\n",
       "      <th>TOTAL_MEALS_ORDERED</th>\n",
       "      <th>UNIQUE_MEALS_PURCH</th>\n",
       "      <th>CONTACTS_W_CUSTOMER_SERVICE</th>\n",
       "      <th>PRODUCT_CATEGORIES_VIEWED</th>\n",
       "      <th>AVG_TIME_PER_SITE_VISIT</th>\n",
       "      <th>MOBILE_NUMBER</th>\n",
       "      <th>CANCELLATIONS_BEFORE_NOON</th>\n",
       "      <th>CANCELLATIONS_AFTER_NOON</th>\n",
       "      <th>...</th>\n",
       "      <th>AVG_CLICKS_PER_VISIT</th>\n",
       "      <th>TOTAL_PHOTOS_VIEWED</th>\n",
       "      <th>number_of_names</th>\n",
       "      <th>skill_0</th>\n",
       "      <th>skill_1</th>\n",
       "      <th>skill_2</th>\n",
       "      <th>skill_3</th>\n",
       "      <th>junk</th>\n",
       "      <th>personal</th>\n",
       "      <th>professional</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1946.000000</td>\n",
       "      <td>1946.000000</td>\n",
       "      <td>1946.000000</td>\n",
       "      <td>1946.000000</td>\n",
       "      <td>1946.000000</td>\n",
       "      <td>1946.000000</td>\n",
       "      <td>1946.000000</td>\n",
       "      <td>1946.000000</td>\n",
       "      <td>1946.000000</td>\n",
       "      <td>1946.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1946.000000</td>\n",
       "      <td>1946.000000</td>\n",
       "      <td>1946.000000</td>\n",
       "      <td>1946.000000</td>\n",
       "      <td>1946.000000</td>\n",
       "      <td>1946.000000</td>\n",
       "      <td>1946.000000</td>\n",
       "      <td>1946.000000</td>\n",
       "      <td>1946.000000</td>\n",
       "      <td>1946.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2107.292652</td>\n",
       "      <td>0.678828</td>\n",
       "      <td>74.634121</td>\n",
       "      <td>4.904933</td>\n",
       "      <td>6.983556</td>\n",
       "      <td>5.383864</td>\n",
       "      <td>99.604651</td>\n",
       "      <td>0.877698</td>\n",
       "      <td>1.404933</td>\n",
       "      <td>0.165982</td>\n",
       "      <td>...</td>\n",
       "      <td>13.508222</td>\n",
       "      <td>106.433710</td>\n",
       "      <td>1.834532</td>\n",
       "      <td>0.478931</td>\n",
       "      <td>0.440904</td>\n",
       "      <td>0.077081</td>\n",
       "      <td>0.003083</td>\n",
       "      <td>0.199897</td>\n",
       "      <td>0.442446</td>\n",
       "      <td>0.357657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1138.290709</td>\n",
       "      <td>0.467047</td>\n",
       "      <td>55.309782</td>\n",
       "      <td>2.502175</td>\n",
       "      <td>2.281193</td>\n",
       "      <td>3.044001</td>\n",
       "      <td>62.341756</td>\n",
       "      <td>0.327719</td>\n",
       "      <td>1.549677</td>\n",
       "      <td>0.432241</td>\n",
       "      <td>...</td>\n",
       "      <td>2.333876</td>\n",
       "      <td>181.014124</td>\n",
       "      <td>0.778476</td>\n",
       "      <td>0.499684</td>\n",
       "      <td>0.496623</td>\n",
       "      <td>0.266789</td>\n",
       "      <td>0.055456</td>\n",
       "      <td>0.400026</td>\n",
       "      <td>0.496804</td>\n",
       "      <td>0.479434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>131.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.330000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1350.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1740.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>94.160000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2670.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>117.287500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8793.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>493.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1645.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           REVENUE  CROSS_SELL_SUCCESS  TOTAL_MEALS_ORDERED  \\\n",
       "count  1946.000000         1946.000000          1946.000000   \n",
       "mean   2107.292652            0.678828            74.634121   \n",
       "std    1138.290709            0.467047            55.309782   \n",
       "min     131.000000            0.000000            11.000000   \n",
       "25%    1350.000000            0.000000            39.000000   \n",
       "50%    1740.000000            1.000000            60.000000   \n",
       "75%    2670.000000            1.000000            95.000000   \n",
       "max    8793.750000            1.000000           493.000000   \n",
       "\n",
       "       UNIQUE_MEALS_PURCH  CONTACTS_W_CUSTOMER_SERVICE  \\\n",
       "count         1946.000000                  1946.000000   \n",
       "mean             4.904933                     6.983556   \n",
       "std              2.502175                     2.281193   \n",
       "min              1.000000                     1.000000   \n",
       "25%              3.000000                     5.000000   \n",
       "50%              5.000000                     7.000000   \n",
       "75%              7.000000                     8.000000   \n",
       "max             19.000000                    18.000000   \n",
       "\n",
       "       PRODUCT_CATEGORIES_VIEWED  AVG_TIME_PER_SITE_VISIT  MOBILE_NUMBER  \\\n",
       "count                1946.000000              1946.000000    1946.000000   \n",
       "mean                    5.383864                99.604651       0.877698   \n",
       "std                     3.044001                62.341756       0.327719   \n",
       "min                     1.000000                10.330000       0.000000   \n",
       "25%                     3.000000                72.000000       1.000000   \n",
       "50%                     5.000000                94.160000       1.000000   \n",
       "75%                     8.000000               117.287500       1.000000   \n",
       "max                    10.000000              1645.600000       1.000000   \n",
       "\n",
       "       CANCELLATIONS_BEFORE_NOON  CANCELLATIONS_AFTER_NOON  ...  \\\n",
       "count                1946.000000               1946.000000  ...   \n",
       "mean                    1.404933                  0.165982  ...   \n",
       "std                     1.549677                  0.432241  ...   \n",
       "min                     0.000000                  0.000000  ...   \n",
       "25%                     0.000000                  0.000000  ...   \n",
       "50%                     1.000000                  0.000000  ...   \n",
       "75%                     2.000000                  0.000000  ...   \n",
       "max                    13.000000                  3.000000  ...   \n",
       "\n",
       "       AVG_CLICKS_PER_VISIT  TOTAL_PHOTOS_VIEWED  number_of_names  \\\n",
       "count           1946.000000          1946.000000      1946.000000   \n",
       "mean              13.508222           106.433710         1.834532   \n",
       "std                2.333876           181.014124         0.778476   \n",
       "min                5.000000             0.000000         1.000000   \n",
       "25%               12.000000             0.000000         1.000000   \n",
       "50%               13.000000             0.000000         2.000000   \n",
       "75%               15.000000           174.000000         2.000000   \n",
       "max               19.000000          1600.000000         6.000000   \n",
       "\n",
       "           skill_0      skill_1      skill_2      skill_3         junk  \\\n",
       "count  1946.000000  1946.000000  1946.000000  1946.000000  1946.000000   \n",
       "mean      0.478931     0.440904     0.077081     0.003083     0.199897   \n",
       "std       0.499684     0.496623     0.266789     0.055456     0.400026   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       1.000000     1.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "          personal  professional  \n",
       "count  1946.000000   1946.000000  \n",
       "mean      0.442446      0.357657  \n",
       "std       0.496804      0.479434  \n",
       "min       0.000000      0.000000  \n",
       "25%       0.000000      0.000000  \n",
       "50%       0.000000      0.000000  \n",
       "75%       1.000000      1.000000  \n",
       "max       1.000000      1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chef.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            chef_data,\n",
    "            chef_target,\n",
    "            test_size    = 0.25,\n",
    "            random_state = 219,\n",
    "            stratify     = chef_target)\n",
    "\n",
    "\n",
    "# merging training data for statsmodels\n",
    "chef_train = pd.concat([X_train, y_train], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Response Variable Proportions (Training Set)\n",
      "--------------------------------------------\n",
      "1    0.68\n",
      "0    0.32\n",
      "Name: CROSS_SELL_SUCCESS, dtype: float64\n",
      "\n",
      "\n",
      "\n",
      "Response Variable Proportions (Testing Set)\n",
      "--------------------------------------------\n",
      "1    0.68\n",
      "0    0.32\n",
      "Name: CROSS_SELL_SUCCESS, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "\n",
    "Response Variable Proportions (Training Set)\n",
    "--------------------------------------------\n",
    "{y_train.value_counts(normalize = True).round(decimals = 2)}\n",
    "\n",
    "\n",
    "\n",
    "Response Variable Proportions (Testing Set)\n",
    "--------------------------------------------\n",
    "{y_test.value_counts(normalize = True).round(decimals = 2)}\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " REVENUE + \n",
      " TOTAL_MEALS_ORDERED + \n",
      " UNIQUE_MEALS_PURCH + \n",
      " CONTACTS_W_CUSTOMER_SERVICE + \n",
      " PRODUCT_CATEGORIES_VIEWED + \n",
      " AVG_TIME_PER_SITE_VISIT + \n",
      " MOBILE_NUMBER + \n",
      " CANCELLATIONS_BEFORE_NOON + \n",
      " CANCELLATIONS_AFTER_NOON + \n",
      " TASTES_AND_PREFERENCES + \n",
      " PC_LOGINS + \n",
      " MOBILE_LOGINS + \n",
      " WEEKLY_PLAN + \n",
      " EARLY_DELIVERIES + \n",
      " LATE_DELIVERIES + \n",
      " PACKAGE_LOCKER + \n",
      " REFRIGERATED_LOCKER + \n",
      " AVG_PREP_VID_TIME + \n",
      " LARGEST_ORDER_SIZE + \n",
      " MEDIAN_MEAL_RATING + \n",
      " AVG_CLICKS_PER_VISIT + \n",
      " TOTAL_PHOTOS_VIEWED + \n",
      " number_of_names + \n",
      " skill_0 + \n",
      " skill_1 + \n",
      " skill_2 + \n",
      " skill_3 + \n",
      " junk + \n",
      " personal + \n",
      " professional + \n"
     ]
    }
   ],
   "source": [
    "for val in chef_data:\n",
    "    print(f\" {val} + \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.533805\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>   <td>CROSS_SELL_SUCCESS</td> <th>  No. Observations:  </th>  <td>  1459</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                  <td>Logit</td>       <th>  Df Residuals:      </th>  <td>  1431</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                  <td>MLE</td>        <th>  Df Model:          </th>  <td>    27</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 26 Jan 2021</td>  <th>  Pseudo R-squ.:     </th>  <td>0.1499</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:28:26</td>      <th>  Log-Likelihood:    </th> <td> -778.82</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>              <td>True</td>        <th>  LL-Null:           </th> <td> -916.19</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>     <th>  LLR p-value:       </th> <td>7.443e-43</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "               <td></td>                  <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                   <td>   -2.2156</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>REVENUE</th>                     <td>   -0.0002</td> <td> 8.89e-05</td> <td>   -2.394</td> <td> 0.017</td> <td>   -0.000</td> <td>-3.86e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TOTAL_MEALS_ORDERED</th>         <td>   -0.0005</td> <td>    0.001</td> <td>   -0.323</td> <td> 0.747</td> <td>   -0.003</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UNIQUE_MEALS_PURCH</th>          <td>   -0.0113</td> <td>    0.026</td> <td>   -0.431</td> <td> 0.666</td> <td>   -0.063</td> <td>    0.040</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CONTACTS_W_CUSTOMER_SERVICE</th> <td>    0.0507</td> <td>    0.028</td> <td>    1.796</td> <td> 0.073</td> <td>   -0.005</td> <td>    0.106</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PRODUCT_CATEGORIES_VIEWED</th>   <td>   -0.0175</td> <td>    0.021</td> <td>   -0.849</td> <td> 0.396</td> <td>   -0.058</td> <td>    0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AVG_TIME_PER_SITE_VISIT</th>     <td>    0.0003</td> <td>    0.001</td> <td>    0.301</td> <td> 0.763</td> <td>   -0.002</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>MOBILE_NUMBER</th>               <td>    0.8987</td> <td>    0.179</td> <td>    5.023</td> <td> 0.000</td> <td>    0.548</td> <td>    1.249</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CANCELLATIONS_BEFORE_NOON</th>   <td>    0.2795</td> <td>    0.047</td> <td>    5.947</td> <td> 0.000</td> <td>    0.187</td> <td>    0.372</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CANCELLATIONS_AFTER_NOON</th>    <td>   -0.2471</td> <td>    0.144</td> <td>   -1.721</td> <td> 0.085</td> <td>   -0.528</td> <td>    0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TASTES_AND_PREFERENCES</th>      <td>    0.3622</td> <td>    0.137</td> <td>    2.641</td> <td> 0.008</td> <td>    0.093</td> <td>    0.631</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PC_LOGINS</th>                   <td>    0.2386</td> <td>    0.109</td> <td>    2.195</td> <td> 0.028</td> <td>    0.026</td> <td>    0.452</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>MOBILE_LOGINS</th>               <td>   -0.1849</td> <td>    0.119</td> <td>   -1.552</td> <td> 0.121</td> <td>   -0.419</td> <td>    0.049</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>WEEKLY_PLAN</th>                 <td>    0.0056</td> <td>    0.005</td> <td>    1.179</td> <td> 0.238</td> <td>   -0.004</td> <td>    0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>EARLY_DELIVERIES</th>            <td>    0.0659</td> <td>    0.028</td> <td>    2.353</td> <td> 0.019</td> <td>    0.011</td> <td>    0.121</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LATE_DELIVERIES</th>             <td>    0.0163</td> <td>    0.023</td> <td>    0.713</td> <td> 0.476</td> <td>   -0.028</td> <td>    0.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PACKAGE_LOCKER</th>              <td>    0.0328</td> <td>    0.149</td> <td>    0.221</td> <td> 0.825</td> <td>   -0.259</td> <td>    0.325</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>REFRIGERATED_LOCKER</th>         <td>    0.4995</td> <td>    0.239</td> <td>    2.094</td> <td> 0.036</td> <td>    0.032</td> <td>    0.967</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AVG_PREP_VID_TIME</th>           <td>    0.0050</td> <td>    0.003</td> <td>    1.904</td> <td> 0.057</td> <td>   -0.000</td> <td>    0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LARGEST_ORDER_SIZE</th>          <td>   -0.0773</td> <td>    0.070</td> <td>   -1.106</td> <td> 0.269</td> <td>   -0.214</td> <td>    0.060</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>MEDIAN_MEAL_RATING</th>          <td>    0.1330</td> <td>    0.170</td> <td>    0.780</td> <td> 0.435</td> <td>   -0.201</td> <td>    0.467</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AVG_CLICKS_PER_VISIT</th>        <td>   -0.0185</td> <td>    0.052</td> <td>   -0.358</td> <td> 0.721</td> <td>   -0.120</td> <td>    0.083</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>number_of_names</th>             <td>    0.5530</td> <td>    0.095</td> <td>    5.835</td> <td> 0.000</td> <td>    0.367</td> <td>    0.739</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>skill_0</th>                     <td>   -0.4989</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>skill_1</th>                     <td>   -0.2055</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>skill_2</th>                     <td>   -0.1875</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>skill_3</th>                     <td>   -1.3238</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>junk</th>                        <td>   -1.8436</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>personal</th>                    <td>   -0.4943</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>professional</th>                <td>    0.1223</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:     CROSS_SELL_SUCCESS   No. Observations:                 1459\n",
       "Model:                          Logit   Df Residuals:                     1431\n",
       "Method:                           MLE   Df Model:                           27\n",
       "Date:                Tue, 26 Jan 2021   Pseudo R-squ.:                  0.1499\n",
       "Time:                        23:28:26   Log-Likelihood:                -778.82\n",
       "converged:                       True   LL-Null:                       -916.19\n",
       "Covariance Type:            nonrobust   LLR p-value:                 7.443e-43\n",
       "===============================================================================================\n",
       "                                  coef    std err          z      P>|z|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------------------\n",
       "Intercept                      -2.2156        nan        nan        nan         nan         nan\n",
       "REVENUE                        -0.0002   8.89e-05     -2.394      0.017      -0.000   -3.86e-05\n",
       "TOTAL_MEALS_ORDERED            -0.0005      0.001     -0.323      0.747      -0.003       0.002\n",
       "UNIQUE_MEALS_PURCH             -0.0113      0.026     -0.431      0.666      -0.063       0.040\n",
       "CONTACTS_W_CUSTOMER_SERVICE     0.0507      0.028      1.796      0.073      -0.005       0.106\n",
       "PRODUCT_CATEGORIES_VIEWED      -0.0175      0.021     -0.849      0.396      -0.058       0.023\n",
       "AVG_TIME_PER_SITE_VISIT         0.0003      0.001      0.301      0.763      -0.002       0.002\n",
       "MOBILE_NUMBER                   0.8987      0.179      5.023      0.000       0.548       1.249\n",
       "CANCELLATIONS_BEFORE_NOON       0.2795      0.047      5.947      0.000       0.187       0.372\n",
       "CANCELLATIONS_AFTER_NOON       -0.2471      0.144     -1.721      0.085      -0.528       0.034\n",
       "TASTES_AND_PREFERENCES          0.3622      0.137      2.641      0.008       0.093       0.631\n",
       "PC_LOGINS                       0.2386      0.109      2.195      0.028       0.026       0.452\n",
       "MOBILE_LOGINS                  -0.1849      0.119     -1.552      0.121      -0.419       0.049\n",
       "WEEKLY_PLAN                     0.0056      0.005      1.179      0.238      -0.004       0.015\n",
       "EARLY_DELIVERIES                0.0659      0.028      2.353      0.019       0.011       0.121\n",
       "LATE_DELIVERIES                 0.0163      0.023      0.713      0.476      -0.028       0.061\n",
       "PACKAGE_LOCKER                  0.0328      0.149      0.221      0.825      -0.259       0.325\n",
       "REFRIGERATED_LOCKER             0.4995      0.239      2.094      0.036       0.032       0.967\n",
       "AVG_PREP_VID_TIME               0.0050      0.003      1.904      0.057      -0.000       0.010\n",
       "LARGEST_ORDER_SIZE             -0.0773      0.070     -1.106      0.269      -0.214       0.060\n",
       "MEDIAN_MEAL_RATING              0.1330      0.170      0.780      0.435      -0.201       0.467\n",
       "AVG_CLICKS_PER_VISIT           -0.0185      0.052     -0.358      0.721      -0.120       0.083\n",
       "number_of_names                 0.5530      0.095      5.835      0.000       0.367       0.739\n",
       "skill_0                        -0.4989        nan        nan        nan         nan         nan\n",
       "skill_1                        -0.2055        nan        nan        nan         nan         nan\n",
       "skill_2                        -0.1875        nan        nan        nan         nan         nan\n",
       "skill_3                        -1.3238        nan        nan        nan         nan         nan\n",
       "junk                           -1.8436        nan        nan        nan         nan         nan\n",
       "personal                       -0.4943        nan        nan        nan         nan         nan\n",
       "professional                    0.1223        nan        nan        nan         nan         nan\n",
       "===============================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiating a logistic regression model object\n",
    "logistic_full = smf.logit(formula = \"\"\"  CROSS_SELL_SUCCESS ~\n",
    "REVENUE + \n",
    " TOTAL_MEALS_ORDERED + \n",
    " UNIQUE_MEALS_PURCH + \n",
    " CONTACTS_W_CUSTOMER_SERVICE + \n",
    " PRODUCT_CATEGORIES_VIEWED + \n",
    " AVG_TIME_PER_SITE_VISIT + \n",
    " MOBILE_NUMBER + \n",
    " CANCELLATIONS_BEFORE_NOON + \n",
    " CANCELLATIONS_AFTER_NOON + \n",
    " TASTES_AND_PREFERENCES + \n",
    " PC_LOGINS + \n",
    " MOBILE_LOGINS + \n",
    " WEEKLY_PLAN + \n",
    " EARLY_DELIVERIES + \n",
    " LATE_DELIVERIES + \n",
    " PACKAGE_LOCKER + \n",
    " REFRIGERATED_LOCKER + \n",
    " AVG_PREP_VID_TIME + \n",
    " LARGEST_ORDER_SIZE + \n",
    " MEDIAN_MEAL_RATING + \n",
    " AVG_CLICKS_PER_VISIT + \n",
    " number_of_names + \n",
    " skill_0 + \n",
    " skill_1 + \n",
    " skill_2 + \n",
    " skill_3 + \n",
    " junk + \n",
    " personal + \n",
    " professional\"\"\",\n",
    "                                         data    = chef_train)\n",
    "\n",
    "\n",
    "# fitting the model object\n",
    "results_full = logistic_full.fit()\n",
    "\n",
    "\n",
    "# checking the results SUMMARY\n",
    "results_full.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Develop a model where all features are significant based on their p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.590401\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>   <td>CROSS_SELL_SUCCESS</td> <th>  No. Observations:  </th>  <td>  1459</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                  <td>Logit</td>       <th>  Df Residuals:      </th>  <td>  1450</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                  <td>MLE</td>        <th>  Df Model:          </th>  <td>     8</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 26 Jan 2021</td>  <th>  Pseudo R-squ.:     </th>  <td>0.05981</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:28:27</td>      <th>  Log-Likelihood:    </th> <td> -861.40</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>              <td>True</td>        <th>  LL-Null:           </th> <td> -916.19</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>     <th>  LLR p-value:       </th> <td>4.609e-20</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "              <td></td>                 <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                 <td>   -2.5476</td> <td>    0.618</td> <td>   -4.120</td> <td> 0.000</td> <td>   -3.760</td> <td>   -1.336</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>MOBILE_NUMBER</th>             <td>    0.7225</td> <td>    0.168</td> <td>    4.308</td> <td> 0.000</td> <td>    0.394</td> <td>    1.051</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>REVENUE</th>                   <td> 3.549e-05</td> <td> 5.25e-05</td> <td>    0.676</td> <td> 0.499</td> <td>-6.75e-05</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CANCELLATIONS_BEFORE_NOON</th> <td>    0.2624</td> <td>    0.045</td> <td>    5.842</td> <td> 0.000</td> <td>    0.174</td> <td>    0.350</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TASTES_AND_PREFERENCES</th>    <td>    0.3586</td> <td>    0.128</td> <td>    2.812</td> <td> 0.005</td> <td>    0.109</td> <td>    0.609</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PC_LOGINS</th>                 <td>    0.1893</td> <td>    0.101</td> <td>    1.877</td> <td> 0.060</td> <td>   -0.008</td> <td>    0.387</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>EARLY_DELIVERIES</th>          <td>    0.0506</td> <td>    0.026</td> <td>    1.955</td> <td> 0.051</td> <td>   -0.000</td> <td>    0.101</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>REFRIGERATED_LOCKER</th>       <td>    0.4659</td> <td>    0.199</td> <td>    2.340</td> <td> 0.019</td> <td>    0.076</td> <td>    0.856</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>number_of_names</th>           <td>    0.4743</td> <td>    0.088</td> <td>    5.364</td> <td> 0.000</td> <td>    0.301</td> <td>    0.648</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:     CROSS_SELL_SUCCESS   No. Observations:                 1459\n",
       "Model:                          Logit   Df Residuals:                     1450\n",
       "Method:                           MLE   Df Model:                            8\n",
       "Date:                Tue, 26 Jan 2021   Pseudo R-squ.:                 0.05981\n",
       "Time:                        23:28:27   Log-Likelihood:                -861.40\n",
       "converged:                       True   LL-Null:                       -916.19\n",
       "Covariance Type:            nonrobust   LLR p-value:                 4.609e-20\n",
       "=============================================================================================\n",
       "                                coef    std err          z      P>|z|      [0.025      0.975]\n",
       "---------------------------------------------------------------------------------------------\n",
       "Intercept                    -2.5476      0.618     -4.120      0.000      -3.760      -1.336\n",
       "MOBILE_NUMBER                 0.7225      0.168      4.308      0.000       0.394       1.051\n",
       "REVENUE                    3.549e-05   5.25e-05      0.676      0.499   -6.75e-05       0.000\n",
       "CANCELLATIONS_BEFORE_NOON     0.2624      0.045      5.842      0.000       0.174       0.350\n",
       "TASTES_AND_PREFERENCES        0.3586      0.128      2.812      0.005       0.109       0.609\n",
       "PC_LOGINS                     0.1893      0.101      1.877      0.060      -0.008       0.387\n",
       "EARLY_DELIVERIES              0.0506      0.026      1.955      0.051      -0.000       0.101\n",
       "REFRIGERATED_LOCKER           0.4659      0.199      2.340      0.019       0.076       0.856\n",
       "number_of_names               0.4743      0.088      5.364      0.000       0.301       0.648\n",
       "=============================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiating a logistic regression model object\n",
    "logit_sig = smf.logit(formula = \"\"\" CROSS_SELL_SUCCESS ~\n",
    "MOBILE_NUMBER+\n",
    "                                    REVENUE +\n",
    "                                    CANCELLATIONS_BEFORE_NOON +\n",
    "                                    TASTES_AND_PREFERENCES +\n",
    "                                    PC_LOGINS +\n",
    "                                    EARLY_DELIVERIES+\n",
    "                                    REFRIGERATED_LOCKER+\n",
    "                                    number_of_names\n",
    "                                    \"\"\",\n",
    "                                    data = chef_train)\n",
    "\n",
    "\n",
    "# fitting the model object\n",
    "logit_sig = logit_sig.fit()\n",
    "\n",
    "\n",
    "# checking the results SUMMARY\n",
    "logit_sig.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explanatory sets from last session\n",
    "\n",
    "# creating a dictionary to store candidate models\n",
    "\n",
    "candidate_dict = {\n",
    "\n",
    " # full model\n",
    "        'logit_full'  :        ['TOTAL_MEALS_ORDERED',          'UNIQUE_MEALS_PURCH',\n",
    "       'CONTACTS_W_CUSTOMER_SERVICE',   'PRODUCT_CATEGORIES_VIEWED',\n",
    "           'AVG_TIME_PER_SITE_VISIT',               'MOBILE_NUMBER',\n",
    "         'CANCELLATIONS_BEFORE_NOON',    'CANCELLATIONS_AFTER_NOON',\n",
    "            'TASTES_AND_PREFERENCES',                   'PC_LOGINS',\n",
    "                     'MOBILE_LOGINS',                 'WEEKLY_PLAN',\n",
    "                  'EARLY_DELIVERIES',             'LATE_DELIVERIES',\n",
    "                    'PACKAGE_LOCKER',         'REFRIGERATED_LOCKER',\n",
    "                 'AVG_PREP_VID_TIME',          'LARGEST_ORDER_SIZE',          'MEDIAN_MEAL_RATING',\n",
    "              'AVG_CLICKS_PER_VISIT',          'number_of_names',\n",
    "                'skill_0','skill_1','skill_2','skill_3',\n",
    "                              'junk',                    'personal',\n",
    "                      'professional'],\n",
    " \n",
    "\n",
    " # significant variables only (set 1)\n",
    " 'logit_sig'    : ['MOBILE_NUMBER', 'CANCELLATIONS_BEFORE_NOON' ,\n",
    "                                    'TASTES_AND_PREFERENCES' ,\n",
    "                                    'number_of_names'\n",
    "                                    ],\n",
    "    \n",
    "    \n",
    " # significant variables only (set 2)\n",
    " 'logit_sig_2'  :  ['REVENUE' ,'MOBILE_NUMBER', 'CANCELLATIONS_BEFORE_NOON' ,\n",
    "                                    'TASTES_AND_PREFERENCES' ,\n",
    "                                    'PC_LOGINS' ,\n",
    "                                    'EARLY_DELIVERIES',\n",
    "                                    'REFRIGERATED_LOCKER',\n",
    "                                    'number_of_names','skill_0','skill_1','skill_2','skill_3',  'junk',                    'personal',\n",
    "                      'professional'\n",
    "                                    ]\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/--------------------------\\\n",
      "|Explanatory Variable Sets |\n",
      "\\--------------------------/\n",
      "\n",
      "Full Model:\n",
      "-----------\n",
      "['TOTAL_MEALS_ORDERED', 'UNIQUE_MEALS_PURCH', 'CONTACTS_W_CUSTOMER_SERVICE', 'PRODUCT_CATEGORIES_VIEWED', 'AVG_TIME_PER_SITE_VISIT', 'MOBILE_NUMBER', 'CANCELLATIONS_BEFORE_NOON', 'CANCELLATIONS_AFTER_NOON', 'TASTES_AND_PREFERENCES', 'PC_LOGINS', 'MOBILE_LOGINS', 'WEEKLY_PLAN', 'EARLY_DELIVERIES', 'LATE_DELIVERIES', 'PACKAGE_LOCKER', 'REFRIGERATED_LOCKER', 'AVG_PREP_VID_TIME', 'LARGEST_ORDER_SIZE', 'MEDIAN_MEAL_RATING', 'AVG_CLICKS_PER_VISIT', 'number_of_names', 'skill_0', 'skill_1', 'skill_2', 'skill_3', 'junk', 'personal', 'professional']\n",
      "\n",
      "\n",
      "First Significant p-value Model:\n",
      "--------------------------------\n",
      "['MOBILE_NUMBER', 'CANCELLATIONS_BEFORE_NOON', 'TASTES_AND_PREFERENCES', 'number_of_names']\n",
      "\n",
      "\n",
      "Second Significant p-value Model:\n",
      "---------------------------------\n",
      "['REVENUE', 'MOBILE_NUMBER', 'CANCELLATIONS_BEFORE_NOON', 'TASTES_AND_PREFERENCES', 'PC_LOGINS', 'EARLY_DELIVERIES', 'REFRIGERATED_LOCKER', 'number_of_names', 'skill_0', 'skill_1', 'skill_2', 'skill_3', 'junk', 'personal', 'professional']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printing candidate variable sets\n",
    "print(f\"\"\"\n",
    "/--------------------------\\\\\n",
    "|Explanatory Variable Sets |\n",
    "\\\\--------------------------/\n",
    "\n",
    "Full Model:\n",
    "-----------\n",
    "{candidate_dict['logit_full']}\n",
    "\n",
    "\n",
    "First Significant p-value Model:\n",
    "--------------------------------\n",
    "{candidate_dict['logit_sig']}\n",
    "\n",
    "\n",
    "Second Significant p-value Model:\n",
    "---------------------------------\n",
    "{candidate_dict['logit_sig_2']}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REVENUE</th>\n",
       "      <th>CROSS_SELL_SUCCESS</th>\n",
       "      <th>TOTAL_MEALS_ORDERED</th>\n",
       "      <th>UNIQUE_MEALS_PURCH</th>\n",
       "      <th>CONTACTS_W_CUSTOMER_SERVICE</th>\n",
       "      <th>PRODUCT_CATEGORIES_VIEWED</th>\n",
       "      <th>AVG_TIME_PER_SITE_VISIT</th>\n",
       "      <th>MOBILE_NUMBER</th>\n",
       "      <th>CANCELLATIONS_BEFORE_NOON</th>\n",
       "      <th>CANCELLATIONS_AFTER_NOON</th>\n",
       "      <th>...</th>\n",
       "      <th>AVG_CLICKS_PER_VISIT</th>\n",
       "      <th>TOTAL_PHOTOS_VIEWED</th>\n",
       "      <th>number_of_names</th>\n",
       "      <th>skill_0</th>\n",
       "      <th>skill_1</th>\n",
       "      <th>skill_2</th>\n",
       "      <th>skill_3</th>\n",
       "      <th>junk</th>\n",
       "      <th>personal</th>\n",
       "      <th>professional</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>393.0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>48.00</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1365.0</td>\n",
       "      <td>1</td>\n",
       "      <td>87</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>40.35</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>170</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>800.0</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>19.77</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>600.0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>90.00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1490.0</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>40.38</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>205</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1941</th>\n",
       "      <td>3450.0</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>108.90</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1942</th>\n",
       "      <td>5829.0</td>\n",
       "      <td>0</td>\n",
       "      <td>244</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>133.91</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>424</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1943</th>\n",
       "      <td>1900.0</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>102.71</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>480</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1944</th>\n",
       "      <td>1600.0</td>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>638.87</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>796</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1945</th>\n",
       "      <td>2050.0</td>\n",
       "      <td>1</td>\n",
       "      <td>188</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>71.45</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1946 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      REVENUE  CROSS_SELL_SUCCESS  TOTAL_MEALS_ORDERED  UNIQUE_MEALS_PURCH  \\\n",
       "0       393.0                   1                   14                   6   \n",
       "1      1365.0                   1                   87                   3   \n",
       "2       800.0                   1                   15                   7   \n",
       "3       600.0                   1                   13                   6   \n",
       "4      1490.0                   1                   47                   8   \n",
       "...       ...                 ...                  ...                 ...   \n",
       "1941   3450.0                   0                   87                   8   \n",
       "1942   5829.0                   0                  244                   4   \n",
       "1943   1900.0                   0                   57                   2   \n",
       "1944   1600.0                   0                   74                   3   \n",
       "1945   2050.0                   1                  188                   4   \n",
       "\n",
       "      CONTACTS_W_CUSTOMER_SERVICE  PRODUCT_CATEGORIES_VIEWED  \\\n",
       "0                              12                         10   \n",
       "1                               8                          8   \n",
       "2                              11                          5   \n",
       "3                              11                          5   \n",
       "4                               6                         10   \n",
       "...                           ...                        ...   \n",
       "1941                            8                          7   \n",
       "1942                            7                          2   \n",
       "1943                            8                          4   \n",
       "1944                           10                         10   \n",
       "1945                            9                          5   \n",
       "\n",
       "      AVG_TIME_PER_SITE_VISIT  MOBILE_NUMBER  CANCELLATIONS_BEFORE_NOON  \\\n",
       "0                       48.00              1                          3   \n",
       "1                       40.35              1                          0   \n",
       "2                       19.77              1                          3   \n",
       "3                       90.00              1                          2   \n",
       "4                       40.38              1                          0   \n",
       "...                       ...            ...                        ...   \n",
       "1941                   108.90              1                          0   \n",
       "1942                   133.91              1                          1   \n",
       "1943                   102.71              1                          2   \n",
       "1944                   638.87              0                          0   \n",
       "1945                    71.45              1                          4   \n",
       "\n",
       "      CANCELLATIONS_AFTER_NOON  ...  AVG_CLICKS_PER_VISIT  \\\n",
       "0                            1  ...                    17   \n",
       "1                            0  ...                    13   \n",
       "2                            0  ...                    16   \n",
       "3                            0  ...                    14   \n",
       "4                            0  ...                    12   \n",
       "...                        ...  ...                   ...   \n",
       "1941                         0  ...                    11   \n",
       "1942                         2  ...                    10   \n",
       "1943                         0  ...                    12   \n",
       "1944                         0  ...                    11   \n",
       "1945                         0  ...                    12   \n",
       "\n",
       "      TOTAL_PHOTOS_VIEWED  number_of_names  skill_0  skill_1  skill_2  \\\n",
       "0                       0                1        1        0        0   \n",
       "1                     170                2        1        0        0   \n",
       "2                       0                2        1        0        0   \n",
       "3                       0                2        1        0        0   \n",
       "4                     205                2        0        1        0   \n",
       "...                   ...              ...      ...      ...      ...   \n",
       "1941                    0                2        0        0        1   \n",
       "1942                  424                2        0        1        0   \n",
       "1943                  480                2        1        0        0   \n",
       "1944                  796                1        0        0        0   \n",
       "1945                    0                2        0        0        1   \n",
       "\n",
       "      skill_3  junk  personal  professional  \n",
       "0           0     0         0             1  \n",
       "1           0     0         0             1  \n",
       "2           0     0         0             1  \n",
       "3           0     0         0             1  \n",
       "4           0     0         0             1  \n",
       "...       ...   ...       ...           ...  \n",
       "1941        0     0         1             0  \n",
       "1942        0     0         1             0  \n",
       "1943        0     0         1             0  \n",
       "1944        1     0         1             0  \n",
       "1945        0     0         0             1  \n",
       "\n",
       "[1946 rows x 31 columns]"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MOBILE_NUMBER</th>\n",
       "      <th>CANCELLATIONS_BEFORE_NOON</th>\n",
       "      <th>TASTES_AND_PREFERENCES</th>\n",
       "      <th>number_of_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1941</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1942</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1943</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1944</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1945</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1946 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MOBILE_NUMBER  CANCELLATIONS_BEFORE_NOON  TASTES_AND_PREFERENCES  \\\n",
       "0                 1                          3                       1   \n",
       "1                 1                          0                       1   \n",
       "2                 1                          3                       1   \n",
       "3                 1                          2                       1   \n",
       "4                 1                          0                       0   \n",
       "...             ...                        ...                     ...   \n",
       "1941              1                          0                       1   \n",
       "1942              1                          1                       1   \n",
       "1943              1                          2                       1   \n",
       "1944              0                          0                       1   \n",
       "1945              1                          4                       1   \n",
       "\n",
       "      number_of_names  \n",
       "0                   1  \n",
       "1                   2  \n",
       "2                   2  \n",
       "3                   2  \n",
       "4                   2  \n",
       "...               ...  \n",
       "1941                2  \n",
       "1942                2  \n",
       "1943                2  \n",
       "1944                1  \n",
       "1945                2  \n",
       "\n",
       "[1946 rows x 4 columns]"
      ]
     },
     "execution_count": 532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chef_data   =  chef.loc[ : , candidate_dict['logit_sig']]\n",
    "chef_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       1\n",
       "2       1\n",
       "3       1\n",
       "4       1\n",
       "       ..\n",
       "1941    0\n",
       "1942    0\n",
       "1943    0\n",
       "1944    0\n",
       "1945    1\n",
       "Name: CROSS_SELL_SUCCESS, Length: 1946, dtype: int64"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chef_target =  chef.loc[ : , 'CROSS_SELL_SUCCESS']\n",
    "chef_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg Training ACCURACY: 0.7231\n",
      "LogReg Testing  ACCURACY: 0.7207\n",
      "LogReg Train-Test Gap   : 0.0024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# train/test split with the full model\n",
    "chef_data   =  chef.loc[ : , candidate_dict['logit_sig_2']]\n",
    "chef_target =  chef.loc[ : , 'CROSS_SELL_SUCCESS']\n",
    "\n",
    "# this is the exact code we were using before\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            chef_data,\n",
    "            chef_target,\n",
    "            random_state = 219,\n",
    "            test_size    = 0.25,\n",
    "            stratify     = chef_target)\n",
    "\n",
    "\n",
    "# INSTANTIATING a logistic regression model\n",
    "logreg = LogisticRegression(solver = 'lbfgs',\n",
    "                            C = 1,\n",
    "                            random_state = 219)\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "logreg_fit = logreg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "logreg_pred = logreg_fit.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('LogReg Training ACCURACY:', logreg_fit.score(X_train, y_train).round(4))\n",
    "print('LogReg Testing  ACCURACY:', logreg_fit.score(X_test, y_test).round(4))\n",
    "\n",
    "# saving scoring data for future use\n",
    "logreg_train_score = logreg_fit.score(X_train, y_train).round(4) # accuracy\n",
    "logreg_test_score  = logreg_fit.score(X_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('LogReg Train-Test Gap   :', abs(logreg_train_score - logreg_test_score).round(4))\n",
    "logreg_test_gap = abs(logreg_train_score - logreg_test_score).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True Negatives : 49\n",
      "False Positives: 107\n",
      "False Negatives: 29\n",
      "True Positives : 302\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unpacking the confusion matrix\n",
    "logreg_tn, \\\n",
    "logreg_fp, \\\n",
    "logreg_fn, \\\n",
    "logreg_tp = confusion_matrix(y_true = y_test, y_pred = logreg_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {logreg_tn}\n",
    "False Positives: {logreg_fp}\n",
    "False Negatives: {logreg_fn}\n",
    "True Positives : {logreg_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6132\n"
     ]
    }
   ],
   "source": [
    "# area under the roc curve (auc)\n",
    "print(roc_auc_score(y_true  = y_test,\n",
    "                    y_score = logreg_pred).round(decimals = 4))\n",
    "\n",
    "\n",
    "# saving AUC score for future use\n",
    "logreg_auc_score = roc_auc_score(y_true  = y_test,\n",
    "                                 y_score = logreg_pred).round(decimals = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('intercept', -0.16)\n",
      "('MOBILE_NUMBER', -0.0)\n",
      "('CANCELLATIONS_BEFORE_NOON', 0.48)\n",
      "('TASTES_AND_PREFERENCES', 0.27)\n",
      "('number_of_names', 0.27)\n"
     ]
    }
   ],
   "source": [
    "# zipping each feature name to its coefficient\n",
    "logreg_model_values = zip(chef[candidate_dict['logit_sig']].columns,\n",
    "                          logreg_fit.coef_.ravel().round(decimals = 2))\n",
    "\n",
    "\n",
    "# setting up a placeholder list to store model features\n",
    "logreg_model_lst = [('intercept', logreg_fit.intercept_[0].round(decimals = 2))]\n",
    "\n",
    "\n",
    "# printing out each feature-coefficient pair one by one\n",
    "for val in logreg_model_values:\n",
    "    logreg_model_lst.append(val)\n",
    "    \n",
    "\n",
    "# checking the results\n",
    "for pair in logreg_model_lst:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Trees (CART Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# display_tree\n",
    "########################################\n",
    "def display_tree(tree, feature_df, height = 500, width = 800):\n",
    "    \"\"\"\n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    tree       : fitted tree model object\n",
    "        fitted CART model to visualized\n",
    "    feature_df : DataFrame\n",
    "        DataFrame of explanatory features (used to generate labels)\n",
    "    height     : int, default 500\n",
    "        height in pixels to which to constrain image in html\n",
    "    width      : int, default 800\n",
    "        width in pixels to which to constrain image in html\n",
    "    \"\"\"\n",
    "\n",
    "    # visualizing the tree\n",
    "    dot_data = StringIO()\n",
    "\n",
    "    \n",
    "    # exporting tree to graphviz\n",
    "    export_graphviz(decision_tree      = tree,\n",
    "                    out_file           = dot_data,\n",
    "                    filled             = True,\n",
    "                    rounded            = True,\n",
    "                    special_characters = True,\n",
    "                    feature_names      = feature_df.columns)\n",
    "\n",
    "\n",
    "    # declaring a graph object\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "\n",
    "\n",
    "    # creating image\n",
    "    img = Image(graph.create_png(),\n",
    "                height = height,\n",
    "                width  = width)\n",
    "    \n",
    "    return img\n",
    "\n",
    "########################################\n",
    "# plot_feature_importances\n",
    "########################################\n",
    "def plot_feature_importances(model, train, export = False):\n",
    "    \"\"\"\n",
    "    Plots the importance of features from a CART model.\n",
    "    \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    model  : CART model\n",
    "    train  : explanatory variable training data\n",
    "    export : whether or not to export as a .png image, default False\n",
    "    \"\"\"\n",
    "    \n",
    "    # declaring the number\n",
    "    n_features = X_train.shape[1]\n",
    "    \n",
    "    # setting plot window\n",
    "    fig, ax = plt.subplots(figsize=(12,9))\n",
    "    \n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center')\n",
    "    plt.yticks(pd.np.arange(n_features), train.columns)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    \n",
    "    if export == True:\n",
    "        plt.savefig('Tree_Leaf_50_Feature_Importance.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Tree Training ACCURACY: 1.0\n",
      "Full Tree Testing ACCURACY : 0.6735\n",
      "Full Tree AUC Score: 0.6192\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING a classification tree object\n",
    "full_tree = DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "full_tree_fit = full_tree.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "full_tree_pred = full_tree_fit.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the model\n",
    "print('Full Tree Training ACCURACY:', full_tree_fit.score(X_train,\n",
    "                                                     y_train).round(4))\n",
    "\n",
    "print('Full Tree Testing ACCURACY :', full_tree_fit.score(X_test,\n",
    "                                                     y_test).round(4))\n",
    "\n",
    "print('Full Tree AUC Score:', roc_auc_score(y_true  = y_test,\n",
    "                                            y_score = full_tree_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "full_tree_train_score = full_tree_fit.score(X_train, y_train).round(4) # accuracy\n",
    "full_tree_test_score  = full_tree_fit.score(X_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving AUC\n",
    "full_tree_auc_score   = roc_auc_score(y_true  = y_test,\n",
    "                                      y_score = full_tree_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True Negatives : 73\n",
      "False Positives: 83\n",
      "False Negatives: 76\n",
      "True Positives : 255\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unpacking the confusion matrix\n",
    "full_tree_tn, \\\n",
    "full_tree_fp, \\\n",
    "full_tree_fn, \\\n",
    "full_tree_tp = confusion_matrix(y_true = y_test, y_pred = full_tree_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {full_tree_tn}\n",
    "False Positives: {full_tree_fp}\n",
    "False Negatives: {full_tree_fn}\n",
    "True Positives : {full_tree_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puned tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.7512\n",
      "Testing  ACCURACY: 0.7803\n",
      "AUC Score        : 0.713\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING a classification tree object\n",
    "pruned_tree = DecisionTreeClassifier(max_depth = 4,\n",
    "                                     min_samples_leaf = 25,\n",
    "                                     random_state = 219)\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "pruned_tree_fit  = pruned_tree.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "pruned_tree_pred = pruned_tree_fit.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the model\n",
    "print('Training ACCURACY:', pruned_tree_fit.score(X_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', pruned_tree_fit.score(X_test, y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = pruned_tree_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "pruned_tree_train_score = pruned_tree_fit.score(X_train, y_train).round(4) # accuracy\n",
    "pruned_tree_test_score  = pruned_tree_fit.score(X_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving auc score\n",
    "pruned_tree_auc_score   = roc_auc_score(y_true  = y_test,\n",
    "                                        y_score = pruned_tree_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True Negatives : 82\n",
      "False Positives: 74\n",
      "False Negatives: 33\n",
      "True Positives : 298\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unpacking the confusion matrix\n",
    "pruned_tree_tn, \\\n",
    "pruned_tree_fp, \\\n",
    "pruned_tree_fn, \\\n",
    "pruned_tree_tp = confusion_matrix(y_true = y_test, y_pred = pruned_tree_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {pruned_tree_tn}\n",
    "False Positives: {pruned_tree_fp}\n",
    "False Negatives: {pruned_tree_fn}\n",
    "True Positives : {pruned_tree_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model         AUC Score      TN, FP, FN, TP\n",
      "-----         ---------      --------------\n",
      "Logistic      0.6132         (49, 107, 29, 302)\n",
      "Full Tree     0.6192         (73, 83, 76, 255)\n",
      "Pruned Tree   0.713         (82, 74, 33, 298)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# comparing results\n",
    "print(f\"\"\"\n",
    "Model         AUC Score      TN, FP, FN, TP\n",
    "-----         ---------      --------------\n",
    "Logistic      {logreg_auc_score}         {logreg_tn, logreg_fp, logreg_fn, logreg_tp}\n",
    "Full Tree     {full_tree_auc_score}         {full_tree_tn, full_tree_fp, full_tree_fn, full_tree_tp}\n",
    "Pruned Tree   {pruned_tree_auc_score}         {pruned_tree_tn, pruned_tree_fp, pruned_tree_fn, pruned_tree_tp}\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# creating a dictionary for model results\n",
    "model_performance = {\n",
    "    \n",
    "    'Model Name'    : ['Logistic', 'Full Tree', 'Pruned Tree'],\n",
    "           \n",
    "    'AUC Score' : [logreg_auc_score, full_tree_auc_score, pruned_tree_auc_score],\n",
    "    \n",
    "    'Training Accuracy' : [logreg_train_score, full_tree_train_score,\n",
    "                           pruned_tree_train_score],\n",
    "           \n",
    "    'Testing Accuracy'  : [logreg_test_score, full_tree_test_score,\n",
    "                           pruned_tree_test_score],\n",
    "\n",
    "    'Confusion Matrix'  : [(logreg_tn, logreg_fp, logreg_fn, logreg_tp),\n",
    "                           (full_tree_tn, full_tree_fp, full_tree_fn, full_tree_tp),\n",
    "                           (pruned_tree_tn, pruned_tree_fp, pruned_tree_fn, pruned_tree_tp)]}\n",
    "\n",
    "\n",
    "# converting model_performance into a DataFrame\n",
    "model_performance = pd.DataFrame(model_performance)\n",
    "\n",
    "\n",
    "# sending model results to Excel\n",
    "model_performance.to_excel('./model_results/classification_model_performance.xlsx',\n",
    "                           index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification Modeling with KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal number of neighbors is: 13\n"
     ]
    }
   ],
   "source": [
    "# determining the optimal number of neighbors\n",
    "opt_neighbors = optimal_neighbors(X_data        = chef_data,\n",
    "                                  y_data        = chef_target,\n",
    "                                  response_type = 'class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.7375\n",
      "Testing  ACCURACY: 0.7064\n",
      "AUC Score        : 0.5942\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING StandardScaler()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "# FITTING the data\n",
    "scaler.fit(chef_data)\n",
    "\n",
    "\n",
    "# TRANSFORMING the data\n",
    "X_scaled     = scaler.transform(chef_data)\n",
    "\n",
    "\n",
    "# converting to a DataFrame\n",
    "X_scaled_df  = pd.DataFrame(X_scaled) \n",
    "\n",
    "\n",
    "# train-test split with the scaled data\n",
    "X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = train_test_split(\n",
    "            X_scaled_df,\n",
    "            chef_target,\n",
    "            random_state = 219,\n",
    "            test_size = 0.25,\n",
    "            stratify = chef_target)\n",
    "\n",
    "\n",
    "# INSTANTIATING a KNN classification model with optimal neighbors\n",
    "knn_opt = KNeighborsClassifier(n_neighbors = opt_neighbors)\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "knn_fit = knn_opt.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "knn_pred = knn_fit.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', knn_fit.score(X_train_scaled, y_train_scaled).round(4))\n",
    "print('Testing  ACCURACY:', knn_fit.score(X_test_scaled, y_test_scaled).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = knn_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data\n",
    "knn_train_score = knn_fit.score(X_train_scaled, y_train_scaled).round(4)\n",
    "knn_test_score  = knn_fit.score(X_test_scaled, y_test_scaled).round(4)\n",
    "\n",
    "\n",
    "# saving AUC score\n",
    "knn_auc_score   = roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = knn_pred).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True Negatives : 82\n",
      "False Positives: 74\n",
      "False Negatives: 33\n",
      "True Positives : 298\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unpacking the confusion matrix\n",
    "knn_tree_tn, \\\n",
    "knn_tree_fp, \\\n",
    "knn_tree_fn, \\\n",
    "knn_tree_tp = confusion_matrix(y_true = y_test, y_pred = knn_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {pruned_tree_tn}\n",
    "False Positives: {pruned_tree_fp}\n",
    "False Negatives: {pruned_tree_fn}\n",
    "True Positives : {pruned_tree_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split with the logit_sig variables\n",
    "chef_data   =  chef.loc[ : , candidate_dict['logit_sig_2']]\n",
    "chef_target =  chef.loc[ : , 'CROSS_SELL_SUCCESS']\n",
    "\n",
    "\n",
    "# train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            chef_data,\n",
    "            chef_target,\n",
    "            random_state = 219,\n",
    "            test_size    = 0.25,\n",
    "            stratify     = chef_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-599-8b2c492ff40a>:6: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  C_space          = pd.np.arange(0.1, 5.0, 0.1)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/scipy/optimize/linesearch.py:477: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/scipy/optimize/linesearch.py:477: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/scipy/optimize/linesearch.py:477: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/scipy/optimize/linesearch.py:477: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/scipy/optimize/linesearch.py:477: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/scipy/optimize/linesearch.py:477: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/scipy/optimize/linesearch.py:477: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/scipy/optimize/linesearch.py:477: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/scipy/optimize/linesearch.py:477: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/scipy/optimize/linesearch.py:477: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/scipy/optimize/linesearch.py:477: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/scipy/optimize/linesearch.py:437: LineSearchWarning: Rounding errors prevent the line search from converging\n",
      "  warn(msg, LineSearchWarning)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/scipy/optimize/linesearch.py:477: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/scipy/optimize/linesearch.py:437: LineSearchWarning: Rounding errors prevent the line search from converging\n",
      "  warn(msg, LineSearchWarning)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/scipy/optimize/linesearch.py:327: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Parameters  : {'warm_start': True, 'solver': 'newton-cg', 'C': 0.6}\n",
      "Tuned CV AUC      : 0.6549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# RandomizedSearchCV\n",
    "########################################\n",
    "\n",
    "# declaring a hyperparameter space\n",
    "C_space          = pd.np.arange(0.1, 5.0, 0.1)\n",
    "warm_start_space = [True, False]\n",
    "solver_space     = ['newton-cg', 'sag', 'lbfgs']\n",
    "\n",
    "\n",
    "# creating a hyperparameter grid\n",
    "param_grid = {'C'          : C_space,\n",
    "              'warm_start' : warm_start_space,\n",
    "              'solver'     : solver_space}\n",
    "\n",
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "lr_tuned = LogisticRegression(random_state = 219,\n",
    "                              max_iter     = 1000)\n",
    "\n",
    "\n",
    "# GridSearchCV object\n",
    "lr_tuned_cv = RandomizedSearchCV(estimator           = lr_tuned,   # the model object\n",
    "                                 param_distributions = param_grid, # parameters to tune\n",
    "                                 cv                  = 3,          # how many folds in cross-validation\n",
    "                                 n_iter              = 250,        # number of combinations of hyperparameters to try\n",
    "                                 random_state        = 219,        # starting point for random sequence\n",
    "                                 scoring = make_scorer(\n",
    "                                           roc_auc_score,\n",
    "                                           needs_threshold = False)) # scoring criteria (AUC)\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "lr_tuned_cv.fit(chef_data, chef_target)\n",
    "\n",
    "\n",
    "# PREDICT step is not needed\n",
    "\n",
    "\n",
    "# printing the optimal parameters and best score\n",
    "print(\"Tuned Parameters  :\", lr_tuned_cv.best_params_)\n",
    "print(\"Tuned CV AUC      :\", lr_tuned_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.15956569, 0.08670425, 0.05159195, 0.04564301, 0.161484  ,\n",
       "        0.09777427, 0.16400941, 0.03993305, 0.16233166, 0.15849606,\n",
       "        0.06434838, 0.08784175, 0.04214176, 0.16129557, 0.10947029,\n",
       "        0.16043663, 0.03301565, 0.10177501, 0.16175111, 0.10512431,\n",
       "        0.0431598 , 0.09208298, 0.0923512 , 0.16087993, 0.08988873,\n",
       "        0.16194598, 0.15722267, 0.03881772, 0.15991267, 0.08460999,\n",
       "        0.16373197, 0.04289762, 0.08540471, 0.08215038, 0.07069016,\n",
       "        0.09411073, 0.10480873, 0.10946361, 0.16592304, 0.05054426,\n",
       "        0.08244713, 0.04731337, 0.16070875, 0.15845442, 0.0993549 ,\n",
       "        0.07370567, 0.05057931, 0.04591155, 0.15982167, 0.04799501,\n",
       "        0.02837745, 0.16248775, 0.15720638, 0.15879067, 0.08094327,\n",
       "        0.16072226, 0.03561568, 0.16025194, 0.09222968, 0.08663678,\n",
       "        0.0809261 , 0.16387439, 0.15746808, 0.04599182, 0.16180174,\n",
       "        0.08571037, 0.07629506, 0.16027069, 0.02929934, 0.04230634,\n",
       "        0.03476755, 0.06879314, 0.075454  , 0.07781498, 0.03032009,\n",
       "        0.08300734, 0.08738105, 0.09607434, 0.06780799, 0.0932068 ,\n",
       "        0.07049743, 0.16685406, 0.1572485 , 0.09049996, 0.04913672,\n",
       "        0.16436434, 0.07093898, 0.16357732, 0.15857577, 0.02998702,\n",
       "        0.16222127, 0.15877859, 0.04653668, 0.08763909, 0.10107597,\n",
       "        0.10169204, 0.0339721 , 0.10015663, 0.04281108, 0.07828856,\n",
       "        0.0539279 , 0.08881172, 0.16352733, 0.04617365, 0.06986491,\n",
       "        0.05810459, 0.16255959, 0.15785495, 0.15909561, 0.15884662,\n",
       "        0.02808722, 0.16196489, 0.04352164, 0.16040699, 0.15682022,\n",
       "        0.15723006, 0.15645345, 0.03721595, 0.02905973, 0.07962203,\n",
       "        0.08202394, 0.16780774, 0.10550682, 0.10215012, 0.10173853,\n",
       "        0.03122362, 0.16161267, 0.03868055, 0.08127626, 0.0985655 ,\n",
       "        0.05747676, 0.1651744 , 0.04680403, 0.04940311, 0.16158398,\n",
       "        0.0350879 , 0.09144322, 0.04387879, 0.07343022, 0.11099966,\n",
       "        0.16437435, 0.05200148, 0.03364499, 0.07398868, 0.0345517 ,\n",
       "        0.03137175, 0.06525763, 0.0787247 , 0.04402137, 0.08349204,\n",
       "        0.03217769, 0.16286   , 0.15982588, 0.10864202, 0.08716512,\n",
       "        0.16332459, 0.08708994, 0.04588413, 0.07602906, 0.16599838,\n",
       "        0.15694801, 0.15773598, 0.15643803, 0.15715996, 0.15674305,\n",
       "        0.08549142, 0.08267204, 0.0836846 , 0.10728947, 0.05842018,\n",
       "        0.11305968, 0.09622701, 0.04364141, 0.06589238, 0.04725289,\n",
       "        0.05248761, 0.07489793, 0.07797805, 0.08651121, 0.07102553,\n",
       "        0.16860096, 0.1584332 , 0.04244844, 0.03073438, 0.02634199,\n",
       "        0.04510172, 0.04973785, 0.03328904, 0.04465024, 0.03434889,\n",
       "        0.053708  , 0.16599917, 0.15662758, 0.0601    , 0.16615407,\n",
       "        0.15853969, 0.05609282, 0.07197587, 0.08120163, 0.10609444,\n",
       "        0.10673436, 0.16541394, 0.05018409, 0.04155596, 0.16728203,\n",
       "        0.15968553, 0.09017897, 0.07150563, 0.07796367, 0.0426844 ,\n",
       "        0.07441966, 0.07822037, 0.16743302, 0.16039371, 0.09663963,\n",
       "        0.05863865, 0.16694252, 0.15953104, 0.09373013, 0.16678675,\n",
       "        0.03835972, 0.16223232, 0.08333731, 0.04617731, 0.07907216,\n",
       "        0.17463334, 0.10899703, 0.16382043, 0.16051563, 0.08369851,\n",
       "        0.03255987, 0.16586002, 0.10756063, 0.16713365, 0.09712799,\n",
       "        0.16689507, 0.09904575, 0.08941182, 0.10806767, 0.16353559,\n",
       "        0.1595703 , 0.03538561, 0.16535266, 0.04760734, 0.0472343 ,\n",
       "        0.04760965, 0.16728131, 0.04449224, 0.11961206, 0.16532278]),\n",
       " 'std_fit_time': array([2.11457050e-03, 1.65312408e-02, 2.84375997e-02, 3.20034553e-02,\n",
       "        3.08402733e-03, 5.12756850e-03, 6.12738800e-03, 1.03250517e-02,\n",
       "        6.08512998e-03, 9.97849352e-04, 8.66250300e-03, 1.10565803e-02,\n",
       "        2.43582095e-02, 3.33600893e-03, 1.27077392e-02, 2.95930462e-03,\n",
       "        1.04896165e-02, 1.16638857e-02, 3.42860602e-03, 1.83782997e-02,\n",
       "        1.03675511e-02, 3.17292926e-02, 8.63222612e-03, 3.18551703e-03,\n",
       "        1.73830080e-02, 4.13283837e-03, 3.85248932e-04, 1.94474530e-02,\n",
       "        3.27475484e-03, 4.30617392e-03, 9.60540346e-03, 7.06356176e-03,\n",
       "        1.25655424e-02, 3.84139731e-03, 3.57323428e-03, 1.59482145e-02,\n",
       "        1.93917707e-02, 3.06422750e-02, 8.40788446e-03, 5.46383117e-03,\n",
       "        5.73864237e-03, 8.18652352e-03, 3.94559429e-03, 2.86325593e-04,\n",
       "        1.41156771e-02, 1.01689851e-02, 2.08312540e-02, 1.33353553e-02,\n",
       "        3.46772974e-03, 1.82695632e-02, 2.79750688e-03, 1.47932050e-03,\n",
       "        3.45888383e-04, 1.46785829e-03, 4.03534269e-03, 3.88406005e-03,\n",
       "        1.19196764e-02, 4.11445568e-03, 5.90945505e-03, 8.09676546e-03,\n",
       "        4.17451707e-03, 9.49324036e-03, 8.65078173e-04, 1.34172331e-02,\n",
       "        3.72610415e-03, 4.35425724e-03, 3.24082890e-02, 3.00587884e-03,\n",
       "        1.14665945e-02, 1.53872127e-02, 1.01511425e-02, 6.30203959e-03,\n",
       "        4.23127622e-03, 5.48394079e-03, 5.46581933e-03, 1.11189569e-02,\n",
       "        1.38173217e-02, 9.94262541e-03, 7.38262618e-03, 2.29021486e-02,\n",
       "        2.36126352e-02, 1.13492272e-02, 8.17896062e-04, 2.65255245e-02,\n",
       "        3.10009323e-02, 9.70221940e-03, 2.54712108e-02, 6.73088274e-03,\n",
       "        8.59645114e-04, 7.59393210e-03, 6.01402728e-03, 8.82039285e-04,\n",
       "        2.84241413e-02, 1.34325225e-02, 1.40466124e-02, 4.94366557e-03,\n",
       "        1.18203431e-02, 1.55759654e-02, 7.25026568e-03, 5.83343830e-03,\n",
       "        2.08706605e-02, 9.24877189e-03, 6.75400760e-03, 1.98692281e-02,\n",
       "        2.81544372e-02, 9.88633288e-03, 2.20734057e-03, 5.11365792e-04,\n",
       "        3.72433750e-04, 8.48350953e-04, 5.46952357e-03, 2.33560730e-03,\n",
       "        1.12439143e-02, 2.77701554e-03, 4.32182209e-04, 6.34917743e-04,\n",
       "        4.19376421e-04, 2.24295357e-03, 1.14709505e-02, 9.94475448e-03,\n",
       "        1.46756442e-02, 1.07489293e-02, 1.95442321e-02, 7.70290633e-03,\n",
       "        6.58605171e-03, 8.97865487e-03, 3.67849319e-03, 2.35450452e-03,\n",
       "        2.11331394e-02, 1.81515079e-02, 3.69647007e-02, 4.46778414e-03,\n",
       "        1.07359664e-02, 4.90675560e-03, 3.52560175e-03, 6.16379191e-03,\n",
       "        9.41083718e-03, 2.20919687e-03, 4.90169946e-03, 3.71866142e-03,\n",
       "        5.19583865e-03, 1.84697354e-02, 5.35574733e-03, 9.13684066e-03,\n",
       "        7.30151906e-03, 4.70719559e-03, 2.30510013e-02, 5.72698063e-03,\n",
       "        2.02976201e-02, 5.10548646e-03, 8.80015979e-03, 3.96344885e-03,\n",
       "        9.57369526e-04, 6.04881609e-03, 2.32945814e-02, 4.98227796e-03,\n",
       "        1.96885968e-02, 1.76036320e-02, 8.22221400e-03, 6.07503783e-03,\n",
       "        4.14467429e-04, 1.14410357e-03, 2.55527799e-04, 7.17401711e-04,\n",
       "        7.66982551e-04, 1.01090403e-02, 1.17314807e-02, 6.12455699e-03,\n",
       "        2.44046944e-02, 1.74684305e-02, 1.52099576e-02, 9.89410510e-03,\n",
       "        3.31422038e-02, 3.90448068e-02, 1.47390534e-02, 1.88397155e-02,\n",
       "        1.06614448e-02, 3.99230960e-03, 7.08130921e-03, 2.99728882e-03,\n",
       "        4.97825961e-03, 1.57480568e-03, 9.56956383e-03, 7.60305070e-03,\n",
       "        7.81736009e-03, 1.53879414e-02, 2.40258364e-02, 3.43276891e-03,\n",
       "        5.64806513e-03, 1.44651814e-02, 2.09718080e-02, 1.86487315e-03,\n",
       "        8.91941151e-05, 2.28768798e-02, 1.09586585e-02, 1.74209074e-03,\n",
       "        3.01223680e-02, 9.08934146e-03, 7.12856086e-03, 9.21045435e-03,\n",
       "        5.23643382e-03, 4.80507744e-03, 3.00736883e-02, 4.63081261e-03,\n",
       "        6.03316278e-03, 2.83427298e-03, 2.00829174e-02, 1.13148198e-02,\n",
       "        4.02999139e-03, 5.45059756e-03, 2.30067126e-03, 1.06264524e-02,\n",
       "        5.40292893e-03, 2.90689405e-03, 6.52718154e-03, 3.04120914e-02,\n",
       "        7.93194876e-03, 1.00059407e-03, 4.03543901e-03, 7.03970894e-03,\n",
       "        2.15658094e-02, 5.26499343e-03, 7.58986881e-03, 1.45468669e-02,\n",
       "        5.25142420e-03, 5.10550387e-03, 1.45709798e-02, 1.98346399e-03,\n",
       "        4.89606676e-04, 8.93667131e-03, 2.55383063e-03, 4.20919387e-03,\n",
       "        1.34881402e-02, 7.80405782e-03, 3.07381369e-03, 5.03112975e-03,\n",
       "        4.56187553e-03, 1.38241600e-02, 2.10477478e-03, 2.66418305e-03,\n",
       "        2.12256126e-03, 5.52015188e-03, 3.96064984e-03, 1.79567921e-02,\n",
       "        6.36661377e-03, 5.98138318e-03, 2.63866863e-03, 9.21107446e-03,\n",
       "        3.37387012e-03, 1.75406065e-03]),\n",
       " 'mean_score_time': array([0.00166233, 0.00141001, 0.00137067, 0.00129795, 0.00176032,\n",
       "        0.0018727 , 0.00179513, 0.00140055, 0.0016923 , 0.0017643 ,\n",
       "        0.00153732, 0.00143758, 0.00137281, 0.00173537, 0.00159208,\n",
       "        0.00176597, 0.0014507 , 0.00164294, 0.00180332, 0.00147772,\n",
       "        0.00157094, 0.00155433, 0.00148606, 0.00171638, 0.00148002,\n",
       "        0.00187016, 0.00167616, 0.00155997, 0.00171638, 0.00138243,\n",
       "        0.00160837, 0.00128762, 0.00139793, 0.00221793, 0.00137154,\n",
       "        0.00142392, 0.00151404, 0.00151396, 0.00185362, 0.00193961,\n",
       "        0.00151571, 0.00156999, 0.00180761, 0.00172536, 0.00149417,\n",
       "        0.00196759, 0.00127832, 0.00146476, 0.0016547 , 0.00137981,\n",
       "        0.00149488, 0.00171065, 0.00168729, 0.00178798, 0.00144998,\n",
       "        0.00172869, 0.00247232, 0.00174745, 0.00182033, 0.00143488,\n",
       "        0.00137726, 0.00164294, 0.0016818 , 0.00233833, 0.00169698,\n",
       "        0.00151531, 0.00130296, 0.00185633, 0.00128309, 0.00172003,\n",
       "        0.00137202, 0.0012846 , 0.00146421, 0.00132362, 0.00235295,\n",
       "        0.00186388, 0.00148193, 0.00148201, 0.00128794, 0.00146159,\n",
       "        0.00161099, 0.00169754, 0.00156228, 0.00142272, 0.00137329,\n",
       "        0.00166297, 0.00132442, 0.00171932, 0.00176318, 0.0013744 ,\n",
       "        0.00163301, 0.00179235, 0.00139475, 0.00153772, 0.00148416,\n",
       "        0.00150172, 0.00136328, 0.00188128, 0.00146866, 0.00126998,\n",
       "        0.00154233, 0.00153891, 0.00156355, 0.00186189, 0.00194701,\n",
       "        0.00144688, 0.00191601, 0.00160042, 0.00185426, 0.00175373,\n",
       "        0.00152246, 0.0018177 , 0.00156236, 0.00181063, 0.00155298,\n",
       "        0.00175865, 0.00152818, 0.00137464, 0.0012811 , 0.00168427,\n",
       "        0.0013241 , 0.00174403, 0.00154781, 0.00157102, 0.00160265,\n",
       "        0.00140699, 0.00164636, 0.00145404, 0.001525  , 0.00140897,\n",
       "        0.00163356, 0.00188057, 0.00163229, 0.00156887, 0.00186324,\n",
       "        0.00155632, 0.0015192 , 0.00142654, 0.00128921, 0.00151881,\n",
       "        0.00173434, 0.00131639, 0.00143003, 0.00132187, 0.00126521,\n",
       "        0.00125066, 0.00133936, 0.00133562, 0.00137019, 0.00136065,\n",
       "        0.00132259, 0.00169603, 0.00177169, 0.0019904 , 0.00218312,\n",
       "        0.00180308, 0.00147502, 0.00129882, 0.00137329, 0.00153724,\n",
       "        0.00160201, 0.00166035, 0.0015409 , 0.00165637, 0.00155369,\n",
       "        0.00140103, 0.00218463, 0.00297236, 0.00153867, 0.00137814,\n",
       "        0.00159629, 0.00154034, 0.00129159, 0.0013682 , 0.00194184,\n",
       "        0.00144267, 0.0013748 , 0.00134722, 0.00213337, 0.00134174,\n",
       "        0.00160257, 0.00149242, 0.00143027, 0.00124963, 0.00136685,\n",
       "        0.00125806, 0.00132902, 0.00126561, 0.00126974, 0.00124041,\n",
       "        0.00196648, 0.00153454, 0.00140882, 0.00146898, 0.0015076 ,\n",
       "        0.00158993, 0.0013636 , 0.00128531, 0.0015173 , 0.00171995,\n",
       "        0.00170294, 0.00172504, 0.00142996, 0.00139268, 0.00172091,\n",
       "        0.00148424, 0.00139594, 0.00130924, 0.00140134, 0.00127506,\n",
       "        0.00136423, 0.0021716 , 0.00173338, 0.00160066, 0.00140961,\n",
       "        0.00172933, 0.00161664, 0.00177304, 0.00148447, 0.00179267,\n",
       "        0.00131138, 0.001635  , 0.00146476, 0.00129302, 0.00149417,\n",
       "        0.00160845, 0.00163309, 0.00193834, 0.00190632, 0.00140651,\n",
       "        0.00138783, 0.00179195, 0.00158525, 0.00185998, 0.00160901,\n",
       "        0.00181007, 0.00176191, 0.00148813, 0.00159939, 0.0018301 ,\n",
       "        0.00160376, 0.00157642, 0.00184162, 0.00162737, 0.0017097 ,\n",
       "        0.0015409 , 0.00193119, 0.00159971, 0.00167998, 0.00179521]),\n",
       " 'std_score_time': array([1.57548846e-04, 9.89922439e-05, 9.90452494e-05, 2.53539598e-05,\n",
       "        1.16986587e-04, 5.40891746e-04, 8.75290279e-05, 1.25420310e-04,\n",
       "        2.60685832e-04, 3.13325194e-04, 2.15800645e-05, 9.72195102e-05,\n",
       "        1.39544068e-04, 7.71465772e-05, 8.48514123e-05, 1.03580366e-04,\n",
       "        8.35124013e-05, 3.20994176e-04, 2.14518639e-04, 1.01556986e-04,\n",
       "        1.79253145e-05, 6.77515335e-05, 1.04524184e-05, 1.78763806e-04,\n",
       "        1.11140969e-04, 8.28245234e-05, 1.19726384e-04, 3.63544686e-05,\n",
       "        1.93694908e-04, 1.00687252e-04, 2.29675272e-04, 3.33401671e-05,\n",
       "        8.63922990e-05, 1.28432520e-03, 7.42454324e-05, 6.45453693e-05,\n",
       "        2.49876118e-05, 1.40133347e-04, 2.05393912e-05, 8.40435858e-04,\n",
       "        1.71863619e-05, 2.68408921e-05, 2.17720276e-04, 3.63963139e-05,\n",
       "        5.12875254e-05, 7.86598318e-04, 1.80321099e-05, 5.23860593e-05,\n",
       "        2.57738485e-04, 1.29982424e-04, 2.08403843e-05, 7.49946768e-05,\n",
       "        8.83322871e-05, 7.76862549e-05, 8.64032644e-05, 1.09800377e-04,\n",
       "        1.84143366e-04, 6.51647404e-05, 4.22583329e-04, 1.11573329e-04,\n",
       "        8.68520921e-05, 1.02796477e-04, 6.79725752e-05, 1.14459840e-03,\n",
       "        7.04160931e-05, 5.34909615e-05, 3.24220048e-05, 1.53341889e-04,\n",
       "        1.78589503e-05, 6.44119179e-04, 7.76064581e-05, 1.12234137e-05,\n",
       "        1.45461431e-04, 1.54197829e-05, 9.94995430e-04, 6.91447431e-04,\n",
       "        1.28784635e-04, 6.50747341e-05, 1.58878153e-05, 9.11653816e-05,\n",
       "        1.55458177e-04, 7.47806249e-05, 1.89898798e-04, 8.71635051e-05,\n",
       "        8.43033621e-05, 1.11466912e-04, 8.28766667e-05, 1.37057070e-04,\n",
       "        4.04020822e-05, 9.22375581e-05, 1.88500242e-04, 8.79917100e-05,\n",
       "        6.96625742e-05, 1.95323401e-04, 9.92361786e-05, 3.52560452e-05,\n",
       "        6.25496341e-05, 4.57739843e-04, 8.74862999e-05, 2.88957897e-06,\n",
       "        3.68930443e-05, 1.90990231e-04, 1.66852242e-04, 5.35778637e-04,\n",
       "        4.64560557e-04, 1.07862444e-04, 7.19190301e-05, 1.61076450e-04,\n",
       "        5.21220798e-05, 7.26362808e-05, 8.27937100e-05, 6.50537665e-05,\n",
       "        5.40546327e-05, 2.09318780e-04, 1.87654692e-04, 6.55127326e-05,\n",
       "        1.91475734e-04, 9.16475422e-05, 2.91887272e-05, 4.76166625e-04,\n",
       "        2.40052769e-05, 1.19510271e-04, 4.47909771e-05, 2.73384327e-05,\n",
       "        6.03461240e-05, 1.05142563e-04, 9.61459359e-05, 1.34744587e-04,\n",
       "        1.49455469e-05, 7.57529362e-05, 9.36426382e-05, 1.33461081e-04,\n",
       "        5.48429531e-05, 7.34510681e-05, 2.30147008e-04, 5.35338055e-05,\n",
       "        2.08021633e-05, 8.57384357e-05, 2.24133191e-05, 2.69615703e-05,\n",
       "        1.47964337e-04, 4.38371891e-05, 1.32222539e-04, 3.06886688e-05,\n",
       "        3.89984263e-06, 4.18573827e-06, 1.02691790e-04, 9.50840873e-05,\n",
       "        6.89528625e-05, 9.35871127e-05, 8.10979160e-05, 9.04670668e-05,\n",
       "        4.85277967e-05, 7.01173228e-04, 1.12032949e-03, 5.51791231e-05,\n",
       "        4.12917913e-05, 2.82800573e-05, 1.45166962e-04, 1.71707436e-04,\n",
       "        1.47271980e-04, 1.95561765e-04, 1.92115476e-04, 1.13347969e-04,\n",
       "        2.10592612e-04, 7.30794322e-05, 1.11655503e-03, 1.17718696e-03,\n",
       "        3.53933610e-05, 6.57511970e-05, 1.16729751e-04, 2.93492744e-05,\n",
       "        3.69363314e-05, 1.04413970e-04, 5.12080570e-04, 8.60628327e-05,\n",
       "        1.39605202e-04, 1.06802712e-04, 1.16521816e-03, 8.60089494e-05,\n",
       "        1.86231654e-04, 2.13638775e-04, 9.54383348e-05, 9.67938976e-06,\n",
       "        1.32207491e-04, 9.84244978e-06, 1.15426826e-04, 1.42644375e-05,\n",
       "        9.26327689e-06, 9.28983001e-06, 8.42148039e-04, 1.85032104e-04,\n",
       "        7.30980114e-05, 1.04448806e-04, 8.56114903e-05, 2.28639509e-04,\n",
       "        7.07484341e-05, 5.84976394e-06, 2.83451783e-04, 2.83929668e-04,\n",
       "        2.66684962e-04, 9.16785489e-05, 1.14966275e-04, 9.16835090e-05,\n",
       "        5.73940426e-05, 2.14768812e-04, 7.67181939e-05, 1.98430856e-05,\n",
       "        9.05742388e-05, 9.28506966e-06, 9.45150510e-05, 1.15518337e-03,\n",
       "        1.39667757e-04, 1.87754425e-04, 8.20530240e-05, 3.10277291e-04,\n",
       "        2.40479253e-04, 5.09153778e-05, 3.70919512e-05, 1.31728512e-05,\n",
       "        5.91256712e-05, 1.82445455e-04, 5.04548145e-05, 2.53070837e-05,\n",
       "        6.79508287e-05, 1.71679736e-04, 1.35780352e-04, 1.13841813e-04,\n",
       "        1.80235861e-04, 8.70199145e-05, 9.67640731e-05, 9.39655583e-05,\n",
       "        2.18857434e-04, 1.02897130e-04, 1.46591864e-04, 8.54410493e-05,\n",
       "        2.72317021e-04, 1.32966241e-04, 7.64235581e-05, 6.79257280e-05,\n",
       "        1.82187193e-04, 3.72197799e-05, 1.23199185e-04, 1.03836941e-04,\n",
       "        1.93169768e-04, 9.20513931e-06, 2.33164675e-04, 3.16192138e-05,\n",
       "        1.59521609e-04, 1.20090775e-04]),\n",
       " 'param_warm_start': masked_array(data=[False, True, False, False, False, True, True, False,\n",
       "                    False, True, True, False, False, False, True, False,\n",
       "                    False, True, True, False, True, False, True, True,\n",
       "                    False, False, False, True, False, False, False, True,\n",
       "                    False, True, True, False, False, False, False, False,\n",
       "                    False, False, True, True, True, True, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    True, True, False, True, False, False, True, True,\n",
       "                    False, True, True, False, False, True, False, True,\n",
       "                    True, True, True, True, True, True, False, False,\n",
       "                    False, True, True, True, True, True, True, False, True,\n",
       "                    False, True, False, True, False, True, False, False,\n",
       "                    True, True, False, True, False, True, True, False,\n",
       "                    False, True, True, True, True, True, False, False,\n",
       "                    True, True, False, False, False, True, True, True,\n",
       "                    False, True, False, False, False, True, False, True,\n",
       "                    False, False, False, False, True, False, False, True,\n",
       "                    True, False, True, True, False, True, True, True,\n",
       "                    False, False, True, False, False, False, True, True,\n",
       "                    False, False, True, False, True, True, False, True,\n",
       "                    False, True, False, True, True, True, True, True,\n",
       "                    False, False, False, True, True, True, False, False,\n",
       "                    True, True, False, False, True, True, True, True, True,\n",
       "                    True, True, False, False, True, True, True, True, True,\n",
       "                    True, True, False, False, True, True, True, False,\n",
       "                    True, True, False, False, True, False, True, False,\n",
       "                    False, True, False, False, True, False, False, True,\n",
       "                    True, False, True, True, False, False, False, False,\n",
       "                    True, False, True, False, True, True, True, False,\n",
       "                    True, True, False, False, False, False, True, False,\n",
       "                    True, False, False, False, True, False, False],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_solver': masked_array(data=['sag', 'newton-cg', 'lbfgs', 'lbfgs', 'sag',\n",
       "                    'newton-cg', 'sag', 'lbfgs', 'sag', 'sag', 'lbfgs',\n",
       "                    'newton-cg', 'lbfgs', 'sag', 'newton-cg', 'sag',\n",
       "                    'lbfgs', 'newton-cg', 'sag', 'newton-cg', 'lbfgs',\n",
       "                    'lbfgs', 'newton-cg', 'sag', 'newton-cg', 'sag', 'sag',\n",
       "                    'lbfgs', 'sag', 'newton-cg', 'sag', 'lbfgs',\n",
       "                    'newton-cg', 'newton-cg', 'newton-cg', 'newton-cg',\n",
       "                    'newton-cg', 'newton-cg', 'sag', 'lbfgs', 'newton-cg',\n",
       "                    'lbfgs', 'sag', 'sag', 'newton-cg', 'newton-cg',\n",
       "                    'lbfgs', 'lbfgs', 'sag', 'lbfgs', 'lbfgs', 'sag',\n",
       "                    'sag', 'sag', 'newton-cg', 'sag', 'lbfgs', 'sag',\n",
       "                    'newton-cg', 'newton-cg', 'newton-cg', 'sag', 'sag',\n",
       "                    'lbfgs', 'sag', 'newton-cg', 'lbfgs', 'sag', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'newton-cg', 'newton-cg',\n",
       "                    'newton-cg', 'lbfgs', 'newton-cg', 'newton-cg',\n",
       "                    'newton-cg', 'newton-cg', 'newton-cg', 'lbfgs', 'sag',\n",
       "                    'sag', 'newton-cg', 'lbfgs', 'sag', 'lbfgs', 'sag',\n",
       "                    'sag', 'lbfgs', 'sag', 'sag', 'lbfgs', 'newton-cg',\n",
       "                    'newton-cg', 'newton-cg', 'lbfgs', 'newton-cg',\n",
       "                    'lbfgs', 'newton-cg', 'lbfgs', 'newton-cg', 'sag',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'sag', 'sag', 'sag', 'sag',\n",
       "                    'lbfgs', 'sag', 'lbfgs', 'sag', 'sag', 'sag', 'sag',\n",
       "                    'lbfgs', 'lbfgs', 'newton-cg', 'newton-cg', 'sag',\n",
       "                    'newton-cg', 'newton-cg', 'newton-cg', 'lbfgs', 'sag',\n",
       "                    'lbfgs', 'lbfgs', 'newton-cg', 'lbfgs', 'sag', 'lbfgs',\n",
       "                    'lbfgs', 'sag', 'lbfgs', 'newton-cg', 'lbfgs',\n",
       "                    'newton-cg', 'newton-cg', 'sag', 'lbfgs', 'lbfgs',\n",
       "                    'newton-cg', 'lbfgs', 'lbfgs', 'lbfgs', 'newton-cg',\n",
       "                    'lbfgs', 'newton-cg', 'lbfgs', 'sag', 'sag',\n",
       "                    'newton-cg', 'newton-cg', 'sag', 'newton-cg', 'lbfgs',\n",
       "                    'newton-cg', 'sag', 'sag', 'sag', 'sag', 'sag', 'sag',\n",
       "                    'newton-cg', 'newton-cg', 'newton-cg', 'newton-cg',\n",
       "                    'lbfgs', 'newton-cg', 'newton-cg', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'newton-cg', 'newton-cg',\n",
       "                    'newton-cg', 'newton-cg', 'sag', 'sag', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'sag', 'sag', 'lbfgs', 'sag', 'sag',\n",
       "                    'lbfgs', 'newton-cg', 'newton-cg', 'newton-cg',\n",
       "                    'newton-cg', 'sag', 'lbfgs', 'lbfgs', 'sag', 'sag',\n",
       "                    'newton-cg', 'newton-cg', 'newton-cg', 'lbfgs',\n",
       "                    'newton-cg', 'newton-cg', 'sag', 'sag', 'newton-cg',\n",
       "                    'lbfgs', 'sag', 'sag', 'newton-cg', 'sag', 'lbfgs',\n",
       "                    'sag', 'newton-cg', 'lbfgs', 'newton-cg', 'sag',\n",
       "                    'newton-cg', 'sag', 'sag', 'newton-cg', 'lbfgs', 'sag',\n",
       "                    'newton-cg', 'sag', 'newton-cg', 'sag', 'newton-cg',\n",
       "                    'newton-cg', 'newton-cg', 'sag', 'sag', 'lbfgs', 'sag',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'sag', 'lbfgs', 'newton-cg',\n",
       "                    'sag'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_C': masked_array(data=[1.4000000000000001, 3.0000000000000004,\n",
       "                    2.4000000000000004, 3.0000000000000004,\n",
       "                    1.3000000000000003, 3.9000000000000004,\n",
       "                    1.4000000000000001, 0.2, 3.3000000000000003, 4.3,\n",
       "                    1.5000000000000002, 1.8000000000000003, 0.9,\n",
       "                    3.9000000000000004, 4.0, 0.30000000000000004,\n",
       "                    1.4000000000000001, 3.6, 0.8, 2.5000000000000004,\n",
       "                    2.8000000000000003, 4.8, 2.9000000000000004, 2.1,\n",
       "                    3.8000000000000003, 1.2000000000000002,\n",
       "                    1.5000000000000002, 2.3000000000000003, 0.5,\n",
       "                    4.3999999999999995, 0.7000000000000001,\n",
       "                    4.3999999999999995, 1.0, 1.0, 3.5000000000000004,\n",
       "                    2.4000000000000004, 0.7000000000000001, 4.0, 1.0,\n",
       "                    1.5000000000000002, 0.8, 1.8000000000000003, 0.1,\n",
       "                    1.3000000000000003, 1.1, 1.2000000000000002, 4.7,\n",
       "                    0.30000000000000004, 3.8000000000000003,\n",
       "                    4.3999999999999995, 0.7000000000000001, 0.9, 0.8, 2.7,\n",
       "                    2.7, 3.7, 1.4000000000000001, 2.2, 0.4,\n",
       "                    3.3000000000000003, 3.9000000000000004, 2.0, 0.5, 4.1,\n",
       "                    1.7000000000000002, 1.6, 3.4000000000000004, 0.4,\n",
       "                    2.3000000000000003, 0.30000000000000004, 2.6, 0.6,\n",
       "                    1.8000000000000003, 2.1, 1.7000000000000002, 0.5, 0.2,\n",
       "                    2.2, 3.5000000000000004, 4.3, 0.5, 0.2, 0.4,\n",
       "                    1.3000000000000003, 0.6, 3.2, 2.9000000000000004, 4.1,\n",
       "                    1.0, 0.4, 3.7, 2.4000000000000004, 3.0000000000000004,\n",
       "                    2.9000000000000004, 2.7, 4.6, 3.5000000000000004,\n",
       "                    1.9000000000000001, 2.6, 3.0000000000000004, 1.0, 2.0,\n",
       "                    0.30000000000000004, 0.1, 4.0, 1.1, 2.3000000000000003,\n",
       "                    3.9000000000000004, 3.1, 0.9, 0.7000000000000001, 4.8,\n",
       "                    1.3000000000000003, 2.9000000000000004,\n",
       "                    0.7000000000000001, 3.2, 4.3, 1.9000000000000001, 4.6,\n",
       "                    3.2, 0.8, 2.6, 2.3000000000000003, 0.5, 2.1, 4.6, 4.8,\n",
       "                    1.7000000000000002, 4.8, 3.6, 1.2000000000000002, 2.1,\n",
       "                    2.1, 1.9000000000000001, 0.2, 4.5, 3.4000000000000004,\n",
       "                    1.1, 0.6, 4.7, 1.2000000000000002, 3.3000000000000003,\n",
       "                    2.1, 1.5000000000000002, 1.6, 3.6, 2.9000000000000004,\n",
       "                    2.8000000000000003, 2.7, 4.1, 4.1, 2.4000000000000004,\n",
       "                    4.0, 4.2, 1.3000000000000003, 1.5000000000000002,\n",
       "                    1.9000000000000001, 4.7, 4.2, 3.5000000000000004,\n",
       "                    1.9000000000000001, 3.6, 4.5, 1.8000000000000003, 4.6,\n",
       "                    3.7, 2.5000000000000004, 4.5, 1.4000000000000001, 0.8,\n",
       "                    2.6, 2.3000000000000003, 1.2000000000000002,\n",
       "                    2.4000000000000004, 0.9, 4.2, 0.1, 4.3999999999999995,\n",
       "                    2.6, 1.6, 0.1, 2.8000000000000003, 0.2,\n",
       "                    1.3000000000000003, 4.5, 2.5000000000000004, 4.2, 2.2,\n",
       "                    4.3, 1.0, 3.8000000000000003, 2.6, 2.5000000000000004,\n",
       "                    0.5, 1.7000000000000002, 4.9, 3.9000000000000004, 4.9,\n",
       "                    3.2, 0.4, 4.1, 3.0000000000000004, 0.6, 3.2, 4.2, 1.6,\n",
       "                    1.4000000000000001, 0.1, 0.30000000000000004, 4.3,\n",
       "                    1.2000000000000002, 0.9, 2.7, 2.9000000000000004, 4.7,\n",
       "                    4.0, 2.3000000000000003, 4.0, 2.0, 1.8000000000000003,\n",
       "                    0.1, 3.6, 4.9, 2.5000000000000004, 3.4000000000000004,\n",
       "                    2.8000000000000003, 3.3000000000000003,\n",
       "                    3.8000000000000003, 3.1, 0.9, 3.2, 1.1,\n",
       "                    3.8000000000000003, 3.5000000000000004, 3.1, 4.1, 4.8,\n",
       "                    0.2, 3.7, 3.4000000000000004, 4.6, 4.9, 4.2,\n",
       "                    3.5000000000000004, 2.2, 3.1, 1.1, 1.8000000000000003,\n",
       "                    4.8, 4.9],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'warm_start': False, 'solver': 'sag', 'C': 1.4000000000000001},\n",
       "  {'warm_start': True, 'solver': 'newton-cg', 'C': 3.0000000000000004},\n",
       "  {'warm_start': False, 'solver': 'lbfgs', 'C': 2.4000000000000004},\n",
       "  {'warm_start': False, 'solver': 'lbfgs', 'C': 3.0000000000000004},\n",
       "  {'warm_start': False, 'solver': 'sag', 'C': 1.3000000000000003},\n",
       "  {'warm_start': True, 'solver': 'newton-cg', 'C': 3.9000000000000004},\n",
       "  {'warm_start': True, 'solver': 'sag', 'C': 1.4000000000000001},\n",
       "  {'warm_start': False, 'solver': 'lbfgs', 'C': 0.2},\n",
       "  {'warm_start': False, 'solver': 'sag', 'C': 3.3000000000000003},\n",
       "  {'warm_start': True, 'solver': 'sag', 'C': 4.3},\n",
       "  {'warm_start': True, 'solver': 'lbfgs', 'C': 1.5000000000000002},\n",
       "  {'warm_start': False, 'solver': 'newton-cg', 'C': 1.8000000000000003},\n",
       "  {'warm_start': False, 'solver': 'lbfgs', 'C': 0.9},\n",
       "  {'warm_start': False, 'solver': 'sag', 'C': 3.9000000000000004},\n",
       "  {'warm_start': True, 'solver': 'newton-cg', 'C': 4.0},\n",
       "  {'warm_start': False, 'solver': 'sag', 'C': 0.30000000000000004},\n",
       "  {'warm_start': False, 'solver': 'lbfgs', 'C': 1.4000000000000001},\n",
       "  {'warm_start': True, 'solver': 'newton-cg', 'C': 3.6},\n",
       "  {'warm_start': True, 'solver': 'sag', 'C': 0.8},\n",
       "  {'warm_start': False, 'solver': 'newton-cg', 'C': 2.5000000000000004},\n",
       "  {'warm_start': True, 'solver': 'lbfgs', 'C': 2.8000000000000003},\n",
       "  {'warm_start': False, 'solver': 'lbfgs', 'C': 4.8},\n",
       "  {'warm_start': True, 'solver': 'newton-cg', 'C': 2.9000000000000004},\n",
       "  {'warm_start': True, 'solver': 'sag', 'C': 2.1},\n",
       "  {'warm_start': False, 'solver': 'newton-cg', 'C': 3.8000000000000003},\n",
       "  {'warm_start': False, 'solver': 'sag', 'C': 1.2000000000000002},\n",
       "  {'warm_start': False, 'solver': 'sag', 'C': 1.5000000000000002},\n",
       "  {'warm_start': True, 'solver': 'lbfgs', 'C': 2.3000000000000003},\n",
       "  {'warm_start': False, 'solver': 'sag', 'C': 0.5},\n",
       "  {'warm_start': False, 'solver': 'newton-cg', 'C': 4.3999999999999995},\n",
       "  {'warm_start': False, 'solver': 'sag', 'C': 0.7000000000000001},\n",
       "  {'warm_start': True, 'solver': 'lbfgs', 'C': 4.3999999999999995},\n",
       "  {'warm_start': False, 'solver': 'newton-cg', 'C': 1.0},\n",
       "  {'warm_start': True, 'solver': 'newton-cg', 'C': 1.0},\n",
       "  {'warm_start': True, 'solver': 'newton-cg', 'C': 3.5000000000000004},\n",
       "  {'warm_start': False, 'solver': 'newton-cg', 'C': 2.4000000000000004},\n",
       "  {'warm_start': False, 'solver': 'newton-cg', 'C': 0.7000000000000001},\n",
       "  {'warm_start': False, 'solver': 'newton-cg', 'C': 4.0},\n",
       "  {'warm_start': False, 'solver': 'sag', 'C': 1.0},\n",
       "  {'warm_start': False, 'solver': 'lbfgs', 'C': 1.5000000000000002},\n",
       "  {'warm_start': False, 'solver': 'newton-cg', 'C': 0.8},\n",
       "  {'warm_start': False, 'solver': 'lbfgs', 'C': 1.8000000000000003},\n",
       "  {'warm_start': True, 'solver': 'sag', 'C': 0.1},\n",
       "  {'warm_start': True, 'solver': 'sag', 'C': 1.3000000000000003},\n",
       "  {'warm_start': True, 'solver': 'newton-cg', 'C': 1.1},\n",
       "  {'warm_start': True, 'solver': 'newton-cg', 'C': 1.2000000000000002},\n",
       "  {'warm_start': False, 'solver': 'lbfgs', 'C': 4.7},\n",
       "  {'warm_start': False, 'solver': 'lbfgs', 'C': 0.30000000000000004},\n",
       "  {'warm_start': False, 'solver': 'sag', 'C': 3.8000000000000003},\n",
       "  {'warm_start': False, 'solver': 'lbfgs', 'C': 4.3999999999999995},\n",
       "  {'warm_start': False, 'solver': 'lbfgs', 'C': 0.7000000000000001},\n",
       "  {'warm_start': False, 'solver': 'sag', 'C': 0.9},\n",
       "  {'warm_start': False, 'solver': 'sag', 'C': 0.8},\n",
       "  {'warm_start': False, 'solver': 'sag', 'C': 2.7},\n",
       "  {'warm_start': False, 'solver': 'newton-cg', 'C': 2.7},\n",
       "  {'warm_start': False, 'solver': 'sag', 'C': 3.7},\n",
       "  {'warm_start': True, 'solver': 'lbfgs', 'C': 1.4000000000000001},\n",
       "  {'warm_start': True, 'solver': 'sag', 'C': 2.2},\n",
       "  {'warm_start': False, 'solver': 'newton-cg', 'C': 0.4},\n",
       "  {'warm_start': True, 'solver': 'newton-cg', 'C': 3.3000000000000003},\n",
       "  {'warm_start': False, 'solver': 'newton-cg', 'C': 3.9000000000000004},\n",
       "  {'warm_start': False, 'solver': 'sag', 'C': 2.0},\n",
       "  {'warm_start': True, 'solver': 'sag', 'C': 0.5},\n",
       "  {'warm_start': True, 'solver': 'lbfgs', 'C': 4.1},\n",
       "  {'warm_start': False, 'solver': 'sag', 'C': 1.7000000000000002},\n",
       "  {'warm_start': True, 'solver': 'newton-cg', 'C': 1.6},\n",
       "  {'warm_start': True, 'solver': 'lbfgs', 'C': 3.4000000000000004},\n",
       "  {'warm_start': False, 'solver': 'sag', 'C': 0.4},\n",
       "  {'warm_start': False, 'solver': 'lbfgs', 'C': 2.3000000000000003},\n",
       "  {'warm_start': True, 'solver': 'lbfgs', 'C': 0.30000000000000004},\n",
       "  {'warm_start': False, 'solver': 'lbfgs', 'C': 2.6},\n",
       "  {'warm_start': True, 'solver': 'newton-cg', 'C': 0.6},\n",
       "  {'warm_start': True, 'solver': 'newton-cg', 'C': 1.8000000000000003},\n",
       "  {'warm_start': True, 'solver': 'newton-cg', 'C': 2.1},\n",
       "  {'warm_start': True, 'solver': 'lbfgs', 'C': 1.7000000000000002},\n",
       "  {'warm_start': True, 'solver': 'newton-cg', 'C': 0.5},\n",
       "  {'warm_start': True, 'solver': 'newton-cg', 'C': 0.2},\n",
       "  {'warm_start': True, 'solver': 'newton-cg', 'C': 2.2},\n",
       "  {'warm_start': False, 'solver': 'newton-cg', 'C': 3.5000000000000004},\n",
       "  {'warm_start': False, 'solver': 'newton-cg', 'C': 4.3},\n",
       "  {'warm_start': False, 'solver': 'lbfgs', 'C': 0.5},\n",
       "  {'warm_start': True, 'solver': 'sag', 'C': 0.2},\n",
       "  {'warm_start': True, 'solver': 'sag', 'C': 0.4},\n",
       "  {'warm_start': True, 'solver': 'newton-cg', 'C': 1.3000000000000003},\n",
       "  {'warm_start': True, 'solver': 'lbfgs', 'C': 0.6},\n",
       "  {'warm_start': True, 'solver': 'sag', 'C': 3.2},\n",
       "  {'warm_start': True, 'solver': 'lbfgs', 'C': 2.9000000000000004},\n",
       "  {'warm_start': False, 'solver': 'sag', 'C': 4.1},\n",
       "  {'warm_start': True, 'solver': 'sag', 'C': 1.0},\n",
       "  {'warm_start': False, 'solver': 'lbfgs', 'C': 0.4},\n",
       "  {'warm_start': True, 'solver': 'sag', 'C': 3.7},\n",
       "  {'warm_start': False, 'solver': 'sag', 'C': 2.4000000000000004},\n",
       "  {'warm_start': True, 'solver': 'lbfgs', 'C': 3.0000000000000004},\n",
       "  {'warm_start': False, 'solver': 'newton-cg', 'C': 2.9000000000000004},\n",
       "  {'warm_start': True, 'solver': 'newton-cg', 'C': 2.7},\n",
       "  {'warm_start': False, 'solver': 'newton-cg', 'C': 4.6},\n",
       "  {'warm_start': False, 'solver': 'lbfgs', 'C': 3.5000000000000004},\n",
       "  {'warm_start': True, 'solver': 'newton-cg', 'C': 1.9000000000000001},\n",
       "  {'warm_start': True, 'solver': 'lbfgs', 'C': 2.6},\n",
       "  {'warm_start': False, 'solver': 'newton-cg', 'C': 3.0000000000000004},\n",
       "  {'warm_start': True, 'solver': 'lbfgs', 'C': 1.0},\n",
       "  {'warm_start': False, 'solver': 'newton-cg', 'C': 2.0},\n",
       "  {'warm_start': True, 'solver': 'sag', 'C': 0.30000000000000004},\n",
       "  {'warm_start': True, 'solver': 'lbfgs', 'C': 0.1},\n",
       "  {'warm_start': False, 'solver': 'lbfgs', 'C': 4.0},\n",
       "  {'warm_start': False, 'solver': 'lbfgs', 'C': 1.1},\n",
       "  {'warm_start': True, 'solver': 'sag', 'C': 2.3000000000000003},\n",
       "  {'warm_start': True, 'solver': 'sag', 'C': 3.9000000000000004},\n",
       "  {'warm_start': True, 'solver': 'sag', 'C': 3.1},\n",
       "  {'warm_start': True, 'solver': 'sag', 'C': 0.9},\n",
       "  {'warm_start': True, 'solver': 'lbfgs', 'C': 0.7000000000000001},\n",
       "  {'warm_start': False, 'solver': 'sag', 'C': 4.8},\n",
       "  {'warm_start': False, 'solver': 'lbfgs', 'C': 1.3000000000000003},\n",
       "  {'warm_start': True, 'solver': 'sag', 'C': 2.9000000000000004},\n",
       "  {'warm_start': True, 'solver': 'sag', 'C': 0.7000000000000001},\n",
       "  {'warm_start': False, 'solver': 'sag', 'C': 3.2},\n",
       "  {'warm_start': False, 'solver': 'sag', 'C': 4.3},\n",
       "  {'warm_start': False, 'solver': 'lbfgs', 'C': 1.9000000000000001},\n",
       "  {'warm_start': True, 'solver': 'lbfgs', 'C': 4.6},\n",
       "  {'warm_start': True, 'solver': 'newton-cg', 'C': 3.2},\n",
       "  {'warm_start': True, 'solver': 'newton-cg', 'C': 0.8},\n",
       "  {'warm_start': False, 'solver': 'sag', 'C': 2.6},\n",
       "  {'warm_start': True, 'solver': 'newton-cg', 'C': 2.3000000000000003},\n",
       "  {'warm_start': False, 'solver': 'newton-cg', 'C': 0.5},\n",
       "  {'warm_start': False, 'solver': 'newton-cg', 'C': 2.1},\n",
       "  {'warm_start': False, 'solver': 'lbfgs', 'C': 4.6},\n",
       "  {'warm_start': True, 'solver': 'sag', 'C': 4.8},\n",
       "  {'warm_start': False, 'solver': 'lbfgs', 'C': 1.7000000000000002},\n",
       "  {'warm_start': True, 'solver': 'lbfgs', 'C': 4.8},\n",
       "  {'warm_start': False, 'solver': 'newton-cg', 'C': 3.6},\n",
       "  {'warm_start': False, 'solver': 'lbfgs', 'C': 1.2000000000000002},\n",
       "  {'warm_start': False, 'solver': 'sag', 'C': 2.1},\n",
       "  {'warm_start': False, 'solver': 'lbfgs', 'C': 2.1},\n",
       "  {'warm_start': True, 'solver': 'lbfgs', 'C': 1.9000000000000001},\n",
       "  {'warm_start': False, 'solver': 'sag', 'C': 0.2},\n",
       "  {'warm_start': False, 'solver': 'lbfgs', 'C': 4.5},\n",
       "  {'warm_start': True, 'solver': 'newton-cg', 'C': 3.4000000000000004},\n",
       "  {'warm_start': True, 'solver': 'lbfgs', 'C': 1.1},\n",
       "  {'warm_start': False, 'solver': 'newton-cg', 'C': 0.6},\n",
       "  {'warm_start': True, 'solver': 'newton-cg', 'C': 4.7},\n",
       "  {'warm_start': True, 'solver': 'sag', 'C': 1.2000000000000002},\n",
       "  {'warm_start': False, 'solver': 'lbfgs', 'C': 3.3000000000000003},\n",
       "  {'warm_start': True, 'solver': 'lbfgs', 'C': 2.1},\n",
       "  {'warm_start': True, 'solver': 'newton-cg', 'C': 1.5000000000000002},\n",
       "  {'warm_start': True, 'solver': 'lbfgs', 'C': 1.6},\n",
       "  {'warm_start': False, 'solver': 'lbfgs', 'C': 3.6},\n",
       "  {'warm_start': False, 'solver': 'lbfgs', 'C': 2.9000000000000004},\n",
       "  {'warm_start': True, 'solver': 'newton-cg', 'C': 2.8000000000000003},\n",
       "  {'warm_start': False, 'solver': 'lbfgs', 'C': 2.7},\n",
       "  {'warm_start': False, 'solver': 'newton-cg', 'C': 4.1},\n",
       "  {'warm_start': False, 'solver': 'lbfgs', 'C': 4.1},\n",
       "  {'warm_start': True, 'solver': 'sag', 'C': 2.4000000000000004},\n",
       "  {'warm_start': True, 'solver': 'sag', 'C': 4.0},\n",
       "  {'warm_start': False, 'solver': 'newton-cg', 'C': 4.2},\n",
       "  {'warm_start': False, 'solver': 'newton-cg', 'C': 1.3000000000000003},\n",
       "  {'warm_start': True, 'solver': 'sag', 'C': 1.5000000000000002},\n",
       "  {'warm_start': False, 'solver': 'newton-cg', 'C': 1.9000000000000001},\n",
       "  {'warm_start': True, 'solver': 'lbfgs', 'C': 4.7},\n",
       "  {'warm_start': True, 'solver': 'newton-cg', 'C': 4.2},\n",
       "  {'warm_start': False, 'solver': 'sag', 'C': 3.5000000000000004},\n",
       "  {'warm_start': True, 'solver': 'sag', 'C': 1.9000000000000001},\n",
       "  {'warm_start': False, 'solver': 'sag', 'C': 3.6},\n",
       "  {'warm_start': True, 'solver': 'sag', 'C': 4.5},\n",
       "  {'warm_start': False, 'solver': 'sag', 'C': 1.8000000000000003},\n",
       "  {'warm_start': True, 'solver': 'sag', 'C': 4.6},\n",
       "  {'warm_start': True, 'solver': 'newton-cg', 'C': 3.7},\n",
       "  {'warm_start': True, 'solver': 'newton-cg', 'C': 2.5000000000000004},\n",
       "  {'warm_start': True, 'solver': 'newton-cg', 'C': 4.5},\n",
       "  {'warm_start': True, 'solver': 'newton-cg', 'C': 1.4000000000000001},\n",
       "  {'warm_start': False, 'solver': 'lbfgs', 'C': 0.8},\n",
       "  {'warm_start': False, 'solver': 'newton-cg', 'C': 2.6},\n",
       "  {'warm_start': False, 'solver': 'newton-cg', 'C': 2.3000000000000003},\n",
       "  {'warm_start': True, 'solver': 'lbfgs', 'C': 1.2000000000000002},\n",
       "  {'warm_start': True, 'solver': 'lbfgs', 'C': 2.4000000000000004},\n",
       "  {'warm_start': True, 'solver': 'lbfgs', 'C': 0.9},\n",
       "  {'warm_start': False, 'solver': 'lbfgs', 'C': 4.2},\n",
       "  {'warm_start': False, 'solver': 'newton-cg', 'C': 0.1},\n",
       "  {'warm_start': True, 'solver': 'newton-cg', 'C': 4.3999999999999995},\n",
       "  {'warm_start': True, 'solver': 'newton-cg', 'C': 2.6},\n",
       "  {'warm_start': False, 'solver': 'newton-cg', 'C': 1.6},\n",
       "  {'warm_start': False, 'solver': 'sag', 'C': 0.1},\n",
       "  {'warm_start': True, 'solver': 'sag', 'C': 2.8000000000000003},\n",
       "  {'warm_start': True, 'solver': 'lbfgs', 'C': 0.2},\n",
       "  {'warm_start': True, 'solver': 'lbfgs', 'C': 1.3000000000000003},\n",
       "  {'warm_start': True, 'solver': 'lbfgs', 'C': 4.5},\n",
       "  {'warm_start': True, 'solver': 'lbfgs', 'C': 2.5000000000000004},\n",
       "  {'warm_start': True, 'solver': 'lbfgs', 'C': 4.2},\n",
       "  {'warm_start': True, 'solver': 'lbfgs', 'C': 2.2},\n",
       "  {'warm_start': False, 'solver': 'lbfgs', 'C': 4.3},\n",
       "  {'warm_start': False, 'solver': 'lbfgs', 'C': 1.0},\n",
       "  {'warm_start': True, 'solver': 'lbfgs', 'C': 3.8000000000000003},\n",
       "  {'warm_start': True, 'solver': 'sag', 'C': 2.6},\n",
       "  {'warm_start': True, 'solver': 'sag', 'C': 2.5000000000000004},\n",
       "  {'warm_start': True, 'solver': 'lbfgs', 'C': 0.5},\n",
       "  {'warm_start': True, 'solver': 'sag', 'C': 1.7000000000000002},\n",
       "  {'warm_start': True, 'solver': 'sag', 'C': 4.9},\n",
       "  {'warm_start': True, 'solver': 'lbfgs', 'C': 3.9000000000000004},\n",
       "  {'warm_start': False, 'solver': 'newton-cg', 'C': 4.9},\n",
       "  {'warm_start': False, 'solver': 'newton-cg', 'C': 3.2},\n",
       "  {'warm_start': True, 'solver': 'newton-cg', 'C': 0.4},\n",
       "  {'warm_start': True, 'solver': 'newton-cg', 'C': 4.1},\n",
       "  {'warm_start': True, 'solver': 'sag', 'C': 3.0000000000000004},\n",
       "  {'warm_start': False, 'solver': 'lbfgs', 'C': 0.6},\n",
       "  {'warm_start': True, 'solver': 'lbfgs', 'C': 3.2},\n",
       "  {'warm_start': True, 'solver': 'sag', 'C': 4.2},\n",
       "  {'warm_start': False, 'solver': 'sag', 'C': 1.6},\n",
       "  {'warm_start': False, 'solver': 'newton-cg', 'C': 1.4000000000000001},\n",
       "  {'warm_start': True, 'solver': 'newton-cg', 'C': 0.1},\n",
       "  {'warm_start': False, 'solver': 'newton-cg', 'C': 0.30000000000000004},\n",
       "  {'warm_start': True, 'solver': 'lbfgs', 'C': 4.3},\n",
       "  {'warm_start': False, 'solver': 'newton-cg', 'C': 1.2000000000000002},\n",
       "  {'warm_start': False, 'solver': 'newton-cg', 'C': 0.9},\n",
       "  {'warm_start': True, 'solver': 'sag', 'C': 2.7},\n",
       "  {'warm_start': False, 'solver': 'sag', 'C': 2.9000000000000004},\n",
       "  {'warm_start': False, 'solver': 'newton-cg', 'C': 4.7},\n",
       "  {'warm_start': True, 'solver': 'lbfgs', 'C': 4.0},\n",
       "  {'warm_start': False, 'solver': 'sag', 'C': 2.3000000000000003},\n",
       "  {'warm_start': False, 'solver': 'sag', 'C': 4.0},\n",
       "  {'warm_start': True, 'solver': 'newton-cg', 'C': 2.0},\n",
       "  {'warm_start': True, 'solver': 'sag', 'C': 1.8000000000000003},\n",
       "  {'warm_start': False, 'solver': 'lbfgs', 'C': 0.1},\n",
       "  {'warm_start': True, 'solver': 'sag', 'C': 3.6},\n",
       "  {'warm_start': True, 'solver': 'newton-cg', 'C': 4.9},\n",
       "  {'warm_start': False, 'solver': 'lbfgs', 'C': 2.5000000000000004},\n",
       "  {'warm_start': False, 'solver': 'newton-cg', 'C': 3.4000000000000004},\n",
       "  {'warm_start': False, 'solver': 'sag', 'C': 2.8000000000000003},\n",
       "  {'warm_start': False, 'solver': 'newton-cg', 'C': 3.3000000000000003},\n",
       "  {'warm_start': True, 'solver': 'sag', 'C': 3.8000000000000003},\n",
       "  {'warm_start': False, 'solver': 'sag', 'C': 3.1},\n",
       "  {'warm_start': True, 'solver': 'newton-cg', 'C': 0.9},\n",
       "  {'warm_start': False, 'solver': 'lbfgs', 'C': 3.2},\n",
       "  {'warm_start': True, 'solver': 'sag', 'C': 1.1},\n",
       "  {'warm_start': True, 'solver': 'newton-cg', 'C': 3.8000000000000003},\n",
       "  {'warm_start': True, 'solver': 'sag', 'C': 3.5000000000000004},\n",
       "  {'warm_start': False, 'solver': 'newton-cg', 'C': 3.1},\n",
       "  {'warm_start': True, 'solver': 'sag', 'C': 4.1},\n",
       "  {'warm_start': True, 'solver': 'newton-cg', 'C': 4.8},\n",
       "  {'warm_start': False, 'solver': 'newton-cg', 'C': 0.2},\n",
       "  {'warm_start': False, 'solver': 'newton-cg', 'C': 3.7},\n",
       "  {'warm_start': False, 'solver': 'sag', 'C': 3.4000000000000004},\n",
       "  {'warm_start': False, 'solver': 'sag', 'C': 4.6},\n",
       "  {'warm_start': True, 'solver': 'lbfgs', 'C': 4.9},\n",
       "  {'warm_start': False, 'solver': 'sag', 'C': 4.2},\n",
       "  {'warm_start': True, 'solver': 'lbfgs', 'C': 3.5000000000000004},\n",
       "  {'warm_start': False, 'solver': 'lbfgs', 'C': 2.2},\n",
       "  {'warm_start': False, 'solver': 'lbfgs', 'C': 3.1},\n",
       "  {'warm_start': False, 'solver': 'sag', 'C': 1.1},\n",
       "  {'warm_start': True, 'solver': 'lbfgs', 'C': 1.8000000000000003},\n",
       "  {'warm_start': False, 'solver': 'newton-cg', 'C': 4.8},\n",
       "  {'warm_start': False, 'solver': 'sag', 'C': 4.9}],\n",
       " 'split0_test_score': array([0.5       , 0.6650532 , 0.6444161 , 0.64314604, 0.5       ,\n",
       "        0.6650532 , 0.5       , 0.6259103 , 0.5       , 0.5       ,\n",
       "        0.64314604, 0.6650532 , 0.64074219, 0.5       , 0.6650532 ,\n",
       "        0.5       , 0.63212432, 0.6650532 , 0.5       , 0.6650532 ,\n",
       "        0.62845042, 0.62985675, 0.6650532 , 0.5       , 0.6650532 ,\n",
       "        0.5       , 0.5       , 0.63212432, 0.5       , 0.6650532 ,\n",
       "        0.5       , 0.62845042, 0.6650532 , 0.6650532 , 0.6650532 ,\n",
       "        0.6650532 , 0.66732077, 0.6650532 , 0.5       , 0.64314604,\n",
       "        0.66618699, 0.6338032 , 0.5       , 0.5       , 0.6650532 ,\n",
       "        0.6650532 , 0.65303397, 0.6259103 , 0.5       , 0.62845042,\n",
       "        0.62958421, 0.5       , 0.5       , 0.5       , 0.6650532 ,\n",
       "        0.5       , 0.63212432, 0.5       , 0.66491693, 0.6650532 ,\n",
       "        0.6650532 , 0.5       , 0.5       , 0.62845042, 0.5       ,\n",
       "        0.6650532 , 0.64822628, 0.5       , 0.63212432, 0.6259103 ,\n",
       "        0.64681995, 0.66732077, 0.6650532 , 0.6650532 , 0.63085426,\n",
       "        0.66732077, 0.65403148, 0.6650532 , 0.6650532 , 0.6650532 ,\n",
       "        0.64074219, 0.5       , 0.5       , 0.6650532 , 0.62718036,\n",
       "        0.5       , 0.65770539, 0.5       , 0.5       , 0.62718036,\n",
       "        0.5       , 0.5       , 0.64314604, 0.6650532 , 0.6650532 ,\n",
       "        0.6650532 , 0.62845042, 0.6650532 , 0.64681995, 0.6650532 ,\n",
       "        0.62958421, 0.6650532 , 0.5       , 0.62464024, 0.65784166,\n",
       "        0.64201225, 0.5       , 0.5       , 0.5       , 0.5       ,\n",
       "        0.62958421, 0.5       , 0.64201225, 0.5       , 0.5       ,\n",
       "        0.5       , 0.5       , 0.64681995, 0.62845042, 0.6650532 ,\n",
       "        0.66618699, 0.5       , 0.6650532 , 0.66732077, 0.6650532 ,\n",
       "        0.62845042, 0.5       , 0.63085426, 0.62985675, 0.6650532 ,\n",
       "        0.65190018, 0.5       , 0.65190018, 0.64681995, 0.5       ,\n",
       "        0.63212432, 0.6650532 , 0.64201225, 0.66732077, 0.6650532 ,\n",
       "        0.5       , 0.62845042, 0.65190018, 0.6650532 , 0.64554989,\n",
       "        0.64681995, 0.65770539, 0.6650532 , 0.64314604, 0.6650532 ,\n",
       "        0.62845042, 0.5       , 0.5       , 0.6650532 , 0.6650532 ,\n",
       "        0.5       , 0.6650532 , 0.65303397, 0.6650532 , 0.5       ,\n",
       "        0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
       "        0.6650532 , 0.6650532 , 0.6650532 , 0.6650532 , 0.64074219,\n",
       "        0.6650532 , 0.6650532 , 0.65190018, 0.6444161 , 0.64074219,\n",
       "        0.62972048, 0.63947214, 0.6650532 , 0.6650532 , 0.6650532 ,\n",
       "        0.5       , 0.5       , 0.6259103 , 0.64201225, 0.63212432,\n",
       "        0.64427983, 0.62972048, 0.64455237, 0.64681995, 0.62958421,\n",
       "        0.63239687, 0.5       , 0.5       , 0.64074219, 0.5       ,\n",
       "        0.5       , 0.63466444, 0.6650532 , 0.6650532 , 0.66491693,\n",
       "        0.6650532 , 0.5       , 0.62718036, 0.62845042, 0.5       ,\n",
       "        0.5       , 0.6650532 , 0.63947214, 0.66124302, 0.64681995,\n",
       "        0.6650532 , 0.66618699, 0.5       , 0.5       , 0.6650532 ,\n",
       "        0.65784166, 0.5       , 0.5       , 0.6650532 , 0.5       ,\n",
       "        0.62464024, 0.5       , 0.6650532 , 0.64427983, 0.6650532 ,\n",
       "        0.5       , 0.6650532 , 0.5       , 0.5       , 0.66618699,\n",
       "        0.62845042, 0.5       , 0.6650532 , 0.5       , 0.6650532 ,\n",
       "        0.5       , 0.6650532 , 0.65403148, 0.6650532 , 0.5       ,\n",
       "        0.5       , 0.64201225, 0.5       , 0.62845042, 0.64455237,\n",
       "        0.64681995, 0.5       , 0.6338032 , 0.6650532 , 0.5       ]),\n",
       " 'split1_test_score': array([0.5       , 0.62302632, 0.60400718, 0.60729665, 0.5       ,\n",
       "        0.62188995, 0.5       , 0.60705742, 0.5       , 0.5       ,\n",
       "        0.60161483, 0.62302632, 0.60047847, 0.5       , 0.62188995,\n",
       "        0.5       , 0.59922249, 0.62302632, 0.5       , 0.62302632,\n",
       "        0.60992823, 0.61937799, 0.62302632, 0.5       , 0.62188995,\n",
       "        0.5       , 0.5       , 0.59922249, 0.5       , 0.62188995,\n",
       "        0.5       , 0.59808612, 0.62416268, 0.62416268, 0.62302632,\n",
       "        0.62302632, 0.6229067 , 0.62188995, 0.5       , 0.60161483,\n",
       "        0.6229067 , 0.59934211, 0.5       , 0.5       , 0.62416268,\n",
       "        0.62302632, 0.59946172, 0.59545455, 0.5       , 0.59808612,\n",
       "        0.59683014, 0.5       , 0.5       , 0.5       , 0.62302632,\n",
       "        0.5       , 0.59922249, 0.5       , 0.62278708, 0.62302632,\n",
       "        0.62188995, 0.5       , 0.5       , 0.59694976, 0.5       ,\n",
       "        0.62302632, 0.60047847, 0.5       , 0.59922249, 0.59545455,\n",
       "        0.60047847, 0.62404306, 0.62302632, 0.62302632, 0.59922249,\n",
       "        0.62278708, 0.61549043, 0.62302632, 0.62302632, 0.62188995,\n",
       "        0.60023923, 0.5       , 0.5       , 0.62302632, 0.59796651,\n",
       "        0.5       , 0.59922249, 0.5       , 0.5       , 0.60388756,\n",
       "        0.5       , 0.5       , 0.60729665, 0.62302632, 0.62302632,\n",
       "        0.62188995, 0.59808612, 0.62302632, 0.60047847, 0.62302632,\n",
       "        0.60047847, 0.62302632, 0.5       , 0.60101675, 0.60047847,\n",
       "        0.59808612, 0.5       , 0.5       , 0.5       , 0.5       ,\n",
       "        0.59683014, 0.5       , 0.60047847, 0.5       , 0.5       ,\n",
       "        0.5       , 0.5       , 0.60047847, 0.60047847, 0.62302632,\n",
       "        0.6229067 , 0.5       , 0.62302632, 0.62278708, 0.62302632,\n",
       "        0.60047847, 0.5       , 0.59922249, 0.61937799, 0.62302632,\n",
       "        0.59922249, 0.5       , 0.61208134, 0.60047847, 0.5       ,\n",
       "        0.59808612, 0.62302632, 0.59808612, 0.62404306, 0.62188995,\n",
       "        0.5       , 0.62063397, 0.61208134, 0.62302632, 0.59922249,\n",
       "        0.59808612, 0.59922249, 0.62302632, 0.61710526, 0.62188995,\n",
       "        0.59694976, 0.5       , 0.5       , 0.62188995, 0.62302632,\n",
       "        0.5       , 0.62302632, 0.59946172, 0.62188995, 0.5       ,\n",
       "        0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
       "        0.62302632, 0.62302632, 0.62188995, 0.62302632, 0.61561005,\n",
       "        0.62302632, 0.62302632, 0.59922249, 0.60400718, 0.60047847,\n",
       "        0.59946172, 0.61297847, 0.62188995, 0.62302632, 0.62302632,\n",
       "        0.5       , 0.5       , 0.60705742, 0.60047847, 0.59808612,\n",
       "        0.6027512 , 0.59946172, 0.59922249, 0.60161483, 0.60047847,\n",
       "        0.60047847, 0.5       , 0.5       , 0.60023923, 0.5       ,\n",
       "        0.5       , 0.59922249, 0.62188995, 0.62302632, 0.62278708,\n",
       "        0.62188995, 0.5       , 0.59796651, 0.59808612, 0.5       ,\n",
       "        0.5       , 0.62302632, 0.61297847, 0.6215311 , 0.60161483,\n",
       "        0.62302632, 0.62416268, 0.5       , 0.5       , 0.62188995,\n",
       "        0.60047847, 0.5       , 0.5       , 0.62302632, 0.5       ,\n",
       "        0.60101675, 0.5       , 0.62188995, 0.6027512 , 0.62302632,\n",
       "        0.5       , 0.62302632, 0.5       , 0.5       , 0.62416268,\n",
       "        0.59808612, 0.5       , 0.62188995, 0.5       , 0.62302632,\n",
       "        0.5       , 0.62188995, 0.61549043, 0.62302632, 0.5       ,\n",
       "        0.5       , 0.60047847, 0.5       , 0.59808612, 0.59922249,\n",
       "        0.59922249, 0.5       , 0.59934211, 0.62188995, 0.5       ]),\n",
       " 'split2_test_score': array([0.5       , 0.67229021, 0.68037587, 0.66835664, 0.5       ,\n",
       "        0.66988636, 0.5       , 0.66923077, 0.5       , 0.5       ,\n",
       "        0.67089161, 0.67342657, 0.66354895, 0.5       , 0.66988636,\n",
       "        0.5       , 0.66988636, 0.66988636, 0.5       , 0.67229021,\n",
       "        0.67089161, 0.67810315, 0.67229021, 0.5       , 0.66988636,\n",
       "        0.5       , 0.5       , 0.66254371, 0.5       , 0.66988636,\n",
       "        0.5       , 0.6611451 , 0.67342657, 0.67342657, 0.66988636,\n",
       "        0.67342657, 0.67342657, 0.66988636, 0.5       , 0.67089161,\n",
       "        0.67342657, 0.66975524, 0.5       , 0.5       , 0.67342657,\n",
       "        0.67342657, 0.66975524, 0.65620629, 0.5       , 0.6611451 ,\n",
       "        0.66962413, 0.5       , 0.5       , 0.5       , 0.67229021,\n",
       "        0.5       , 0.66988636, 0.5       , 0.67329545, 0.67229021,\n",
       "        0.66988636, 0.5       , 0.5       , 0.67076049, 0.5       ,\n",
       "        0.67342657, 0.67089161, 0.5       , 0.66254371, 0.65620629,\n",
       "        0.66127622, 0.67342657, 0.67342657, 0.67342657, 0.6673514 ,\n",
       "        0.67342657, 0.67076049, 0.67342657, 0.66988636, 0.66988636,\n",
       "        0.66582168, 0.5       , 0.5       , 0.67342657, 0.6611451 ,\n",
       "        0.5       , 0.66481643, 0.5       , 0.5       , 0.65659965,\n",
       "        0.5       , 0.5       , 0.66835664, 0.67229021, 0.67229021,\n",
       "        0.66988636, 0.67089161, 0.67342657, 0.66127622, 0.67229021,\n",
       "        0.6743007 , 0.67342657, 0.5       , 0.64392483, 0.66975524,\n",
       "        0.66468531, 0.5       , 0.5       , 0.5       , 0.5       ,\n",
       "        0.66962413, 0.5       , 0.6673514 , 0.5       , 0.5       ,\n",
       "        0.5       , 0.5       , 0.66975524, 0.67089161, 0.67229021,\n",
       "        0.67342657, 0.5       , 0.67342657, 0.67342657, 0.67342657,\n",
       "        0.67089161, 0.5       , 0.6673514 , 0.67810315, 0.66988636,\n",
       "        0.66507867, 0.5       , 0.66988636, 0.66975524, 0.5       ,\n",
       "        0.66975524, 0.67229021, 0.66468531, 0.67342657, 0.66988636,\n",
       "        0.5       , 0.66494755, 0.66988636, 0.67342657, 0.6673514 ,\n",
       "        0.67189685, 0.66481643, 0.67229021, 0.6673514 , 0.66988636,\n",
       "        0.67076049, 0.5       , 0.5       , 0.66988636, 0.67342657,\n",
       "        0.5       , 0.67342657, 0.66975524, 0.66988636, 0.5       ,\n",
       "        0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
       "        0.66988636, 0.67229021, 0.66988636, 0.67342657, 0.67189685,\n",
       "        0.67229021, 0.67342657, 0.66507867, 0.68037587, 0.66354895,\n",
       "        0.66848776, 0.65253497, 0.66988636, 0.67229021, 0.67342657,\n",
       "        0.5       , 0.5       , 0.66923077, 0.6673514 , 0.66975524,\n",
       "        0.66975524, 0.66848776, 0.6673514 , 0.67329545, 0.6743007 ,\n",
       "        0.66341783, 0.5       , 0.5       , 0.66582168, 0.5       ,\n",
       "        0.5       , 0.66722028, 0.66988636, 0.67229021, 0.67329545,\n",
       "        0.66988636, 0.5       , 0.6611451 , 0.67089161, 0.5       ,\n",
       "        0.5       , 0.67342657, 0.65253497, 0.67443182, 0.67329545,\n",
       "        0.67342657, 0.67102273, 0.5       , 0.5       , 0.66988636,\n",
       "        0.66975524, 0.5       , 0.5       , 0.67342657, 0.5       ,\n",
       "        0.64392483, 0.5       , 0.66988636, 0.66975524, 0.67229021,\n",
       "        0.5       , 0.67229021, 0.5       , 0.5       , 0.67102273,\n",
       "        0.67089161, 0.5       , 0.66988636, 0.5       , 0.67229021,\n",
       "        0.5       , 0.66988636, 0.67076049, 0.66988636, 0.5       ,\n",
       "        0.5       , 0.67229021, 0.5       , 0.67089161, 0.6673514 ,\n",
       "        0.6673514 , 0.5       , 0.66975524, 0.66988636, 0.5       ]),\n",
       " 'mean_test_score': array([0.5       , 0.65345658, 0.64293305, 0.63959978, 0.5       ,\n",
       "        0.65227651, 0.5       , 0.63406616, 0.5       , 0.5       ,\n",
       "        0.63855083, 0.65383536, 0.6349232 , 0.5       , 0.65227651,\n",
       "        0.5       , 0.63374439, 0.65265529, 0.5       , 0.65345658,\n",
       "        0.63642342, 0.64244596, 0.65345658, 0.5       , 0.65227651,\n",
       "        0.5       , 0.5       , 0.63129684, 0.5       , 0.65227651,\n",
       "        0.5       , 0.62922722, 0.65421415, 0.65421415, 0.65265529,\n",
       "        0.65383536, 0.65455135, 0.65227651, 0.5       , 0.63855083,\n",
       "        0.65417342, 0.63430018, 0.5       , 0.5       , 0.65421415,\n",
       "        0.65383536, 0.64075031, 0.62585705, 0.5       , 0.62922722,\n",
       "        0.63201282, 0.5       , 0.5       , 0.5       , 0.65345658,\n",
       "        0.5       , 0.63374439, 0.5       , 0.65366649, 0.65345658,\n",
       "        0.65227651, 0.5       , 0.5       , 0.63205356, 0.5       ,\n",
       "        0.65383536, 0.63986545, 0.5       , 0.63129684, 0.62585705,\n",
       "        0.63619155, 0.65493014, 0.65383536, 0.65383536, 0.63247605,\n",
       "        0.65451148, 0.6467608 , 0.65383536, 0.65265529, 0.65227651,\n",
       "        0.63560104, 0.5       , 0.5       , 0.65383536, 0.62876399,\n",
       "        0.5       , 0.64058144, 0.5       , 0.5       , 0.62922252,\n",
       "        0.5       , 0.5       , 0.63959978, 0.65345658, 0.65345658,\n",
       "        0.65227651, 0.63247605, 0.65383536, 0.63619155, 0.65345658,\n",
       "        0.63478779, 0.65383536, 0.5       , 0.62319394, 0.64269179,\n",
       "        0.6349279 , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
       "        0.63201282, 0.5       , 0.63661404, 0.5       , 0.5       ,\n",
       "        0.5       , 0.5       , 0.63901789, 0.6332735 , 0.65345658,\n",
       "        0.65417342, 0.5       , 0.65383536, 0.65451148, 0.65383536,\n",
       "        0.6332735 , 0.5       , 0.63247605, 0.64244596, 0.65265529,\n",
       "        0.63873378, 0.5       , 0.64462263, 0.63901789, 0.5       ,\n",
       "        0.6333219 , 0.65345658, 0.6349279 , 0.65493014, 0.65227651,\n",
       "        0.5       , 0.63801065, 0.64462263, 0.65383536, 0.63737459,\n",
       "        0.63893431, 0.64058144, 0.65345658, 0.64253423, 0.65227651,\n",
       "        0.63205356, 0.5       , 0.5       , 0.65227651, 0.65383536,\n",
       "        0.5       , 0.65383536, 0.64075031, 0.65227651, 0.5       ,\n",
       "        0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
       "        0.65265529, 0.65345658, 0.65227651, 0.65383536, 0.6427497 ,\n",
       "        0.65345658, 0.65383536, 0.63873378, 0.64293305, 0.6349232 ,\n",
       "        0.63255665, 0.63499519, 0.65227651, 0.65345658, 0.65383536,\n",
       "        0.5       , 0.5       , 0.63406616, 0.63661404, 0.6333219 ,\n",
       "        0.63892876, 0.63255665, 0.63704209, 0.64057674, 0.63478779,\n",
       "        0.63209772, 0.5       , 0.5       , 0.63560104, 0.5       ,\n",
       "        0.5       , 0.6337024 , 0.65227651, 0.65345658, 0.65366649,\n",
       "        0.65227651, 0.5       , 0.62876399, 0.63247605, 0.5       ,\n",
       "        0.5       , 0.65383536, 0.63499519, 0.65240198, 0.64057674,\n",
       "        0.65383536, 0.6537908 , 0.5       , 0.5       , 0.65227651,\n",
       "        0.64269179, 0.5       , 0.5       , 0.65383536, 0.5       ,\n",
       "        0.62319394, 0.5       , 0.65227651, 0.63892876, 0.65345658,\n",
       "        0.5       , 0.65345658, 0.5       , 0.5       , 0.6537908 ,\n",
       "        0.63247605, 0.5       , 0.65227651, 0.5       , 0.65345658,\n",
       "        0.5       , 0.65227651, 0.6467608 , 0.65265529, 0.5       ,\n",
       "        0.5       , 0.63826031, 0.5       , 0.63247605, 0.63704209,\n",
       "        0.63779794, 0.5       , 0.63430018, 0.65227651, 0.5       ]),\n",
       " 'std_test_score': array([0.        , 0.02171933, 0.03119502, 0.02505344, 0.        ,\n",
       "        0.02157695, 0.        , 0.02602909, 0.        , 0.        ,\n",
       "        0.02846817, 0.02205185, 0.02607511, 0.        , 0.02157695,\n",
       "        0.        , 0.02887114, 0.02104356, 0.        , 0.02171933,\n",
       "        0.02551875, 0.02557378, 0.02171933, 0.        , 0.02157695,\n",
       "        0.        , 0.        , 0.0258574 , 0.        , 0.02157695,\n",
       "        0.        , 0.02574958, 0.0215228 , 0.0215228 , 0.02104356,\n",
       "        0.02205185, 0.02251456, 0.02157695, 0.        , 0.02846817,\n",
       "        0.02230559, 0.02874819, 0.        , 0.        , 0.0215228 ,\n",
       "        0.02205185, 0.0299829 , 0.02480183, 0.        , 0.02574958,\n",
       "        0.0297676 , 0.        , 0.        , 0.        , 0.02171933,\n",
       "        0.        , 0.02887114, 0.        , 0.02210133, 0.02171933,\n",
       "        0.02157695, 0.        , 0.        , 0.03024062, 0.        ,\n",
       "        0.02205185, 0.02934769, 0.        , 0.0258574 , 0.02480183,\n",
       "        0.02593343, 0.02198225, 0.02205185, 0.02205185, 0.02783714,\n",
       "        0.0225706 , 0.0231422 , 0.02205185, 0.02104356, 0.02157695,\n",
       "        0.0270196 , 0.        , 0.        , 0.02205185, 0.02581685,\n",
       "        0.        , 0.02938893, 0.        , 0.        , 0.02156802,\n",
       "        0.        , 0.        , 0.02505344, 0.02171933, 0.02171933,\n",
       "        0.02157695, 0.02985871, 0.02205185, 0.02593343, 0.02171933,\n",
       "        0.03036158, 0.02205185, 0.        , 0.01754698, 0.03024298,\n",
       "        0.02764663, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.0297676 , 0.        , 0.02756632, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.02881518, 0.02894764, 0.02171933,\n",
       "        0.02230559, 0.        , 0.02205185, 0.0225706 , 0.02205185,\n",
       "        0.02894764, 0.        , 0.02783714, 0.02557378, 0.02104356,\n",
       "        0.028452  , 0.        , 0.02415336, 0.02881518, 0.        ,\n",
       "        0.02927105, 0.02171933, 0.02764663, 0.02198225, 0.02157695,\n",
       "        0.        , 0.01931272, 0.02415336, 0.02205185, 0.02840791,\n",
       "        0.03064467, 0.02938893, 0.02171933, 0.02051746, 0.02157695,\n",
       "        0.03024062, 0.        , 0.        , 0.02157695, 0.02205185,\n",
       "        0.        , 0.02205185, 0.0299829 , 0.02157695, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.02104356, 0.02171933, 0.02157695, 0.02205185, 0.0230228 ,\n",
       "        0.02171933, 0.02205185, 0.028452  , 0.03119502, 0.02607511,\n",
       "        0.02825103, 0.01645623, 0.02157695, 0.02171933, 0.02205185,\n",
       "        0.        , 0.        , 0.02602909, 0.02756632, 0.02927105,\n",
       "        0.02761474, 0.02825103, 0.02831596, 0.02959461, 0.03036158,\n",
       "        0.02569576, 0.        , 0.        , 0.0270196 , 0.        ,\n",
       "        0.        , 0.02776832, 0.02157695, 0.02171933, 0.02210133,\n",
       "        0.02157695, 0.        , 0.02581685, 0.02985871, 0.        ,\n",
       "        0.        , 0.02205185, 0.01645623, 0.02248325, 0.02959461,\n",
       "        0.02205185, 0.02104305, 0.        , 0.        , 0.02157695,\n",
       "        0.03024298, 0.        , 0.        , 0.02205185, 0.        ,\n",
       "        0.01754698, 0.        , 0.02157695, 0.02761474, 0.02171933,\n",
       "        0.        , 0.02171933, 0.        , 0.        , 0.02104305,\n",
       "        0.02985871, 0.        , 0.02157695, 0.        , 0.02171933,\n",
       "        0.        , 0.02157695, 0.0231422 , 0.02104356, 0.        ,\n",
       "        0.        , 0.02943682, 0.        , 0.02985871, 0.02831596,\n",
       "        0.02853576, 0.        , 0.02874819, 0.02157695, 0.        ]),\n",
       " 'rank_test_score': array([168,  36,  86, 101, 168,  61, 168, 135, 168, 168, 110,  11, 129,\n",
       "        168,  61, 168, 137,  54, 168,  36, 120,  92,  36, 168,  61, 168,\n",
       "        168, 157, 168,  61, 168, 159,   6,   6,  54,  11,   3,  61, 168,\n",
       "        110,   9, 133, 168, 168,   6,  11,  94, 164, 168, 159, 155, 168,\n",
       "        168, 168,  36, 168, 137, 168,  34,  36,  61, 168, 168, 153, 168,\n",
       "         11, 100, 168, 157, 164, 121,   1,  11,  11, 146,   4,  82,  11,\n",
       "         54,  61, 123, 168, 168,  11, 162, 168,  96, 168, 168, 161, 168,\n",
       "        168, 101,  36,  36,  61, 146,  11, 121,  36, 131,  11, 168, 166,\n",
       "         89, 127, 168, 168, 168, 168, 155, 168, 118, 168, 168, 168, 168,\n",
       "        103, 142,  36,   9, 168,  11,   4,  11, 142, 168, 146,  92,  54,\n",
       "        108, 168,  84, 103, 168, 140,  36, 127,   1,  61, 168, 113,  84,\n",
       "         11, 115, 105,  96,  36,  91,  61, 153, 168, 168,  61,  11, 168,\n",
       "         11,  94,  61, 168, 168, 168, 168, 168, 168,  54,  36,  61,  11,\n",
       "         88,  36,  11, 108,  86, 129, 144, 125,  61,  36,  11, 168, 168,\n",
       "        135, 118, 140, 106, 144, 116,  98, 131, 152, 168, 168, 123, 168,\n",
       "        168, 139,  61,  36,  34,  61, 168, 162, 146, 168, 168,  11, 125,\n",
       "         60,  98,  11,  32, 168, 168,  61,  89, 168, 168,  11, 168, 166,\n",
       "        168,  61, 106,  36, 168,  36, 168, 168,  32, 146, 168,  61, 168,\n",
       "         36, 168,  61,  82,  54, 168, 168, 112, 168, 146, 116, 114, 168,\n",
       "        133,  61, 168], dtype=int32)}"
      ]
     },
     "execution_count": 600,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the results of RandomizedSearch CV\n",
    "lr_tuned_cv.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.6, max_iter=1000, random_state=219, solver='newton-cg',\n",
       "                   warm_start=True)"
      ]
     },
     "execution_count": 601,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the best estimator for the model\n",
    "lr_tuned_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned LR\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Tuned Training ACCURACY: 0.7443\n",
      "LR Tuned Testing  ACCURACY: 0.7474\n",
      "LR Tuned AUC Score        : 0.6549\n"
     ]
    }
   ],
   "source": [
    "# building a model based on hyperparameter tuning results\n",
    "\n",
    "# INSTANTIATING a logistic regression model with tuned values\n",
    "lr_tuned = lr_tuned_cv.best_estimator_\n",
    "\n",
    "\n",
    "# FIT step is not needed\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "lr_tuned_pred = lr_tuned.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('LR Tuned Training ACCURACY:', lr_tuned.score(X_train, y_train).round(4))\n",
    "print('LR Tuned Testing  ACCURACY:', lr_tuned.score(X_test, y_test).round(4))\n",
    "print('LR Tuned AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = lr_tuned_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "lr_tuned_train_score = lr_tuned.score(X_train, y_train).round(4) # accuracy\n",
    "lr_tuned_test_score  = lr_tuned.score(X_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving the AUC score\n",
    "lr_tuned_auc         = roc_auc_score(y_true  = y_test,\n",
    "                                     y_score = lr_tuned_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True Negatives : 62\n",
      "False Positives: 94\n",
      "False Negatives: 29\n",
      "True Positives : 302\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unpacking the confusion matrix\n",
    "lr_tuned_tn, \\\n",
    "lr_tuned_fp, \\\n",
    "lr_tuned_fn, \\\n",
    "lr_tuned_tp = confusion_matrix(y_true = y_test, y_pred = lr_tuned_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {lr_tuned_tn}\n",
    "False Positives: {lr_tuned_fp}\n",
    "False Negatives: {lr_tuned_fn}\n",
    "True Positives : {lr_tuned_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>AUC Score</th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Testing Accuracy</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>0.6132</td>\n",
       "      <td>0.7231</td>\n",
       "      <td>0.7207</td>\n",
       "      <td>(49, 107, 29, 302)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Full Tree</td>\n",
       "      <td>0.6192</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6735</td>\n",
       "      <td>(73, 83, 76, 255)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pruned Tree</td>\n",
       "      <td>0.7130</td>\n",
       "      <td>0.7512</td>\n",
       "      <td>0.7803</td>\n",
       "      <td>(82, 74, 33, 298)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tuned LR</td>\n",
       "      <td>0.6549</td>\n",
       "      <td>0.7443</td>\n",
       "      <td>0.7474</td>\n",
       "      <td>(62, 94, 29, 302)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tuned LR</td>\n",
       "      <td>0.6549</td>\n",
       "      <td>0.7443</td>\n",
       "      <td>0.7474</td>\n",
       "      <td>(62, 94, 29, 302)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tuned LR</td>\n",
       "      <td>0.6549</td>\n",
       "      <td>0.7443</td>\n",
       "      <td>0.7474</td>\n",
       "      <td>(62, 94, 29, 302)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tuned LR</td>\n",
       "      <td>0.6549</td>\n",
       "      <td>0.7443</td>\n",
       "      <td>0.7474</td>\n",
       "      <td>(62, 94, 29, 302)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Model Name  AUC Score  Training Accuracy  Testing Accuracy  \\\n",
       "0     Logistic     0.6132             0.7231            0.7207   \n",
       "1    Full Tree     0.6192             1.0000            0.6735   \n",
       "2  Pruned Tree     0.7130             0.7512            0.7803   \n",
       "3     Tuned LR     0.6549             0.7443            0.7474   \n",
       "4     Tuned LR     0.6549             0.7443            0.7474   \n",
       "5     Tuned LR     0.6549             0.7443            0.7474   \n",
       "6     Tuned LR     0.6549             0.7443            0.7474   \n",
       "\n",
       "     Confusion Matrix  \n",
       "0  (49, 107, 29, 302)  \n",
       "1   (73, 83, 76, 255)  \n",
       "2   (82, 74, 33, 298)  \n",
       "3   (62, 94, 29, 302)  \n",
       "4   (62, 94, 29, 302)  \n",
       "5   (62, 94, 29, 302)  \n",
       "6   (62, 94, 29, 302)  "
      ]
     },
     "execution_count": 607,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading model performance\n",
    "\n",
    "# declaring model performance objects\n",
    "lr_train_acc = lr_tuned.score(X_train, y_train).round(4)\n",
    "lr_test_acc  = lr_tuned.score(X_test, y_test).round(4)\n",
    "lr_auc       = roc_auc_score(y_true  = y_test,\n",
    "                             y_score = lr_tuned_pred).round(4)\n",
    "\n",
    "\n",
    "# appending to model_performance\n",
    "model_performance = model_performance.append(\n",
    "                          {'Model Name'        : 'Tuned LR',\n",
    "                           'Training Accuracy' : lr_train_acc,\n",
    "                           'Testing Accuracy'  : lr_test_acc,\n",
    "                           'AUC Score'         : lr_auc,\n",
    "                           'Confusion Matrix'  : (lr_tuned_tn,\n",
    "                                                  lr_tuned_fp,\n",
    "                                                  lr_tuned_fn,\n",
    "                                                  lr_tuned_tp)},\n",
    "                           ignore_index = True)\n",
    "\n",
    "\n",
    "# checking the results\n",
    "model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned Tree\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-609-4dc3f988466f>:4: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  depth_space     = pd.np.arange(1, 25, 1)\n",
      "<ipython-input-609-4dc3f988466f>:5: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  leaf_space      = pd.np.arange(1, 100, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Parameters  : {'splitter': 'best', 'min_samples_leaf': 16, 'max_depth': 3, 'criterion': 'gini'}\n",
      "Tuned Training AUC: 0.7032\n"
     ]
    }
   ],
   "source": [
    "# declaring a hyperparameter space\n",
    "criterion_space = ['gini', 'entropy']\n",
    "splitter_space  = ['best', 'random']\n",
    "depth_space     = pd.np.arange(1, 25, 1)\n",
    "leaf_space      = pd.np.arange(1, 100, 1)\n",
    "\n",
    "\n",
    "# creating a hyperparameter grid\n",
    "param_grid = {'criterion'        : criterion_space,\n",
    "              'splitter'         : splitter_space,\n",
    "              'max_depth'        : depth_space,\n",
    "              'min_samples_leaf' : leaf_space}\n",
    "\n",
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "tuned_tree = DecisionTreeClassifier(random_state = 219)\n",
    "\n",
    "\n",
    "# RandomizedSearchCV object\n",
    "tuned_tree_cv = RandomizedSearchCV(estimator             = tuned_tree,\n",
    "                                   param_distributions   = param_grid,\n",
    "                                   cv                    = 3,\n",
    "                                   n_iter                = 1000,\n",
    "                                   random_state          = 219,\n",
    "                                   scoring = make_scorer(roc_auc_score,\n",
    "                                             needs_threshold = False))\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "tuned_tree_cv.fit(chef_data, chef_target)\n",
    "\n",
    "\n",
    "# PREDICT step is not needed\n",
    "\n",
    "\n",
    "# printing the optimal parameters and best score\n",
    "print(\"Tuned Parameters  :\", tuned_tree_cv.best_params_)\n",
    "print(\"Tuned Training AUC:\", tuned_tree_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.7402\n",
      "Testing  ACCURACY: 0.7762\n",
      "AUC Score        : 0.732\n"
     ]
    }
   ],
   "source": [
    "# building a model based on hyperparameter tuning results\n",
    "\n",
    "# INSTANTIATING a logistic regression model with tuned values\n",
    "tree_tuned = tuned_tree_cv.best_estimator_\n",
    "\n",
    "\n",
    "# FIT step is not needed\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "tree_tuned_pred = tree_tuned.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', tree_tuned.score(X_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', tree_tuned.score(X_test, y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = tree_tuned_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "tree_tuned_train_score = tree_tuned.score(X_train, y_train).round(4) # accuracy\n",
    "tree_tuned_test_score  = tree_tuned.score(X_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving the AUC score\n",
    "tree_tuned_auc         = roc_auc_score(y_true  = y_test,\n",
    "                                     y_score = tree_tuned_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True Negatives : 95\n",
      "False Positives: 61\n",
      "False Negatives: 48\n",
      "True Positives : 283\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unpacking the confusion matrix\n",
    "tuned_tree_tn, \\\n",
    "tuned_tree_fp, \\\n",
    "tuned_tree_fn, \\\n",
    "tuned_tree_tp = confusion_matrix(y_true = y_test, y_pred = tree_tuned_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {tuned_tree_tn}\n",
    "False Positives: {tuned_tree_fp}\n",
    "False Negatives: {tuned_tree_fn}\n",
    "True Positives : {tuned_tree_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>AUC Score</th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Testing Accuracy</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>0.6132</td>\n",
       "      <td>0.7231</td>\n",
       "      <td>0.7207</td>\n",
       "      <td>(49, 107, 29, 302)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Full Tree</td>\n",
       "      <td>0.6192</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6735</td>\n",
       "      <td>(73, 83, 76, 255)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pruned Tree</td>\n",
       "      <td>0.7130</td>\n",
       "      <td>0.7512</td>\n",
       "      <td>0.7803</td>\n",
       "      <td>(82, 74, 33, 298)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tuned LR</td>\n",
       "      <td>0.6549</td>\n",
       "      <td>0.7443</td>\n",
       "      <td>0.7474</td>\n",
       "      <td>(62, 94, 29, 302)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tuned LR</td>\n",
       "      <td>0.6549</td>\n",
       "      <td>0.7443</td>\n",
       "      <td>0.7474</td>\n",
       "      <td>(62, 94, 29, 302)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tuned LR</td>\n",
       "      <td>0.6549</td>\n",
       "      <td>0.7443</td>\n",
       "      <td>0.7474</td>\n",
       "      <td>(62, 94, 29, 302)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tuned LR</td>\n",
       "      <td>0.6549</td>\n",
       "      <td>0.7443</td>\n",
       "      <td>0.7474</td>\n",
       "      <td>(62, 94, 29, 302)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Tuned Tree</td>\n",
       "      <td>0.7320</td>\n",
       "      <td>0.7402</td>\n",
       "      <td>0.7762</td>\n",
       "      <td>(95, 61, 48, 283)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Model Name  AUC Score  Training Accuracy  Testing Accuracy  \\\n",
       "0     Logistic     0.6132             0.7231            0.7207   \n",
       "1    Full Tree     0.6192             1.0000            0.6735   \n",
       "2  Pruned Tree     0.7130             0.7512            0.7803   \n",
       "3     Tuned LR     0.6549             0.7443            0.7474   \n",
       "4     Tuned LR     0.6549             0.7443            0.7474   \n",
       "5     Tuned LR     0.6549             0.7443            0.7474   \n",
       "6     Tuned LR     0.6549             0.7443            0.7474   \n",
       "7   Tuned Tree     0.7320             0.7402            0.7762   \n",
       "\n",
       "     Confusion Matrix  \n",
       "0  (49, 107, 29, 302)  \n",
       "1   (73, 83, 76, 255)  \n",
       "2   (82, 74, 33, 298)  \n",
       "3   (62, 94, 29, 302)  \n",
       "4   (62, 94, 29, 302)  \n",
       "5   (62, 94, 29, 302)  \n",
       "6   (62, 94, 29, 302)  \n",
       "7   (95, 61, 48, 283)  "
      ]
     },
     "execution_count": 612,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# declaring model performance objects\n",
    "tree_train_acc = tree_tuned.score(X_train, y_train).round(4)\n",
    "tree_test_acc  = tree_tuned.score(X_test, y_test).round(4)\n",
    "tree_auc       = roc_auc_score(y_true  = y_test,\n",
    "                              y_score = tree_tuned_pred).round(4)\n",
    "\n",
    "\n",
    "# appending to model_performance\n",
    "model_performance = model_performance.append(\n",
    "                          {'Model Name'        : 'Tuned Tree',\n",
    "                           'Training Accuracy' : tree_train_acc,\n",
    "                           'Testing Accuracy'  : tree_test_acc,\n",
    "                           'AUC Score'         : tree_auc,\n",
    "                           'Confusion Matrix'  : (tuned_tree_tn,\n",
    "                                                  tuned_tree_fp,\n",
    "                                                  tuned_tree_fn,\n",
    "                                                  tuned_tree_tp)},\n",
    "                           ignore_index = True)\n",
    "\n",
    "\n",
    "# checking the results\n",
    "model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Ensemble Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# plot_feature_importances\n",
    "########################################\n",
    "def plot_feature_importances(model, train, export = False):\n",
    "    \"\"\"\n",
    "    Plots the importance of features from a CART model.\n",
    "    \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    model  : CART model\n",
    "    train  : explanatory variable training data\n",
    "    export : whether or not to export as a .png image, default False\n",
    "    \"\"\"\n",
    "    \n",
    "    # declaring the number\n",
    "    n_features = train.shape[1]\n",
    "    \n",
    "    # setting plot window\n",
    "    fig, ax = plt.subplots(figsize=(12,9))\n",
    "    \n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center')\n",
    "    plt.yticks(pd.np.arange(n_features), train.columns)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    \n",
    "    if export == True:\n",
    "        plt.savefig('./analysis_images/Feature_Importance.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split with the logit_sig variables\n",
    "chef_data   =  chef.loc[ : , candidate_dict['logit_full']]\n",
    "chef_target =  chef.loc[ : , 'CROSS_SELL_SUCCESS']\n",
    "\n",
    "\n",
    "# train/test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "            chef_data,\n",
    "            chef_target,\n",
    "            random_state = 219,\n",
    "            test_size    = 0.25,\n",
    "            stratify     = chef_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest (Full)\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING a random forest model with default values\n",
    "rf_default = RandomForestClassifier(n_estimators     = 100,\n",
    "                                    criterion        = 'gini',\n",
    "                                    max_depth        = None,\n",
    "                                    min_samples_leaf = 1,\n",
    "                                    bootstrap        = True,\n",
    "                                    warm_start       = False,\n",
    "                                    random_state     = 219)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 1.0\n",
      "Testing  ACCURACY: 0.7556\n",
      "AUC Score        : 0.6711\n"
     ]
    }
   ],
   "source": [
    "# FITTING the training data\n",
    "rf_default_fit = rf_default.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "rf_default_fit_pred = rf_default_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', rf_default_fit.score(x_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', rf_default_fit.score(x_test, y_test).round(4))\n",
    "\n",
    "\n",
    "# saving AUC score\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = rf_default_fit_pred).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-614-141a9830ad1b>:22: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  plt.yticks(pd.np.arange(n_features), train.columns)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting feature importances\n",
    "plot_feature_importances(rf_default_fit,\n",
    "                         train = x_train,\n",
    "                         export = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True Negatives : 68\n",
      "False Positives: 88\n",
      "False Negatives: 31\n",
      "True Positives : 300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unpacking the confusion matrix\n",
    "rf_tn, \\\n",
    "rf_fp, \\\n",
    "rf_fn, \\\n",
    "rf_tp = confusion_matrix(y_true = y_test, y_pred = rf_default_fit_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {rf_tn}\n",
    "False Positives: {rf_fp}\n",
    "False Negatives: {rf_fn}\n",
    "True Positives : {rf_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>AUC Score</th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Testing Accuracy</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>0.6132</td>\n",
       "      <td>0.7231</td>\n",
       "      <td>0.7207</td>\n",
       "      <td>(49, 107, 29, 302)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Full Tree</td>\n",
       "      <td>0.6192</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6735</td>\n",
       "      <td>(73, 83, 76, 255)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pruned Tree</td>\n",
       "      <td>0.7130</td>\n",
       "      <td>0.7512</td>\n",
       "      <td>0.7803</td>\n",
       "      <td>(82, 74, 33, 298)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tuned LR</td>\n",
       "      <td>0.6549</td>\n",
       "      <td>0.7443</td>\n",
       "      <td>0.7474</td>\n",
       "      <td>(62, 94, 29, 302)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tuned LR</td>\n",
       "      <td>0.6549</td>\n",
       "      <td>0.7443</td>\n",
       "      <td>0.7474</td>\n",
       "      <td>(62, 94, 29, 302)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tuned LR</td>\n",
       "      <td>0.6549</td>\n",
       "      <td>0.7443</td>\n",
       "      <td>0.7474</td>\n",
       "      <td>(62, 94, 29, 302)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tuned LR</td>\n",
       "      <td>0.6549</td>\n",
       "      <td>0.7443</td>\n",
       "      <td>0.7474</td>\n",
       "      <td>(62, 94, 29, 302)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Tuned Tree</td>\n",
       "      <td>0.7320</td>\n",
       "      <td>0.7402</td>\n",
       "      <td>0.7762</td>\n",
       "      <td>(95, 61, 48, 283)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Random Forest (Full)</td>\n",
       "      <td>0.6711</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.7556</td>\n",
       "      <td>(68, 88, 31, 300)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Model Name  AUC Score  Training Accuracy  Testing Accuracy  \\\n",
       "0              Logistic     0.6132             0.7231            0.7207   \n",
       "1             Full Tree     0.6192             1.0000            0.6735   \n",
       "2           Pruned Tree     0.7130             0.7512            0.7803   \n",
       "3              Tuned LR     0.6549             0.7443            0.7474   \n",
       "4              Tuned LR     0.6549             0.7443            0.7474   \n",
       "5              Tuned LR     0.6549             0.7443            0.7474   \n",
       "6              Tuned LR     0.6549             0.7443            0.7474   \n",
       "7            Tuned Tree     0.7320             0.7402            0.7762   \n",
       "8  Random Forest (Full)     0.6711             1.0000            0.7556   \n",
       "\n",
       "     Confusion Matrix  \n",
       "0  (49, 107, 29, 302)  \n",
       "1   (73, 83, 76, 255)  \n",
       "2   (82, 74, 33, 298)  \n",
       "3   (62, 94, 29, 302)  \n",
       "4   (62, 94, 29, 302)  \n",
       "5   (62, 94, 29, 302)  \n",
       "6   (62, 94, 29, 302)  \n",
       "7   (95, 61, 48, 283)  \n",
       "8   (68, 88, 31, 300)  "
      ]
     },
     "execution_count": 627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# declaring model performance objects\n",
    "rf_train_acc = rf_default_fit.score(x_train, y_train).round(4)\n",
    "rf_test_acc  = rf_default_fit.score(x_test, y_test).round(4)\n",
    "rf_auc       = roc_auc_score(y_true  = y_test,\n",
    "                             y_score = rf_default_fit_pred).round(4)\n",
    "\n",
    "\n",
    "# appending to model_performance\n",
    "model_performance = model_performance.append(\n",
    "                          {'Model Name'         : 'Random Forest (Full)',\n",
    "                           'Training Accuracy'  : rf_train_acc,\n",
    "                           'Testing Accuracy'   : rf_test_acc,\n",
    "                           'AUC Score'          : rf_auc,\n",
    "                           'Confusion Matrix'   : (rf_tn,\n",
    "                                                   rf_fp,\n",
    "                                                   rf_fn,\n",
    "                                                   rf_tp)},\n",
    "                          ignore_index = True)\n",
    "\n",
    "\n",
    "# checking the results\n",
    "model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuned Random Forest (Full)\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FITTING the training data\n",
    "rf_default_fit = rf_default.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "rf_default_fit_pred = rf_default_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# declaring a hyperparameter space\n",
    "estimator_space  = pd.np.arange(100, 1100, 250)\n",
    "leaf_space       = pd.np.arange(1, 31, 10)\n",
    "criterion_space  = ['gini', 'entropy']\n",
    "bootstrap_space  = [True, False]\n",
    "warm_start_space = [True, False]\n",
    "\n",
    "\n",
    "# creating a hyperparameter grid\n",
    "param_grid = {'n_estimators'     : estimator_space,\n",
    "              'min_samples_leaf' : leaf_space,\n",
    "              'criterion'        : criterion_space,\n",
    "              'bootstrap'        : bootstrap_space,\n",
    "              'warm_start'       : warm_start_space}\n",
    "\n",
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "forest_grid = RandomForestClassifier(random_state = 219)\n",
    "\n",
    "\n",
    "# GridSearchCV object\n",
    "forest_cv = RandomizedSearchCV(estimator           = forest_grid,\n",
    "                               param_distributions = param_grid,\n",
    "                               cv         = 3,\n",
    "                               n_iter     = 1000,\n",
    "                               scoring    = make_scorer(roc_auc_score,\n",
    "                                            needs_threshold = False))\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "forest_cv.fit(chef_data, chef_target)\n",
    "\n",
    "\n",
    "# PREDICT step is not needed\n",
    "\n",
    "\n",
    "# printing the optimal parameters and best score\n",
    "print(\"Tuned Parameters  :\", forest_cv.best_params_)\n",
    "print(\"Tuned Training AUC:\", forest_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best estimators based on RandomizedSearchCV\n",
    "forest_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a model based on hyperparameter tuning results\n",
    "\n",
    "# copy/pasting in the best_estimator_ results\n",
    "# to avoid running another RandomizedSearch\n",
    "forest_tuned = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
    "                       criterion='entropy', max_depth=None, max_features='auto',\n",
    "                       max_leaf_nodes=None, max_samples=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=11, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=350,\n",
    "                       n_jobs=None, oob_score=False, random_state=219,\n",
    "                       verbose=0, warm_start=True)\n",
    "\n",
    "\n",
    "# FITTING the model object\n",
    "forest_tuned_fit = forest_tuned.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "forest_tuned_pred = forest_tuned_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Forest Tuned Training ACCURACY:', forest_tuned.score(x_train, y_train).round(4))\n",
    "print('Forest Tuned Testing  ACCURACY:', forest_tuned.score(x_test, y_test).round(4))\n",
    "print('Forest Tuned AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                                   y_score = forest_tuned_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "forest_tuned_train_score = forest_tuned.score(x_train, y_train).round(4) # accuracy\n",
    "forest_tuned_test_score  = forest_tuned.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving the AUC score\n",
    "forest_tuned_auc = roc_auc_score(y_true  = y_test,\n",
    "                                 y_score = forest_tuned_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting feature importances\n",
    "plot_feature_importances(forest_tuned_fit,\n",
    "                         train = x_train,\n",
    "                         export = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpacking the confusion matrix\n",
    "tuned_rf_tn, \\\n",
    "tuned_rf_fp, \\\n",
    "tuned_rf_fn, \\\n",
    "tuned_rf_tp = confusion_matrix(y_true = y_test, y_pred = forest_tuned_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {tuned_rf_tn}\n",
    "False Positives: {tuned_rf_fp}\n",
    "False Negatives: {tuned_rf_fn}\n",
    "True Positives : {tuned_rf_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring model performance objects\n",
    "tuned_rf_train_acc = forest_tuned_fit.score(x_train, y_train).round(4)\n",
    "tuned_rf_test_acc  = forest_tuned_fit.score(x_test, y_test).round(4)\n",
    "tuned_rf_auc       = roc_auc_score(y_true  = y_test,\n",
    "                                   y_score = forest_tuned_pred).round(4)\n",
    "\n",
    "\n",
    "# appending to model_performance\n",
    "model_performance = model_performance.append(\n",
    "                          {'Model Name'         : 'Tuned Random Forest (Full)',\n",
    "                           'Training Accuracy'  : tuned_rf_train_acc,\n",
    "                           'Testing Accuracy'   : tuned_rf_test_acc,\n",
    "                           'AUC Score'          : tuned_rf_auc,\n",
    "                           'Confusion Matrix'   : (tuned_rf_tn,\n",
    "                                                   tuned_rf_fp,\n",
    "                                                   tuned_rf_fn,\n",
    "                                                   tuned_rf_tp)},\n",
    "                          ignore_index = True)\n",
    "\n",
    "\n",
    "# checking the results\n",
    "model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GBM (Full)\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.8417\n",
      "Testing ACCURACY : 0.7515\n",
      "AUC Score        : 0.6698\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING the model object without hyperparameters\n",
    "full_gbm_default = GradientBoostingClassifier(loss          = 'deviance',\n",
    "                                              learning_rate = 0.1,\n",
    "                                              n_estimators  = 100,\n",
    "                                              criterion     = 'friedman_mse',\n",
    "                                              max_depth     = 3,\n",
    "                                              warm_start    = False,\n",
    "                                              random_state  = 219)\n",
    "\n",
    "\n",
    "# FIT step is needed as we are not using .best_estimator\n",
    "full_gbm_default_fit = full_gbm_default.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "full_gbm_default_pred = full_gbm_default_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', full_gbm_default_fit.score(x_train, y_train).round(4))\n",
    "print('Testing ACCURACY :', full_gbm_default_fit.score(x_test, y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = full_gbm_default_pred).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True Negatives : 69\n",
      "False Positives: 87\n",
      "False Negatives: 34\n",
      "True Positives : 297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unpacking the confusion matrix\n",
    "gbm_default_tn, \\\n",
    "gbm_default_fp, \\\n",
    "gbm_default_fn, \\\n",
    "gbm_default_tp = confusion_matrix(y_true = y_test, y_pred = full_gbm_default_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {gbm_default_tn}\n",
    "False Positives: {gbm_default_fp}\n",
    "False Negatives: {gbm_default_fn}\n",
    "True Positives : {gbm_default_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>AUC Score</th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Testing Accuracy</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>0.6132</td>\n",
       "      <td>0.7231</td>\n",
       "      <td>0.7207</td>\n",
       "      <td>(49, 107, 29, 302)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Full Tree</td>\n",
       "      <td>0.6192</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6735</td>\n",
       "      <td>(73, 83, 76, 255)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pruned Tree</td>\n",
       "      <td>0.7130</td>\n",
       "      <td>0.7512</td>\n",
       "      <td>0.7803</td>\n",
       "      <td>(82, 74, 33, 298)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tuned LR</td>\n",
       "      <td>0.6549</td>\n",
       "      <td>0.7443</td>\n",
       "      <td>0.7474</td>\n",
       "      <td>(62, 94, 29, 302)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tuned LR</td>\n",
       "      <td>0.6549</td>\n",
       "      <td>0.7443</td>\n",
       "      <td>0.7474</td>\n",
       "      <td>(62, 94, 29, 302)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tuned LR</td>\n",
       "      <td>0.6549</td>\n",
       "      <td>0.7443</td>\n",
       "      <td>0.7474</td>\n",
       "      <td>(62, 94, 29, 302)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tuned LR</td>\n",
       "      <td>0.6549</td>\n",
       "      <td>0.7443</td>\n",
       "      <td>0.7474</td>\n",
       "      <td>(62, 94, 29, 302)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Tuned Tree</td>\n",
       "      <td>0.7320</td>\n",
       "      <td>0.7402</td>\n",
       "      <td>0.7762</td>\n",
       "      <td>(95, 61, 48, 283)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Random Forest (Full)</td>\n",
       "      <td>0.6711</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.7556</td>\n",
       "      <td>(68, 88, 31, 300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Tuned Random Forest (Full)</td>\n",
       "      <td>0.6310</td>\n",
       "      <td>0.8033</td>\n",
       "      <td>0.7495</td>\n",
       "      <td>(47, 109, 13, 318)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GBM (Full)</td>\n",
       "      <td>0.6698</td>\n",
       "      <td>0.8417</td>\n",
       "      <td>0.7515</td>\n",
       "      <td>(69, 87, 34, 297)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model Name  AUC Score  Training Accuracy  \\\n",
       "0                     Logistic     0.6132             0.7231   \n",
       "1                    Full Tree     0.6192             1.0000   \n",
       "2                  Pruned Tree     0.7130             0.7512   \n",
       "3                     Tuned LR     0.6549             0.7443   \n",
       "4                     Tuned LR     0.6549             0.7443   \n",
       "5                     Tuned LR     0.6549             0.7443   \n",
       "6                     Tuned LR     0.6549             0.7443   \n",
       "7                   Tuned Tree     0.7320             0.7402   \n",
       "8         Random Forest (Full)     0.6711             1.0000   \n",
       "9   Tuned Random Forest (Full)     0.6310             0.8033   \n",
       "10                  GBM (Full)     0.6698             0.8417   \n",
       "\n",
       "    Testing Accuracy    Confusion Matrix  \n",
       "0             0.7207  (49, 107, 29, 302)  \n",
       "1             0.6735   (73, 83, 76, 255)  \n",
       "2             0.7803   (82, 74, 33, 298)  \n",
       "3             0.7474   (62, 94, 29, 302)  \n",
       "4             0.7474   (62, 94, 29, 302)  \n",
       "5             0.7474   (62, 94, 29, 302)  \n",
       "6             0.7474   (62, 94, 29, 302)  \n",
       "7             0.7762   (95, 61, 48, 283)  \n",
       "8             0.7556   (68, 88, 31, 300)  \n",
       "9             0.7495  (47, 109, 13, 318)  \n",
       "10            0.7515   (69, 87, 34, 297)  "
      ]
     },
     "execution_count": 638,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# declaring model performance objects\n",
    "gbm_train_acc = full_gbm_default_fit.score(x_train, y_train).round(4)\n",
    "gbm_test_acc  = full_gbm_default_fit.score(x_test, y_test).round(4)\n",
    "gbm_auc       = roc_auc_score(y_true  = y_test,\n",
    "                              y_score = full_gbm_default_pred).round(4)\n",
    "\n",
    "\n",
    "# appending to model_performance\n",
    "model_performance = model_performance.append(\n",
    "                          {'Model Name'       : 'GBM (Full)',\n",
    "                          'Training Accuracy' : gbm_train_acc,\n",
    "                          'Testing Accuracy'  : gbm_test_acc,\n",
    "                          'AUC Score'         : gbm_auc,\n",
    "                          'Confusion Matrix'  : (gbm_default_tn,\n",
    "                                                 gbm_default_fp,\n",
    "                                                 gbm_default_fn,\n",
    "                                                 gbm_default_tp)},\n",
    "                          ignore_index = True)\n",
    "\n",
    "\n",
    "# checking the results\n",
    "model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuned GBM\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-640-4231b2b02a87>:2: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  learn_space        = pd.np.arange(0.1, 2.0, 0.2)\n",
      "<ipython-input-640-4231b2b02a87>:3: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  estimator_space    = pd.np.arange(100, 200, 25)\n",
      "<ipython-input-640-4231b2b02a87>:4: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  depth_space        = pd.np.arange(1, 20, 2)\n"
     ]
    }
   ],
   "source": [
    "# declaring a hyperparameter space\n",
    "learn_space        = pd.np.arange(0.1, 2.0, 0.2)\n",
    "estimator_space    = pd.np.arange(100, 200, 25)\n",
    "depth_space        = pd.np.arange(1, 20, 2)\n",
    "warm_start_space   = [True, False]\n",
    "\n",
    "# creating a hyperparameter grid\n",
    "param_grid = {'learning_rate' : learn_space,\n",
    "              'max_depth'     : depth_space,\n",
    "              'n_estimators'  : estimator_space,\n",
    "              'warm_start'     : warm_start_space}\n",
    "\n",
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "full_gbm_grid = GradientBoostingClassifier(random_state = 219)\n",
    "\n",
    "\n",
    "# GridSearchCV object\n",
    "full_gbm_cv = RandomizedSearchCV(estimator     = full_gbm_grid,\n",
    "                           param_distributions = param_grid,\n",
    "                           cv                  = 3,\n",
    "                           n_iter              = 500,\n",
    "                           random_state        = 219,\n",
    "                           scoring             = make_scorer(roc_auc_score,\n",
    "                                                 needs_threshold = False))\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "full_gbm_cv.fit(chef_data, chef_target)\n",
    "\n",
    "\n",
    "# PREDICT step is not needed\n",
    "\n",
    "\n",
    "# printing the optimal parameters and best score\n",
    "print(\"Tuned Parameters  :\", full_gbm_cv.best_params_)\n",
    "print(\"Tuned Training AUC:\", full_gbm_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the best estimator for the model\n",
    "full_gbm_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING the model object without hyperparameters\n",
    "\n",
    "# I made several attempts to hyperparameter tuning\n",
    "gbm_tuned = GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
    "                           learning_rate=0.7000000000000001, loss='deviance',\n",
    "                           max_depth=1, max_features=None, max_leaf_nodes=None,\n",
    "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                           min_samples_leaf=1, min_samples_split=2,\n",
    "                           min_weight_fraction_leaf=0.0, n_estimators=125,\n",
    "                           n_iter_no_change=None, presort='deprecated',\n",
    "                           random_state=219, subsample=1.0, tol=0.0001,\n",
    "                           validation_fraction=0.1, verbose=0,\n",
    "                           warm_start=True)\n",
    "\n",
    "# FIT step is needed as we are not using .best_estimator\n",
    "gbm_tuned_fit = gbm_tuned.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "gbm_tuned_pred = gbm_tuned_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', gbm_tuned_fit.score(x_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', gbm_tuned_fit.score(x_test, y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = gbm_tuned_pred).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpacking the confusion matrix\n",
    "gbm_tuned_tn, \\\n",
    "gbm_tuned_fp, \\\n",
    "gbm_tuned_fn, \\\n",
    "gbm_tuned_tp = confusion_matrix(y_true = y_test, y_pred = gbm_tuned_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {gbm_tuned_tn}\n",
    "False Positives: {gbm_tuned_fp}\n",
    "False Negatives: {gbm_tuned_fn}\n",
    "True Positives : {gbm_tuned_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring model performance objects\n",
    "gbm_train_acc = gbm_tuned_fit.score(x_train, y_train).round(4)\n",
    "gbm_test_acc  = gbm_tuned_fit.score(x_test, y_test).round(4)\n",
    "gbm_auc       = roc_auc_score(y_true  = y_test,\n",
    "                              y_score = gbm_tuned_pred).round(4)\n",
    "\n",
    "\n",
    "# appending to model_performance\n",
    "model_performance = model_performance.append(\n",
    "                          {'Model Name'        : 'Tuned GBM',\n",
    "                          'Training Accuracy'  : gbm_train_acc,\n",
    "                          'Testing Accuracy'   : gbm_test_acc,\n",
    "                          'AUC Score'          : gbm_auc,\n",
    "                          'Confusion Matrix'   : (gbm_tuned_tn,\n",
    "                                                  gbm_tuned_fp,\n",
    "                                                  gbm_tuned_fn,\n",
    "                                                  gbm_tuned_tp)},\n",
    "                          ignore_index = True)\n",
    "\n",
    "\n",
    "# checking the results\n",
    "model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance.sort_values(by = 'AUC Score',\n",
    "                              ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the DataFrame to Excel\n",
    "model_performance.to_excel('./model_results/classification_model_performance.xlsx',\n",
    "                           index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best I can get is Pruned Tree\t0.7130\t0.7512\t0.7803\t(82, 74, 33, 298)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.7803\t Testing Accuracy and lowest Flase positive/negative error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
