

%pylab inline

import dataiku
import dataiku.spark as dkuspark
import pyspark
from pyspark.sql import SQLContext

# Load PySpark
sc = pyspark.SparkContext.getOrCreate()
sqlContext = SQLContext(sc)

# Example: Read the descriptor of a Dataiku dataset
mydataset = dataiku.Dataset("pokemon_test_binary")
# And read it as a Spark dataframe
df = dkuspark.get_dataframe(sqlContext, mydataset)


from pyspark.ml import Pipeline
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.feature import HashingTF, Tokenizer
from pyspark.sql import Row
from pyspark.sql.functions import UserDefinedFunction
from pyspark.sql.types import *
from pyspark.ml.feature import VectorAssembler
from pyspark.mllib.evaluation import *
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType, DoubleType
from pyspark.ml.linalg import Vectors

#creating vectors with names of variables
vecAssembler = VectorAssembler(inputCols = ['total', 'hp', 'attack', 'defense'], outputCol="features")
v_df = vecAssembler.transform(df)
vhouse_df = v_df.select(['features', 'binaryoutcome'])
vhouse_df = vhouse_df.withColumnRenamed("binaryoutcome", "label")# We have to rename our output variable to 'label'

#splitting the dataset
splits = vhouse_df.randomSplit([0.7, 0.3])
train_df = splits[0]
test_df = splits[1]

#need to call 2 functions to build and evalueate a Feed Forward Neaural Network
from pyspark.ml.classification import MultilayerPerceptronClassifier
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

# specify layers for the neural network:
# input layer of size 4 ( we have 4 features in our data), one intermediate hidden layer of size 3
# and output of size 2 (classes)

layers = [ 4, 3, 2]

# create the trainer and set its parameters
FNN = MultilayerPerceptronClassifier(labelCol="label", featuresCol="features",\
                                         maxIter=100, layers=layers, blockSize=128, seed=1234)
model = FNN.fit(train_df)

# compute accuracy on the test set
result = model.transform(test_df)
predictionAndLabels = result.select("prediction", "label")
evaluator = MulticlassClassificationEvaluator(metricName="accuracy")
print("Test set accuracy = " + str(evaluator.evaluate(predictionAndLabels)))

from pyspark.ml.evaluation import BinaryClassificationEvaluator
evaluator = BinaryClassificationEvaluator(rawPredictionCol="rawPrediction")
AUC_ROC = evaluator.evaluate(result,{evaluator.metricName: "areaUnderROC"})#this result refers to the result when we run this on the test_df
print('AUC ROC:' + str(AUC_ROC))

